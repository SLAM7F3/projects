===============================================================================
PV40 cluster notes
===============================================================================
Last updated on 5/11/16; 6/15/16; 6/16/16; 11/15/16
===============================================================================

*.  Eufori configuration file on PV40 is located in 

	/gpfs/ssdws11/conf/flyover-eufori.dcfg

This configuration file maps the following aliases to PV40 paths:

             corelogic/basepath --> /gpfs/mercury12/corelogic_shapefiles
             tiger/basepath     --> /gpfs/mercury12/tiger_shapefiles
             pr/annotation_path --> /gpfs/mercury11/supersize/labeling_resources

Eufori configuration file on Startown is located in
/startown-gpfs/ssdws03/conf/flyover-eufori.dcfg, and it maps aliases to
Startown paths as follows:

	     corelogic/basepath  -->     /startown-gpfs/ssdws03/devdata/corelogic_shapefiles/
	     tiger/basepath      -->     /startown-gpfs/ssdws03/devdata/tiger_shapefiles/
             pr/annotation_path  -->     /startown-gpfs/sputnik03/supersize/labeling_resources/

*.  In order to check on jobs on PV40, we need to chant the analog of our
startown "ssh_distrib" command:

   /startown-gpfs/sputnik01/supersize/bin/distrib -client pv40m01ls-geo07060101.geo.apple.com

We've put this command into "ssh_pv40_distrib"

*.  On 6/15/16, Erik Soderval told us that something is seriously messed up
with our ~/.ssh/config file.  Some bad characters in this file are the
reason why we're unable to directly mount PV40's /gpfs onto our local
Thinkmate machine.  

Erik created for us a new "pv40_config" which we've placed into ~/.ssh.  
If we chant the following command, then PV40's /gpfs should be mounted to
our mountpoint /pv40-gpfs/ on our Thinkmate:

sshfs -F ~/.ssh/pv40_config -o StrictHostKeyChecking=no -o idmap=user pv40-04:/gpfs /pv40-gpfs/


We've put this command into "mount_pv40"

*.  As of 5/6/16, we are unable to directly mount PV40 cluster's /gpfs
directory on our local Thinkmate machine.  But we can at least login to
PV40's launchpad.  John Wood then showed us that from PV40's launchpad, we
can login to one of the O(60) GPU nodes on the PV40 cluster (which also
contains CPU nodes) via a command like

   ssh pv40m01ls-gpu0002.geo.apple.com

On a gpu node, cd to /gpfs/mercury12/dl/ which contains 

   datasets/ training/

subdirs.  John suggested taking a look inside 

   /gpfs/mercury12/dl/training/20160204_privacy/network/resnet50_l2_norm

at the solver_base.prototxt and train_base.prototxt files.


John has also consolidated all the variable parameters within the training
and solver prototext files within his "config" files such as
resnet50_l2_norm.config.  Snapshot outputs are sent into folders of output/


*.  On 5/11/16, Viktor again tried to get pv40's gpfs to mount on our local
thinkmate.  He followed some discussion in 

  http://www.cyberciti.biz/faq/linux-unix-ssh-proxycommand-passing-through-one-host-gateway-server/

But there is still a "connection reset by peer" error message.  

*.  On 5/3/16, we downloaded ROADS and ADDRFEAT zipfiles for entire US by
chanting

wget --verbose --recursive ftp://ftp2.census.gov/geo/tiger/TIGER2016/ROADS/*.zip

wget --verbose --recursive ftp://ftp2.census.gov/geo/tiger/TIGER2016/ADDRFEAT/*.zip

Copied these files into /startown-gpfs/ssdws03/devdata/tiger_shapefiles/ on
startown cluster and into /gpfs/mercury12/tiger_shapefiles/ on PV40
cluster.

*.  Shape files on PV40 appear to be sitting 

-shape_path /gpfs/mercury11/distrib_batch_shape/shape_efrojd_kitty_hawk_boston_160511_172314.shape 

prj = kitty_hawk_boston
resource = kitty_hawk_boston

===============================================================================
PV40 cluster notes
===============================================================================
Last updated on 6/15/16; 6/16/16; 11/15/16; 11/16/16
===============================================================================

*.  Eufori configuration file on PV40 is located in 

	/gpfs/ssdws11/conf/flyover-eufori.dcfg

This configuration file maps the following aliases to PV40 paths:

             corelogic/basepath --> /gpfs/mercury12/corelogic_shapefiles
             tiger/basepath     --> /gpfs/mercury12/tiger_shapefiles
             pr/annotation_path --> /gpfs/mercury11/supersize/labeling_resources

Eufori configuration file on Startown is located in
/startown-gpfs/ssdws03/conf/flyover-eufori.dcfg, and it maps aliases to
Startown paths as follows:

	     corelogic/basepath  -->     /startown-gpfs/ssdws03/devdata/corelogic_shapefiles/
	     tiger/basepath      -->     /startown-gpfs/ssdws03/devdata/tiger_shapefiles/
             pr/annotation_path  -->     /startown-gpfs/sputnik03/supersize/labeling_resources/

*.  In order to check on jobs on PV40, we need to chant the analog of our
startown "ssh_distrib" command:

   /startown-gpfs/sputnik01/supersize/bin/distrib -client pv40m01ls-geo07060101.geo.apple.com

We've put this command into "ssh_pv40_distrib"

*.  On 6/15/16, Erik Soderval told us that something is seriously messed up
with our ~/.ssh/config file.  Some bad characters in this file are the
reason why we're unable to directly mount PV40's /gpfs onto our local
Thinkmate machine.  

Erik created for us a new "pv40_config" which we've placed into ~/.ssh.  
If we chant the following command, then PV40's /gpfs should be mounted to
our mountpoint /pv40-gpfs/ on our Thinkmate:

sshfs -F ~/.ssh/pv40_config -o StrictHostKeyChecking=no -o idmap=user pv40-04:/gpfs /pv40-gpfs/


We've put this command into "mount_pv40"

*.  As of 5/6/16, we are unable to directly mount PV40 cluster's /gpfs
directory on our local Thinkmate machine.  But we can at least login to
PV40's launchpad.  John Wood then showed us that from PV40's launchpad, we
can login to one of the O(60) GPU nodes on the PV40 cluster (which also
contains CPU nodes) via a command like

   ssh -Y pv40m01ls-gpu0002.geo.apple.com

On a gpu node, cd to /gpfs/mercury12/dl/ which contains 

   datasets/ training/

subdirs.  John suggested taking a look inside 

   /gpfs/mercury12/dl/training/20160204_privacy/network/resnet50_l2_norm

at the solver_base.prototxt and train_base.prototxt files.


John has also consolidated all the variable parameters within the training
and solver prototext files within his "config" files such as
resnet50_l2_norm.config.  Snapshot outputs are sent into folders of output/


*.  On 5/11/16, Viktor again tried to get pv40's gpfs to mount on our local
thinkmate.  He followed some discussion in 

  http://www.cyberciti.biz/faq/linux-unix-ssh-proxycommand-passing-through-one-host-gateway-server/

But there is still a "connection reset by peer" error message.  

*.  On 5/3/16, we downloaded ROADS and ADDRFEAT zipfiles for entire US by
chanting

wget --verbose --recursive ftp://ftp2.census.gov/geo/tiger/TIGER2016/ROADS/*.zip

wget --verbose --recursive ftp://ftp2.census.gov/geo/tiger/TIGER2016/ADDRFEAT/*.zip

Copied these files into /startown-gpfs/ssdws03/devdata/tiger_shapefiles/ on
startown cluster and into /gpfs/mercury12/tiger_shapefiles/ on PV40
cluster.  

In Nov 2016, we copied an updated set of 2016 TIGER files from our local
ThinkMate machine to Startown.  We used the following rsync command to
transfer several hundred TIGER zip files:

   rsync -avz ./TIGER2016/ pcho@10.182.128.166:/home/pcho/tiger_2016/

   rsync -avz ./CoreLogic_2016Q3_ParcelPoints/ pcho@10.182.128.166:/home/pcho/CoreLogic_2016/


   rsync -avz ./CoreLogic_2016Q3_ParcelPolygons/ pcho@10.182.128.166:/home/pcho/CoreLogic_2016_ParcelPolygons/

On the other hand, we were unable to ssh or rsync into a PV40 node.  So we
instead used the following rsync command in order to copy TIGER files to
our local machine's mountpoint:

   rsync -avz ./TIGER2016/ /pv40-gpfs/mercury12/tiger_shapefiles/ftp2.census.gov/geo/tiger/TIGER2016/

   rsync -avz ./CoreLogic_2016Q3_ParcelPolygons/ /pv40-gpfs/mercury12/corelogic_shapefiles/CoreLogic_2016Q3_ParcelPolygons/

*.  Shape files on PV40 appear to be sitting 

-shape_path /gpfs/mercury11/distrib_batch_shape/shape_efrojd_kitty_hawk_boston_160511_172314.shape 

prj = kitty_hawk_boston
resource = kitty_hawk_boston




/gpfs/mercury11/supersize/bin/pwin/pwin_master/latest -vv -nogpfs -cfg /home/pcho/hn_pano/peter_flyover-eufori.dcfg -da hn_pano_gui -prjdb /gpfs/mercury11/distrib_batch_prj/prjdb_khara-desktop_t300_sf_collect_2016_161209_161116.xml -prj t300_sf_collect_2016 -runid pr_house_numbers_t300_sf_collect_2016_2016-12-09__16:12:32 -shp /gpfs/mercury11/distrib_batch_shape/shape_khara_t300_sf_collect_2016_161209_161117.shape -bucket_id maps001_debug -build_id 58547c943ee4438cce17be08 -output_folder /home/pcho/hn_pano/

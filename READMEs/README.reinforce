==========================================================================
Reinforcement learning notes
==========================================================================
Last updated on 10/29/16; 10/30/16; 11/3/16; 11/4/16
==========================================================================

*.  On 10/19/16, we found the following set of hyperparameters which yields
perfect agent learning to not place any of its TT pieces into already
occupied cells in a 4x4 grid:

   batch_size = 10;	// Perform parameter update after this many episodes
    learning_rate = 3E-4;  // Much better than 1E-4 !!!
   lambda = 0.0;	// L2 regularization coefficient (better than 1E-3)
   gamma = 0.90;	// Discount factor for reward
   rmsprop_decay_rate = 0.95; 
   

   int nsize = 4;
   int n_zlevels = 1;
   tictac3d* ttt_ptr = new tictac3d(nsize, n_zlevels);

   int Din = nsize * nsize;	// Input dimensionality
   int H = 200;			// Number of hidden layer neurons
   int Dout = nsize * nsize;	// Output dimensionality
   int Tmax = nsize * nsize * n_zlevels;

   vector<int> layer_dims;
   layer_dims.push_back(Din);
   layer_dims.push_back(H);
   layer_dims.push_back(Dout);
   reinforce* reinforce_ptr = new reinforce(layer_dims, Tmax)

episode_number = 620000
  T = 8 +/- 0
  Running reward mean = 1
n_filled_cells = 16

 O | X | O | O 
----------------
 O | X | X | X 
----------------
 O | X | X | O 
----------------
 X | O | O | X 

*.  On 10/20/16, we started experimenting with filling Z=2 levels of 4x4
grids.  

learning_rate = 1E-3 is much worse than learning rate 3E-4

learning_rate = 5.196E-4 definitely asymptotes to worse reward than 3E-4

learning rate = 3E-4 yields quasi-decent but not great reward results
		for 2 z-levels

learning_rate = Reward results reach same asymoptotic value as for 3E-4.
		But learning is considerably slower than for 3E-4.

learning_rate = 1E-4 is much worse than learning rate 3E-4

Discount rate = 0.7 yields faster (but not better) convergence to
asymptotic running reward sum around 0.4 than discount rate = 0.9.
Discount rate = 0.5 yields even faster (but also no better) convergence
than gamma = 0.7

Running reward --> 1 for Z=2 levels with learning_rate = 0.003, gamma = 0.5
and rmsprop_decay = 0.9, H1 = 300 !!!  Turns history frac --> 1

Running reward --> 0.6 for Z=2 levels with learning_rate = 0.003, gamma = 0.5
and rmsprop_decay = 0.9, H1 = 256.  This is definitely not as good as H1 =
300 !

Z=2 levels with two hidden layers containing 128 nodes, learning_rate =
0.003, gamma = 0.5 and rmsprop_decay = 0.9 yields terrible results.

------------------------------------------
Z=2 levels with H1 = 256, H2 = 128, learning_rate =
0.0010, gamma = 0.5 and rmsprop_decay = 0.9.  Turns history rises and then
falls  - definite worse than learning_rate = 0.0003 !!!


Z=2 levels with H1 = 256, H2 = 128, learning_rate =
0.0003, gamma = 0.5 and rmsprop_decay = 0.9: n_turns_frac --> 0.86

Z=2 levels with H1 = 256, H2 = 128, learning_rate =
0.0001, gamma = 0.5 and rmsprop_decay = 0.9 .  Comparable performance to
learning_rate = 3E-4.

------------------------------------------
Z = 2 levels
------------------------------------------

*.  Z=2 levels with H1 = 300, H2 = 300, learning_rate =
0.0003, gamma = 0.5 and rmsprop_decay = 0.9 .   Turns history frac
asymptotes to 0.8 and definitely not 1!

*.  Z=2 levels with H1 = 300, H2 = 150, learning_rate =
0.0003, gamma = 0.5 and rmsprop_decay = 0.9 .   Turns history frac
asymptotes to 0.8.  

layer = 0 wlo = -2.1208 w_05 = -0.302912 w_25 = -0.0958052
   w_50 = 0.0241359 w_75 = 0.193846 w_95 = 0.437919 whi = 3.84919
layer = 1 wlo = -1.39498 w_05 = -0.365861 w_25 = -0.128735
   w_50 = 0.0142132 w_75 = 0.160051 w_95 = 0.41266 whi = 1.56651
layer = 2 wlo = -2.33946 w_05 = -0.115958 w_25 = 0.359509
   w_50 = 1.10251 w_75 = 1.45926 w_95 = 1.82759 whi = 3.02317
48.74 minutes 

*.  Z=2 levels with H1 = 300, H2 = 150, learning_rate =
0.0001, gamma = 0.5 and rmsprop_decay = 0.9 .   Turns history frac
asymptotes to 0.86.  Better than learning_rate = 3E-4.

layer = 0 wlo = -1.49288 w_05 = -0.392911 w_25 = -0.110789
   w_50 = 0.00682151 w_75 = 0.117715 w_95 = 0.359205 whi = 1.69722
layer = 1 wlo = -0.733989 w_05 = -0.200402 w_25 = -0.075696
   w_50 = 0.00378144 w_75 = 0.0886445 w_95 = 0.240782 whi = 0.691564
layer = 2 wlo = -0.80083 w_05 = 0.340652 w_25 = 0.529501
   w_50 = 0.659939 w_75 = 0.813973 w_95 = 1.11432 whi = 1.43289
46.36 minutes 

*.  Z=2 levels with H1 = 300, H2 = 150, learning_rate =
3E-5, gamma = 0.5 and rmsprop_decay = 0.9 .   Turns history frac
asymptotes to 0.86.  

*.  Z=2 levels with H1 = 300, H2 = 150, learning_rate =
1E-5, gamma = 0.5 and rmsprop_decay = 0.9 .   Turns history frac
reaches 0.6 after 1E6 episodes and appears to keep climbing.

layer = 0 wlo = -1.02333 w_05 = -0.351843 w_25 = -0.117336
   w_50 = -0.000639077 w_75 = 0.102129 w_95 = 0.335207 whi = 1.11941
layer = 1 wlo = -0.431056 w_05 = -0.130479 w_25 = -0.0465206
   w_50 = 0.0104769 w_75 = 0.067586 w_95 = 0.153783 whi = 0.418167
layer = 2 wlo = -0.545656 w_05 = 0.0155456 w_25 = 0.153152
   w_50 = 0.246818 w_75 = 0.335681 w_95 = 0.441951 whi = 0.634136
39 minutes

*.  Z=2 levels with H1 = 300, H2 = 150, learning_rate =
1E-4, gamma = 0.5 and rmsprop_decay = 0.8 .   Loss function trending 
downwards after 3E6 time steps.  Turn frac slowly trending upwards after
3E6 time steps.  Running reward barely trending upwards after 3E6 time
steps.  

layer = 0 wlo = -1.54267 w_05 = -0.401816 w_25 = -0.0836706
   w_50 = 0.0355871 w_75 = 0.159377 w_95 = 0.403266 whi = 2.61627
layer = 1 wlo = -1.56276 w_05 = -0.346325 w_25 = -0.123086
   w_50 = -0.00198429 w_75 = 0.138039 w_95 = 0.470898 whi = 1.75089
layer = 2 wlo = -3.11902 w_05 = -0.0498786 w_25 = 1.10344
   w_50 = 1.97315 w_75 = 2.67847 w_95 = 3.30435 whi = 5.04899

143 minutes

*.  Z=2 levels with H1 = 300, H2 = 100, learning_rate =
1E-4, gamma = 0.5 and rmsprop_decay = 0.9 .   Loss function trending 
downwards after 3E6 time steps.  Turn frac slowly trending upwards after
3E6 time steps.  Running reward barely trending upwards after 3E6 time
steps.

*.  Z=2 levels with H1 = 300, H2 = 80, learning_rate =
1E-4, gamma = 0.5 and rmsprop_decay = 0.85, tmax = 10**7.  Ran for 5.9
hours on m6700 laptop.  Running reward asymptotes around 0.  Loss function
decreases till t = 8E6.  Turns function asymptotes around 1 around t =
7E6.  

------------------------------------------
Z = 3 levels
------------------------------------------

*.  Z=3 levels with H1 = 400, learning_rate = 1E-4, gamma = 0.5 and
rmsprop_decay = 0.85.  Ran for 1.8 hours on m6700 laptop.  Turns frac
barely reached 0.8.  Loss seems to asymptotes to 2.5.  One hidden layer
with 400 nodes seems clearly insufficient.

*.  Z=3 levels with H1 = 700, learning_rate = 1E-4, gamma = 0.5 and
rmsprop_decay = 0.85.  After running for 3.4 hours on m6700 laptop, turns
frac never got beyond 0.8.  Loss again asymptotes to 2.5. 

We conclude one hidden layer CANNOT learn to avoid placing pieces into
already occupied cells for Z=3 (and most likely Z=4) levels.

------------------------------------------
Z = 4 levels
------------------------------------------

1.  Z=4 levels with H1 = 300, H2 = 64, learning_rate = 3E-4, gamma = 0.5
and rmsprop_decay = 0.85.  Running on m6700 laptop.  Loss function drops
from 4 down to around 1.75 after 60E6 time steps.  Turn frac approaches a
local maximum around 4E5 episodes, a local minimum around 3.4E6 episodes
and then starts to slowly rise.  But local max frac is only 0.5.  So we
terminated this experiment after 5E6 episodes.  

2.  Z=4 levels with H1 = 256, H2 = 64, learning_rate = 1E-4, gamma = 0.5
and rmsprop_decay = 0.85.  Running on m6700 laptop.  Turn frac reaches
local maximum around 0.55 near 1.8E6 episodes and then turns downwards.
Loss function seems to continue to decline slowly past 4E6 time steps.
Terminate this experiment after 4.5E6 time steps.

Note:  It's very possible that we actually did NOT have H2=64 set in this
last experiment.  Instead, it may have run with just a single layer with H1
= 256 nodes...


3.  Z=4 levels with H1 = 300, H2 = 64, learning_rate = 1E-4, gamma = 0.5
and rmsprop_decay = 0.85.  Running on Thinkmate.  After 1.5E7 time steps,
loss function is roughly 0.9 and continuing to slowly drop.  After 300E6
time steps, loss function appears to be around 0.5.  Turn frac is roughly
0.71 after 10E6 episodes.  It then dipped significantly downwrads for 1E6
episodes but then rose back up to around 0.71 again and stayed around there
till 15E6 episodes.

*** As of 6:30 am on Mon Oct 24, #3 is our best z=4 level experiment ***

4.  Z=4 levels with H1 = 300, H2 = 100, learning_rate = 1E-4, gamma = 0.5
and rmsprop_decay = 0.85.  Running on Thinkmate.  After 8E6 episodes, loss
is around 1.5 and slowly decreasing.  Turns frac approaches 0.5 after 1E5
episodes and extremely slowly rises to perhaps 0.55 after 5E5 episodes.
Expt 4 seems overall worse than expt 3.

5.  Z=4 levels with H1 = 300, H2 = 32, learning_rate = 1E-4, gamma = 0.5
and rmsprop_decay = 0.85.  Running on Thinkmate.  Turns frac reaches
maximum plateau value near 0.6 around 2.5E5 episodes.  Then it starts to
fall down to 0.4 around 5.5E5.  Slowly starts rising again around 8E5
episodes.  Loss falls to around 1 up till 80E6 time steps.  Then starts
rising again before starting to fall.  Expt 5 seems overall worse than expt
3 except for speed with which expt 5 initially improves.

6.  Z=4 levels with H1 = 300, H2 = 64, learning_rate = 3E-5, gamma = 0.5
and rmsprop_decay = 0.85.  Running on m6700.  After 9E6 episodes, loss
function is very slowly decreasing.  Turn frac is also approaching 0.6 and
very slowly increasing.  Experiment #6 is a slower version of expt #3.  

-----------------
7:30 am on Mon Oct 24 

7.  Z=4 levels with H1 = 300, H2 = 64, learning_rate = 1E-4, gamma = 0.5
and rmsprop_decay = 0.75.  Started running on Thinkmate around 7:35 am.
After 5E6 episode, turns frac has asymptoted below 0.5.  After 60E6 time
steps, loss is around 1.5.  This is definitely worse than expt #3.
Terminated.

8.  Z=4 levels with H1 = 64, H2 = 64, H3 = 64, learning_rate = 1E-4, gamma
= 0.5 and rmsprop_decay = 0.85.  Started running on Thinkmate around 7:50 am
After 6E6 episodes, turn frac is around 0.4 after having risen closer to
0.5 around 3E6 episodes.  Loss has falled to around 1 after 80E6 steps.
This seems like an unpromising architecture.  Terminated.

9.  Z=4 levels with H1 = 128, H2 = 64, H3 = 32, learning_rate = 1E-4, gamma
= 0.5 and rmsprop_decay = 0.85.  Started running on m6700 at 7:42 am.
After running for 4 hours, turns frac appears to asymptote around 0.52.
Loss function drops to around 1 and may continue to slowly fall. But this
expt's results seem significantly worse than those of expt 3. Terminated.

layer = 0 wlo = -2.03131 w_05 = -0.424748 w_25 = -0.146209
   w_50 = -0.00493972 w_75 = 0.140775 w_95 = 0.390811 whi = 2.61713
layer = 1 wlo = -1.57167 w_05 = -0.408438 w_25 = -0.0678079
   w_50 = 0.116902 w_75 = 0.317409 w_95 = 0.663183 whi = 1.69959
layer = 2 wlo = -1.72718 w_05 = -0.611286 w_25 = -0.191232
   w_50 = -0.010166 w_75 = 0.201483 w_95 = 0.662293 whi = 2.44698
layer = 3 wlo = -1.16897 w_05 = 0.0744037 w_25 = 2.99314
   w_50 = 4.60082 w_75 = 6.35227 w_95 = 8.30674 whi = 9.32921

10.  Z=4 levels with H1 = 256, H2 = 128, H3 = 64, learning_rate = 1E-4,
gamma = 0.5 and rmsprop_decay = 0.85.  Started running on Thinkmate around
7:50 am.  After nearly 2E6 episodes, turn frac has risen to 0.5.  After
25E6 timesteps, loss is around 1.25.  This seems more promising than expts
#8 and #9.  Terminated.

11.  Z=4 levels with H1 = 300, H2 = 200, H3 = 100, learning_rate = 1E-4,
gamma = 0.5 and rmsprop_decay = 0.85.  Started running on m6700 at 11 am.
As of 2 pm, it looks like turn frac has asymptoted around 0.5 and loss has
asymptoted around 1.3.  Clearly much worse than expt 3.  Terminated.

12.  Z=4 levels with H1 = 300, H2 = 200, H3 = 100, learning_rate = 3E-4,
gamma = 0.5 and rmsprop_decay = 0.85.  Started on Thinkmate around 11:13
am.  After 800K episodes, turn frac is around 0.5.  Remains around 0.5 for
4E6 episodes.  Loss is around 0.9 after 10E6 time steps.  Running as of
1:50 pm on Mon Oct 24.  Clearly worse than expt #3 results.  Terminated.

13.  Z=4 levels with H1 = 300, H2 = 150, H3 = 64, learning_rate = 1E-4,
gamma = 0.5 and rmsprop_decay = 0.85.  Started on Thinkmate around 11:15
am.  After 1E6 episodes, turn frac is around 0.55 and increasing.  After
4E6 episodes, turn frac asymptotes around 0.6.  After 10E6 time steps, loss
is around 2.25.  Clearly worse than expt #3 results.  Running as of 1:50 pm
on Mon Oct 24.  Terminated.

14.  Z=4 levels with H1 = 64, H2 = 300, learning_rate = 1E-4, gamma = 0.5
and rmsprop_decay = 0.85.  Started on Thinkmate around 1:57 pm.  After 5E6
episodes, turn frac approaches 0.6.  Then jumps down to 0.4 and climbs back
up towards 0.45 around 9E6 episodes.  Clearly worse than expt #3 results.
Terminated.

15.  Z=4 levels with H1 = 64, H2 = 128, H3 = 256, learning_rate = 1E-4,
gamma = 0.5 and rmsprop_decay = 0.85.  Started on Thinkmate around 1:59 pm
Turn frac reaches max value around 0.48 after 4E6 episodes.  Then starts
falling again.  Clearly worse than expt#3 results.  Terminated.

16.  Copy of expt 3 with new batch size: bsize = 30, Z=4 levels with H1 =
300, H2 = 64, learning_rate = 1E-4, gamma = 0.5 and rmsprop_decay = 0.85.
Started running on Thinkmate at 10:50 pm on Mon Oct 24.  After 5E6
episodes, turn frac is approx 0.82 and slowly climbing!  After 12E6, turn
frac is around 0.95 and maybe still climbing.  Loss function after
10E6/28E6 time steps is approximately 1.25 and slowly decreasing.  Still
running on TM as of Tues, Oct 25 at 6:46 am .  Terminated

*** As of 6:14 am on Tues, Oct 25, #16 is our best z=4 level experiment ***

17.  Copy of expt 3 with new batch size: bsize = 100, Z=4 levels with H1 =
300, H2 = 64, learning_rate = 1E-4, gamma = 0.5 and rmsprop_decay = 0.85.
Started running on Thinkmate at 10:55 pm on Mon Oct 24.  After 5E6
episodes, turn frac is approx 0.75 and slowly climbing.  After 12E6
episodes, turn frac is approx 0.92 and still slowly climbing.  Loss
function after 10E6 /28E6 time steps is approximately 1.5 and perhaps
slowly decreasing.  Still running on TM as of Tues, Oct 25 at 6:46 am.
Terminated.

18.  Z=4 levels with H1 = 32, H2 = 64, H3 = 128, learning_rate = 1E-4,
gamma = 0.5 and rmsprop_decay = 0.85, batch_size = 10.  Started on
m6700 at 11 pm on Mon Oct 24.  Turn frac reaches a max value around 0.4
near 8E5 episodes and then starts to decline.  This is horrible compared to
experiment 3.  Terminated.

19.  Copy of expt 3 with lambda=1E-3, Z=4 levels with H1 =
300, H2 = 64, learning_rate = 1E-4, gamma = 0.5 and rmsprop_decay = 0.85.
Started running on m6700 at 4:55 am on Tues Oct 25.  Turn frac never
increases much beyond 0.25.  So lambda=1E-3 is much too large!  Terminated.

20.  Copy of expt 3 with lambda=1E-4, Z=4 levels with H1 = 300, H2 = 64,
learning_rate = 1E-4, gamma = 0.5, rmsprop_decay = 0.85, batch_size = 10.
Started running on m6700 at 5:14 am on Tues Oct 25.  Turn frac around 0.75
after 8E5 episodes.  These results are fairly similar to those for expt #3
which had lambda=0.  Terminated.

21.  Copy of expt 16 with variable learning rate: Multiply learning rate by
0.95 after every 150,000 episodes.  bsize = 30, Z=4 levels with H1 = 300,
H2 = 64, base learning_rate = 3E-4, gamma = 0.5 and rmsprop_decay = 0.85.
Started running on Thinkmate at 6:45 am on Tues, Oct 25.  Turn frac
asymptotes to approximately 0.88 around 4E6 episodes.  So results are not
quite as good as expt 16.  Terminated.

22.  Copy of expt 21 with variable learning rate and smaller gamma:
Multiply learning rate by 0.95 after every 150,000 episodes.  bsize = 30,
Z=4 levels with H1 = 300, H2 = 64, base learning_rate = 3E-4, gamma = 0.4
and rmsprop_decay = 0.85.  Started running on Thinkmate at 6:45 am on Tues,
Oct 25.  Turn frac asymptotes around 0.92 around 5E6 episodes.  So these
results are better than those for expt 21 but still not as good as expt 16.
Terminated.

23.  Copy of expt 21 with variable learning rate and smaller gamma:
Multiply learning rate by 0.95 after every 150,000 episodes.  bsize = 30,
Z=4 levels with H1 = 300, H2 = 64, base learning_rate = 3E-4, gamma = 0.2
and rmsprop_decay = 0.85.  Started running on Thinkmate around noontime on
Tues, Oct 25.   After 3.5E6 episodes, turn frac is approximately 0.897 and
perhaps still slowly increasing.  Terminated.


24.  Base learning rate = 3E-4.  Multiply learning rate by 0.95 after every
150,000 episodes; min learning rate = 3E-5.  bsize = 30, Z=4 levels with H1
= 300, H2 = 64, gamma = 0.5 and rmsprop_decay = 0.85.  Started running on
Thinkmate around 5:50 pm on Tues, Oct 25. After 6E6 episodes, turn frac
asymptotes to 0.89.  Definitely not as good as expt #25.   Terminated.

25.  Base learning rate = 3E-4.  Multiply learning rate by 0.95 after every
150,000 episodes; min learning rate = 3E-5.  bsize = 30, Z=4 levels with H1
= 300, H2 = 64, gamma = 0.25 and rmsprop_decay = 0.85.  Started running on
Thinkmate around 5:50 pm on Tues, Oct 25.  After 6E6 episodes, turn frac
asymptotes to 0.92.  Terminated.

layer = 0 wlo = -2.52149 w_05 = -0.207983 w_25 = -0.0441196
   w_50 = 0.0352673 w_75 = 0.114168 w_95 = 0.260796 whi = 2.4514
layer = 1 wlo = -1.65326 w_05 = -0.412146 w_25 = -0.141895
   w_50 = 0.000682767 w_75 = 0.169127 w_95 = 0.547689 whi = 1.73701
layer = 2 wlo = -6.44991 w_05 = 0.410817 w_25 = 1.79034
   w_50 = 2.12907 w_75 = 2.4182 w_95 = 3.06846 whi = 5.55676

*** As of 3:15 am on Weds, Oct 27, expt #25 is our best z=4 level
experiment ***


26.  Base learning rate = 3E-4.  Multiply learning rate by 0.95 after every
150,000 episodes; min learning rate = 3E-5.  bsize = 30, Z=4 levels with H1
= 300, H2 = 64, gamma = 0.10 and rmsprop_decay = 0.85.  Started running on
Thinkmate around 5:51 pm on Tues, Oct 25.  After 6E6 episodes, turn frac
asymptotes to 0.90.  Not quite as good as expt 25.  Terminated.

27.  Base learning rate = 3E-4.  Multiply learning rate by 0.95 after every
150,000 episodes; min learning rate = 3E-5.  bsize = 30, Z=4 levels with H1
= 300, H2 = 64, gamma = 0.25 and rmsprop_decay = 0.85.  Rescale reward by
n_AI_turns + n_agent_turns.  Started running on m6700 around 6 pm on Tues,
Oct 25.  Turn frac asymptotes to 0.92.  Terminated.

28.  Base learning rate = 3E-4.  Multiply learning rate by 0.95 after every
250,000 episodes; min learning rate = 3E-5.  bsize = 30, Z=4 levels with H1
= 300, H2 = 64, gamma = 0.25 and rmsprop_decay = 0.85.  Rescale reward by
n_AI_turns + n_agent_turns.  Started running on m6700 around 8 pm on Tues,
Oct 25.  Turn frac asymptotes to 0.94.  Better turn frac results than expt
27.  Terminated.




29.  Base learning rate = 3E-3.  Multiply learning rate by 0.8 after every
100,000 episodes; min learning rate = 3E-5.  bsize = 30, Z=4 levels with H1
= 300, H2 = 64, gamma = 0.25 and rmsprop_decay = 0.85.  Started running on
m6700 around 4:14 am on Weds, Oct 26.

30.  Base learning rate = 3E-3.  Multiply learning rate by 0.8 after every
100,000 episodes; min learning rate = 3E-5.  bsize = 30, Z=4 levels with H1
= 300, H2 = 64, gamma = 0.33 and rmsprop_decay = 0.85.  Started running on
m6700 around 4:15 am on Weds, Oct 26.

31.  Base learning rate = 3E-3.  Multiply learning rate by 0.95 after every
100,000 episodes; min learning rate = 3E-5.  bsize = 30, Z=4 levels with H1
= 300, H2 = 64, gamma = 0.25 and rmsprop_decay = 0.90.  Started running on
m6700 around 4:16 am on Weds, Oct 26.


32.  Base learning rate = 2E-3.  Multiply learning rate by 0.8 after every
n_episodes_period = 100,000 episodes ; also increase n_episodes_period by
1.2; min learning rate = 3E-5.  bsize = 30, Z=4 levels with H1 = 300, H2 =
64, gamma = 0.25 and rmsprop_decay = 0.85.  Started running on Thinkmate
around 5:30 am on Weds, Oct 26.


33.   Base learning rate = 2E-3.  Multiply learning rate by 0.8 after every
n_episodes_period = 100,000 episodes ; also increase n_episodes_period by
1.2; min learning rate = 3E-5.  bsize = 30, Z=4 levels with H1 = 300, H2 =
64, gamma = 0.25 and rmsprop_decay = 0.85.  Game over if AI/agent randomly
wins or loses.  Reward = -2 if AI wins; Reward = -1 if illegal move.

Started running on Thinkmate around 6:57 am on Weds, Oct 26.  Terminated.



34.   Base learning rate = 2E-3.  Multiply learning rate by 0.8 after every
n_episodes_period = 100,000 episodes ; also increase n_episodes_period by
1.2; min learning rate = 3E-5.  bsize = 30, Z=4 levels with H1 = 300, H2 =
64, gamma = 0.25 and rmsprop_decay = 0.85.  Game over if AI/agent randomly
wins or loses.  Reward = -1 if AI wins; Reward = -2 if illegal move.

Started running on Thinkmate around 6:59 am on Weds, Oct 26.  TErminated.

35.   Base learning rate = 2E-3.  Multiply learning rate by 0.8 after every
n_episodes_period = 100,000 episodes ; also increase n_episodes_period by
1.2; min learning rate = 3E-5.  bsize = 30, Z=4 levels with H1 = 300, H2 =
64, gamma = 0.25 and rmsprop_decay = 0.85.  Game over if AI/agent randomly
wins or loses.  Reward = -1 if AI wins; Reward = -2 if illegal move.  Check
for genuine game wins/losses in only 50% of episodes to better teach agent
to not make illegal moves.

Started running on Thinkmate around 6:59 am on Weds, Oct 26.  Terminated.


36.  Base learning rate = 1E-3.  Multiply learning rate by 0.8 after every
n_episodes_period = 100,000 episodes ; also increase n_episodes_period by
1.2; min learning rate = 5E-5.  bsize = 30, Z=4 levels with H1 = 300, H2 =
64, gamma = 0.25 and rmsprop_decay = 0.85.  Game over if AI/agent randomly
wins or loses.  Reward = -1 if AI wins; Reward = -2 if illegal move.  Train
for 1E6 episodes with no genuine win/loss game termination.  Then check for
genuine game wins/losses in only 50% of episodes to better teach agent to
not make illegal moves.

Started running on Thinkmate around 8:03 am on Weds, Oct 26.  Terminated.


37.  Trying to reproduce latest best expt #25.  But set H1 = 5 * 64 = 320 rather than 300.  Started running on TM at 10:25 am.

39.  RI agent learns against random AI on genuine 3D TTT.  Action probs are
hacked so that they cannot yield illegal agent moves.  Running reward
approaches 1 after roughly 60K episodes.


40.  RI agent learns against random AI on genuine 3D TTT.  agent_value = 1,
AI_value = -1.  Running reward converges to 1 after 100K episodes.

41.  RI agent learns against random AI on genuine 3D TTT.  Agent and AI
values switch between 1 & -1 every 10K episodes.

42.  RI agent learns against 1-round AI.  Agent and AI values switch
between 1 and -1 every 10K episodes.  Started on TM around 7 am.
Terminated.

43.  RI agent learns against random AI.  AI = X, agent = O.  Starting
player switches every 10K episodes.  Started on TM around 7:20 am.  Running
reward converges to 1 after 90K episodes.  Terminated.

44. Fill grid for Z=1 level.  H1=300, H2 = 64.  Reward --> 0.8 after 1E6
episodes.

49.  Z = 1 TTT, H1 = 320 learning against random AI.  learning rate = 3E-4,
gamma = 0.99, rms_decay = 0.85.  Reward --> 0.8 after 1E6 episodes.  Win
frac --> 0.8, Stalemat frac --> 0.05, Loss frac --> 0.15.  

*** Best Z=1 TTT as of Sun Oct 30 at 4 am ***

50-52.  Z=1 TTT. H1 = 300, H2 = 64.  base learning rate = 3E-4, 1E-3,
3E-3.  All results are MUCH worse than for a single-layer network with H1 = 300.

53 - 55.  Z=1 TTT, H1 = 5*64. H2 = 0.  base learning rate = 1E-4, 1E-3 and
3E-3.  gamma = 0.99, rms_decay = 0.85.  All 3 of these expts yield worse
running reward and game history results than expt 49.


56 - 58: Z = 1 TTT, H1 = 3, 5, 7 * 64, H2 = 0. base learning rate = 3E-4.
gamma = 0.99, rms_decay = 0.85.  Finished running on m6700.  H1 = 5 * 64
yields best results.  Consistent with expt 49.  

59 - 61:  Z = 4 TTT, H1 = 5, 3, 1 * 64, H2 = 1, 3 , 5 * 64, base learning
rate = 3E-4, gamma = 0.99, rms_decay = 0.85.  Started on TM around 5:20 am,
Sun Oct 30.  

64.  Z = 4 TTT, H1 = 320, H2 = 64, base learning rate = 3E-4, gamma = 0.99,
rms_decay = 0.85.  Batch size = 30. 

Round 0:  running reward --> 1
	  win frac --> 0.97
  	  lose frac --> 0.03

Round 1:  running reward --> 0.457
          win frac --> 0.80
	  lose frac --> 0.20

Round 2:  Restarted on TM at 11:38 am
	  running reward --> 1.0
	  win frac --> 0.93
	  lose frac --> 0.07

Round 3:  Started on TM at 12:03 pm
	  After 150K episodes, running reward --> 1
	  win frac --> 0.95
	  lose frac --> 0.05

Round 4:  Started on TM at 12:33 pm
	

64b.  Z = 4 TTT, H1 = 320, H2 = 64, base learning rate = 3E-4, gamma =
0.99, rms_decay = 0.85.  Batch size = 60.  Started training against round 2
snapshot from expt 64 on TM at 12:05 pm.  Worse performance than expt 64.
Terminated.

64c.  Z = 4 TTT, H1 = 320, H2 = 64, base learning rate = 3E-4, gamma =
0.99, rms_decay = 0.85.  Batch size = 120.  Started training against round
2 snapshot from expt 64 on TM at 12:06 pm.  Worse performance than expt 64.
Terminated.

64d.  Z = 4 TTT, H1 = 320, H2 = 64, base learning rate = 1E-4, gamma = 0.99,
rms_decay = 0.85.  Batch size = 30. Started training against round 3
snapshot from expt 64 on TM at 12:38 pm

64e.  Z = 4 TTT, H1 = 320, H2 = 64, base learning rate = 1E-3, gamma = 0.99,
rms_decay = 0.85.  Batch size = 30. Started training against round 3
snapshot from expt 64 on TM at 12:38 pm.  

	After 250K episodes, running reward --> 1
	win frac --> 0.94
	lose frac --> 0.06

*** As of 1 pm on Sun Oct 30, this is our best trained model for 3D TTT
with Z=4 levels ***


65.  Z = 4 TTT, H1 = 3 * 64, H2 = 3 * 64, base learning rate = 3E-4, gamma
= 0.99, rms_decay = 0.85.  Started training against round 1 snapshot from
expt 64 on TM at 11:27 am Worse performance than expt 64.  Terminated.

66.  Z = 4 TTT, H1 = 1 * 64, H2 = 5 * 64, base learning rate = 3E-4, gamma
= 0.99, rms_decay = 0.85.  Started training against round 1 snapshot from
expt 64 on TM at 11:28 am.  Worse performance than expt 64.  Terminated.


70.  Z = 4 TTT, H1 = 5 * 64, H2 = 64, base learning rate = 3E-2, gamma =
0.99, rms_decay = 0.85.  Play against random AI.  Agent rapidly beats
random player.  Terminated.

71.  Z = 4 TTT, H1 = 5 * 64, H2 = 64, base learning rate = 3E-3, gamma =
0.99, rms_decay = 0.85.  Play against random AI. Agent rapidly learns to
beat random player.  Terminated.

72.  Z = 4 TTT, H1 = 5 * 64, H2 = 64, base learning rate = 3E-4, gamma =
0.99, rms_decay = 0.85.  Play against random AI.  Agent takes a while to
learn to beat random player.  Terminated.

73.  Z = 4 TTT, random AI vs random agent.  Sanity check seems OK as of
1:40 pm on Thurs Nov 3.  Random player wins roughly 50% of their games
against another random player.  Terminated.


74.  Z = 4 TTT, H1 = 5 * 64, H2 = 64, base learning rate = 3E-4, gamma =
0.99, rms_decay = 0.85.  Play against level-0 AI.  Running on TM as of 3 pm
Thurs Nov 3.  AS of 8:30 am on Fri Nov 4, agent actually started to win a
few games against AI around 2E6 episodes!  Running reward also started
barely inching away from -2 around 2E6 episodes.  

75.  Z = 4 TTT, H1 = 5 * 64, H2 = 64, base learning rate = 3E-3, gamma =
0.99, rms_decay = 0.85.  Play against level-0 AI.    Running on TM as of 3
pm Thurs Nov 3.  Definitely worse than expt 74.  Terminated.

76.  Z = 4 TTT, H1 = 5 * 64, H2 = 64, base learning rate = 3E-2, gamma =
0.99, rms_decay = 0.85.  Play against level-0 AI.   Terminated.

77.  Z = 4 TTT, H1 = 5 * 64, H2 = 64, base learning rate = 1E-4, gamma =
0.99, rms_decay = 0.85.  Play against level-0 AI.    Running on TM as of 3
pm Thurs Nov 3.  Definitely not as good as expt 74.  Terminated.

78.  Z = 4 TTT, H1 = 5 * 64, H2 = 64, base learning rate = 1E-3, gamma =
0.99, rms_decay = 0.85.  Play against level-0 AI.  Batch size = 30.
Running on TM as of 3 pm Thurs Nov 3.  Definitely not as good as expt 74.
Terminated.


79.  Z = 4 TTT, H1 = 5 * 64, H2 = 64, base learning rate = 1E-3, gamma =
0.99, rms_decay = 0.85.  Play against level-0 AI.  Batch size = 10.
Running on m6700 as of 5:30 pm Thurs Nov 3.  Number of turns definitely
advances beyond 4.  *Tiny* movements of running reward away from -2 min
value also observed.  As of 6:50 am on Fri Nov 4, this expt is our best
result seen on m6700 for Z=4 level TTT against a level-0 AI.

80.  Z = 4 TTT, H1 = 5 * 64, H2 = 64, base learning rate = 1E-3, gamma =
0.99, rms_decay = 0.85.  Play against level-0 AI. Batch size = 3.  Results
not as good as those for expt 79 with batch size = 10.  Terminated.

81.  Z = 4 TTT, H1 = 5 * 64, H2 = 64, base learning rate = 1E-3, gamma =
0.99, rms_decay = 0.85.  Play against level-0 AI.  Batch size = 100.  Very
bad results.  Terminated.

82.  Z = 4 TTT, H1 = 5 * 64, H2 = 64, base learning rate = 1E-3, gamma =
0.90, rms_decay = 0.85.  Play against level-0 AI.  Batch size = 10.
Started on m6700 at 4:20 am on Fri, Nov 4.  No dramatic improvements
relative to expt 79 observed as of 6:50 am.

83.  Z = 4 TTT, H1 = 5 * 64, H2 = 64, base learning rate = 1E-3, gamma =
0.50, rms_decay = 0.85.  Play against level-0 AI.  Batch size = 10.
Started on m6700 at 4:20 am on Fri, Nov 4.  No dramatic improvements
relative to expt 79 observed as of 6:50 am .

84.  Z = 4 TTT, H1 = 5 * 64, H2 = 64, base learning rate = 3E-4, gamma =
0.99, rms_decay = 0.85.  Play against level-0 AI.  Implement Delsey's
"check" idea.  Started on TM at 9:30 am on Fri Nov 4.

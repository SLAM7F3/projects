==========================================================================
Reinforcement learning notes
==========================================================================
Last updated on 10/19/16; 10/20/16; 10/21/16
==========================================================================

*.  On 10/19/16, we found the following set of hyperparameters which yields
perfect agent learning to not place any of its TT pieces into already
occupied cells in a 4x4 grid:

   batch_size = 10;	// Perform parameter update after this many episodes
    learning_rate = 3E-4;  // Much better than 1E-4 !!!
   lambda = 0.0;	// L2 regularization coefficient (better than 1E-3)
   gamma = 0.90;	// Discount factor for reward
   rmsprop_decay_rate = 0.95; 
   

   int nsize = 4;
   int n_zlevels = 1;
   tictac3d* ttt_ptr = new tictac3d(nsize, n_zlevels);

   int Din = nsize * nsize;	// Input dimensionality
   int H = 200;			// Number of hidden layer neurons
   int Dout = nsize * nsize;	// Output dimensionality
   int Tmax = nsize * nsize * n_zlevels;

   vector<int> layer_dims;
   layer_dims.push_back(Din);
   layer_dims.push_back(H);
   layer_dims.push_back(Dout);
   reinforce* reinforce_ptr = new reinforce(layer_dims, Tmax)

episode_number = 620000
  T = 8 +/- 0
  Running reward mean = 1
n_filled_cells = 16

 O | X | O | O 
----------------
 O | X | X | X 
----------------
 O | X | X | O 
----------------
 X | O | O | X 

*.  On 10/20/16, we started experimenting with filling Z=2 levels of 4x4
grids.  

learning_rate = 1E-3 is much worse than learning rate 3E-4

learning_rate = 5.196E-4 definitely asymptotes to worse reward than 3E-4

learning rate = 3E-4 yields quasi-decent but not great reward results
		for 2 z-levels

learning_rate = Reward results reach same asymoptotic value as for 3E-4.
		But learning is considerably slower than for 3E-4.

learning_rate = 1E-4 is much worse than learning rate 3E-4

Discount rate = 0.7 yields faster (but not better) convergence to
asymptotic running reward sum around 0.4 than discount rate = 0.9.
Discount rate = 0.5 yields even faster (but also no better) convergence
than gamma = 0.7

Running reward --> 1 for Z=2 levels with learning_rate = 0.003, gamma = 0.5
and rmsprop_decay = 0.9 !!!

Z=2 levels with two hidden layers containing 128 nodes, learning_rate =
0.003, gamma = 0.5 and rmsprop_decay = 0.9 yields terrible results.



------------------------------------------
Z=2 levels with H1 = 256, H2 = 128, learning_rate =
0.0010, gamma = 0.5 and rmsprop_decay = 0.9.  Turns history rises and then
falls  - definite worse than learning_rate = 0.0003 !!!


Z=2 levels with H1 = 256, H2 = 128, learning_rate =
0.0003, gamma = 0.5 and rmsprop_decay = 0.9: n_turns_frac --> 0.86


Z=2 levels with H1 = 256, H2 = 128, learning_rate =
0.0001, gamma = 0.5 and rmsprop_decay = 0.9 .  Comparable performance to
learning_rate = 3E-4.



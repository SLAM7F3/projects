==========================================================================
Reinforcement learning notes
==========================================================================
Last updated on 10/20/16; 10/21/16; 10/22/16; 10/23/16; 10/24/16
==========================================================================

*.  On 10/19/16, we found the following set of hyperparameters which yields
perfect agent learning to not place any of its TT pieces into already
occupied cells in a 4x4 grid:

   batch_size = 10;	// Perform parameter update after this many episodes
    learning_rate = 3E-4;  // Much better than 1E-4 !!!
   lambda = 0.0;	// L2 regularization coefficient (better than 1E-3)
   gamma = 0.90;	// Discount factor for reward
   rmsprop_decay_rate = 0.95; 
   

   int nsize = 4;
   int n_zlevels = 1;
   tictac3d* ttt_ptr = new tictac3d(nsize, n_zlevels);

   int Din = nsize * nsize;	// Input dimensionality
   int H = 200;			// Number of hidden layer neurons
   int Dout = nsize * nsize;	// Output dimensionality
   int Tmax = nsize * nsize * n_zlevels;

   vector<int> layer_dims;
   layer_dims.push_back(Din);
   layer_dims.push_back(H);
   layer_dims.push_back(Dout);
   reinforce* reinforce_ptr = new reinforce(layer_dims, Tmax)

episode_number = 620000
  T = 8 +/- 0
  Running reward mean = 1
n_filled_cells = 16

 O | X | O | O 
----------------
 O | X | X | X 
----------------
 O | X | X | O 
----------------
 X | O | O | X 

*.  On 10/20/16, we started experimenting with filling Z=2 levels of 4x4
grids.  

learning_rate = 1E-3 is much worse than learning rate 3E-4

learning_rate = 5.196E-4 definitely asymptotes to worse reward than 3E-4

learning rate = 3E-4 yields quasi-decent but not great reward results
		for 2 z-levels

learning_rate = Reward results reach same asymoptotic value as for 3E-4.
		But learning is considerably slower than for 3E-4.

learning_rate = 1E-4 is much worse than learning rate 3E-4

Discount rate = 0.7 yields faster (but not better) convergence to
asymptotic running reward sum around 0.4 than discount rate = 0.9.
Discount rate = 0.5 yields even faster (but also no better) convergence
than gamma = 0.7

Running reward --> 1 for Z=2 levels with learning_rate = 0.003, gamma = 0.5
and rmsprop_decay = 0.9, H1 = 300 !!!  Turns history frac --> 1

Running reward --> 0.6 for Z=2 levels with learning_rate = 0.003, gamma = 0.5
and rmsprop_decay = 0.9, H1 = 256.  This is definitely not as good as H1 =
300 !

Z=2 levels with two hidden layers containing 128 nodes, learning_rate =
0.003, gamma = 0.5 and rmsprop_decay = 0.9 yields terrible results.

------------------------------------------
Z=2 levels with H1 = 256, H2 = 128, learning_rate =
0.0010, gamma = 0.5 and rmsprop_decay = 0.9.  Turns history rises and then
falls  - definite worse than learning_rate = 0.0003 !!!


Z=2 levels with H1 = 256, H2 = 128, learning_rate =
0.0003, gamma = 0.5 and rmsprop_decay = 0.9: n_turns_frac --> 0.86

Z=2 levels with H1 = 256, H2 = 128, learning_rate =
0.0001, gamma = 0.5 and rmsprop_decay = 0.9 .  Comparable performance to
learning_rate = 3E-4.

------------------------------------------
Z = 2 levels
------------------------------------------

*.  Z=2 levels with H1 = 300, H2 = 300, learning_rate =
0.0003, gamma = 0.5 and rmsprop_decay = 0.9 .   Turns history frac
asymptotes to 0.8 and definitely not 1!

*.  Z=2 levels with H1 = 300, H2 = 150, learning_rate =
0.0003, gamma = 0.5 and rmsprop_decay = 0.9 .   Turns history frac
asymptotes to 0.8.  

layer = 0 wlo = -2.1208 w_05 = -0.302912 w_25 = -0.0958052
   w_50 = 0.0241359 w_75 = 0.193846 w_95 = 0.437919 whi = 3.84919
layer = 1 wlo = -1.39498 w_05 = -0.365861 w_25 = -0.128735
   w_50 = 0.0142132 w_75 = 0.160051 w_95 = 0.41266 whi = 1.56651
layer = 2 wlo = -2.33946 w_05 = -0.115958 w_25 = 0.359509
   w_50 = 1.10251 w_75 = 1.45926 w_95 = 1.82759 whi = 3.02317
48.74 minutes 

*.  Z=2 levels with H1 = 300, H2 = 150, learning_rate =
0.0001, gamma = 0.5 and rmsprop_decay = 0.9 .   Turns history frac
asymptotes to 0.86.  Better than learning_rate = 3E-4.

layer = 0 wlo = -1.49288 w_05 = -0.392911 w_25 = -0.110789
   w_50 = 0.00682151 w_75 = 0.117715 w_95 = 0.359205 whi = 1.69722
layer = 1 wlo = -0.733989 w_05 = -0.200402 w_25 = -0.075696
   w_50 = 0.00378144 w_75 = 0.0886445 w_95 = 0.240782 whi = 0.691564
layer = 2 wlo = -0.80083 w_05 = 0.340652 w_25 = 0.529501
   w_50 = 0.659939 w_75 = 0.813973 w_95 = 1.11432 whi = 1.43289
46.36 minutes 

*.  Z=2 levels with H1 = 300, H2 = 150, learning_rate =
3E-5, gamma = 0.5 and rmsprop_decay = 0.9 .   Turns history frac
asymptotes to 0.86.  

*.  Z=2 levels with H1 = 300, H2 = 150, learning_rate =
1E-5, gamma = 0.5 and rmsprop_decay = 0.9 .   Turns history frac
reaches 0.6 after 1E6 episodes and appears to keep climbing.

layer = 0 wlo = -1.02333 w_05 = -0.351843 w_25 = -0.117336
   w_50 = -0.000639077 w_75 = 0.102129 w_95 = 0.335207 whi = 1.11941
layer = 1 wlo = -0.431056 w_05 = -0.130479 w_25 = -0.0465206
   w_50 = 0.0104769 w_75 = 0.067586 w_95 = 0.153783 whi = 0.418167
layer = 2 wlo = -0.545656 w_05 = 0.0155456 w_25 = 0.153152
   w_50 = 0.246818 w_75 = 0.335681 w_95 = 0.441951 whi = 0.634136
39 minutes

*.  Z=2 levels with H1 = 300, H2 = 150, learning_rate =
1E-4, gamma = 0.5 and rmsprop_decay = 0.8 .   Loss function trending 
downwards after 3E6 time steps.  Turn frac slowly trending upwards after
3E6 time steps.  Running reward barely trending upwards after 3E6 time
steps.  

layer = 0 wlo = -1.54267 w_05 = -0.401816 w_25 = -0.0836706
   w_50 = 0.0355871 w_75 = 0.159377 w_95 = 0.403266 whi = 2.61627
layer = 1 wlo = -1.56276 w_05 = -0.346325 w_25 = -0.123086
   w_50 = -0.00198429 w_75 = 0.138039 w_95 = 0.470898 whi = 1.75089
layer = 2 wlo = -3.11902 w_05 = -0.0498786 w_25 = 1.10344
   w_50 = 1.97315 w_75 = 2.67847 w_95 = 3.30435 whi = 5.04899

143 minutes

*.  Z=2 levels with H1 = 300, H2 = 100, learning_rate =
1E-4, gamma = 0.5 and rmsprop_decay = 0.9 .   Loss function trending 
downwards after 3E6 time steps.  Turn frac slowly trending upwards after
3E6 time steps.  Running reward barely trending upwards after 3E6 time
steps.

*.  Z=2 levels with H1 = 300, H2 = 80, learning_rate =
1E-4, gamma = 0.5 and rmsprop_decay = 0.85, tmax = 10**7.  Ran for 5.9
hours on m6700 laptop.  Running reward asymptotes around 0.  Loss function
decreases till t = 8E6.  Turns function asymptotes around 1 around t =
7E6.  

------------------------------------------
Z = 3 levels
------------------------------------------

*.  Z=3 levels with H1 = 400, learning_rate = 1E-4, gamma = 0.5 and
rmsprop_decay = 0.85.  Ran for 1.8 hours on m6700 laptop.  Turns frac
barely reached 0.8.  Loss seems to asymptotes to 2.5.  One hidden layer
with 400 nodes seems clearly insufficient.

*.  Z=3 levels with H1 = 700, learning_rate = 1E-4, gamma = 0.5 and
rmsprop_decay = 0.85.  After running for 3.4 hours on m6700 laptop, turns
frac never got beyond 0.8.  Loss again asymptotes to 2.5. 

We conclude one hidden layer CANNOT learn to avoid placing pieces into
already occupied cells for Z=3 (and most likely Z=4) levels.

------------------------------------------
Z = 4 levels
------------------------------------------

1.  Z=4 levels with H1 = 300, H2 = 64, learning_rate = 3E-4, gamma = 0.5
and rmsprop_decay = 0.85.  Running on m6700 laptop.  Loss function drops
from 4 down to around 1.75 after 60E6 time steps.  Turn frac approaches a
local maximum around 4E5 episodes, a local minimum around 3.4E6 episodes
and then starts to slowly rise.  But local max frac is only 0.5.  So we
terminated this experiment after 5E6 episodes.  

2.  Z=4 levels with H1 = 256, H2 = 64, learning_rate = 1E-4, gamma = 0.5
and rmsprop_decay = 0.85.  Running on m6700 laptop.  Turn frac reaches
local maximum around 0.55 near 1.8E6 episodes and then turns downwards.
Loss function seems to continue to decline slowly past 4E6 time steps.
Terminate this experiment after 4.5E6 time steps.

Note:  It's very possible that we actually did NOT have H2=64 set in this
last experiment.  Instead, it may have run with just a single layer with H1
= 256 nodes...


3.  Z=4 levels with H1 = 300, H2 = 64, learning_rate = 1E-4, gamma = 0.5
and rmsprop_decay = 0.85.  Running on Thinkmate.  After 1.5E7 time steps,
loss function is roughly 0.9 and continuing to slowly drop.  After 300E6
time steps, loss function appears to be around 0.5.  Turn frac is roughly
0.71 after 10E6 episodes.  It then dipped significantly downwrads for 1E6
episodes but then rose back up to around 0.71 again and stayed around there
till 15E6 episodes.

*** As of 6:30 am on Mon Oct 24, #3 is our best z=4 level experiment ***

4.  Z=4 levels with H1 = 300, H2 = 100, learning_rate = 1E-4, gamma = 0.5
and rmsprop_decay = 0.85.  Running on Thinkmate.  After 8E6 episodes, loss
is around 1.5 and slowly decreasing.  Turns frac approaches 0.5 after 1E5
episodes and extremely slowly rises to perhaps 0.55 after 5E5 episodes.
Expt 4 seems overall worse than expt 3.

5.  Z=4 levels with H1 = 300, H2 = 32, learning_rate = 1E-4, gamma = 0.5
and rmsprop_decay = 0.85.  Running on Thinkmate.  Turns frac reaches
maximum plateau value near 0.6 around 2.5E5 episodes.  Then it starts to
fall down to 0.4 around 5.5E5.  Slowly starts rising again around 8E5
episodes.  Loss falls to around 1 up till 80E6 time steps.  Then starts
rising again before starting to fall.  Expt 5 seems overall worse than expt
3 except for speed with which expt 5 initially improves.

6.  Z=4 levels with H1 = 300, H2 = 64, learning_rate = 3E-5, gamma = 0.5
and rmsprop_decay = 0.85.  Running on m6700.  After 9E6 episodes, loss
function is very slowly decreasing.  Turn frac is also approaching 0.6 and
very slowly increasing.  Experiment #6 is a slower version of expt #3.  

-----------------
7:30 am on Mon Oct 24 

7.  Z=4 levels with H1 = 300, H2 = 64, learning_rate = 1E-4, gamma = 0.5
and rmsprop_decay = 0.75.  Started running on Thinkmate around 7:35 am.
After 5E6 episode, turns frac has asymptoted below 0.5.  After 60E6 time
steps, loss is around 1.5.  This is definitely worse than expt #3.
Terminated.

8.  Z=4 levels with H1 = 64, H2 = 64, H3 = 64, learning_rate = 1E-4, gamma
= 0.5 and rmsprop_decay = 0.85.  Started running on Thinkmate around 7:50 am
After 6E6 episodes, turn frac is around 0.4 after having risen closer to
0.5 around 3E6 episodes.  Loss has falled to around 1 after 80E6 steps.
This seems like an unpromising architecture.  Terminated.

9.  Z=4 levels with H1 = 128, H2 = 64, H3 = 32, learning_rate = 1E-4, gamma
= 0.5 and rmsprop_decay = 0.85.  Started running on m6700 at 7:42 am.
After running for 4 hours, turns frac appears to asymptote around 0.52.
Loss function drops to around 1 and may continue to slowly fall. But this
expt's results seem significantly worse than those of expt 3. Terminated.

layer = 0 wlo = -2.03131 w_05 = -0.424748 w_25 = -0.146209
   w_50 = -0.00493972 w_75 = 0.140775 w_95 = 0.390811 whi = 2.61713
layer = 1 wlo = -1.57167 w_05 = -0.408438 w_25 = -0.0678079
   w_50 = 0.116902 w_75 = 0.317409 w_95 = 0.663183 whi = 1.69959
layer = 2 wlo = -1.72718 w_05 = -0.611286 w_25 = -0.191232
   w_50 = -0.010166 w_75 = 0.201483 w_95 = 0.662293 whi = 2.44698
layer = 3 wlo = -1.16897 w_05 = 0.0744037 w_25 = 2.99314
   w_50 = 4.60082 w_75 = 6.35227 w_95 = 8.30674 whi = 9.32921

10.  Z=4 levels with H1 = 256, H2 = 128, H3 = 64, learning_rate = 1E-4,
gamma = 0.5 and rmsprop_decay = 0.85.  Started running on Thinkmate around
7:50 am.  After nearly 2E6 episodes, turn frac has risen to 0.5.  After
25E6 timesteps, loss is around 1.25.  This seems more promising than expts
#8 and #9.  Terminated.

11.  Z=4 levels with H1 = 300, H2 = 200, H3 = 100, learning_rate = 1E-4,
gamma = 0.5 and rmsprop_decay = 0.85.  Started running on m6700 at 11 am.
As of 1:30 pm, it looks like turn frac has asymptoted around 0.5 and loss
has asymptoted around 1.8.  Clearly much worse than expt 3.  

12.  Z=4 levels with H1 = 300, H2 = 200, H3 = 100, learning_rate = 3E-4,
gamma = 0.5 and rmsprop_decay = 0.85.  Started on Thinkmate around 11:13
am.  After 800K episodes, turn frac is around 0.5.  Loss is around 0.9
after 10E6 time steps.  Running as of 1:50 pm on Mon Oct 24.

13.  Z=4 levels with H1 = 300, H2 = 150, H3 = 64, learning_rate = 1E-4,
gamma = 0.5 and rmsprop_decay = 0.85.  Started on Thinkmate around 11:15
am.  After 1E6 episodes, turn frac is around 0.55 and increasing.  After
10E6 time steps, loss is around 2.25.  Running as of 1:50 pm on Mon Oct 24.

14.  Z=4 levels with H1 = 64, H2 = 300, learning_rate = 1E-4, gamma = 0.5
and rmsprop_decay = 0.85.  Started on Thinkmate around 1:50 pm on Mon Oct
24.

15.  Z=4 levels with H1 = 64, H2 = 128, H3 = 256, learning_rate = 1E-4,
gamma = 0.5 and rmsprop_decay = 0.85.  Started on Thinkmate around 1:55 pm
on Mon Oct 24.


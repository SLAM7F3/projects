==========================================================================
Gender detection results for Face02 networks
==========================================================================
Last updated on 8/31/16; 9/7/16; 9/8/16; 9/10/16
==========================================================================

Lessons learned:

0.  Facenet architectures are sufficiently small that Titan GPUs have
enough memory to run two simultaneously.  But we have empirically found
that computation significantly slows when two caffe models are
simultaneously trained.  So this should probably only be done overnight.

1.  Training facenets with 5 and 6 conv layers CAN yield noticeably better
results than facenet01 which has 4 conv layers.  But the conv layers must
be placed towards the end of the convolution stack rather than the
beginning.

2.  On Sun Aug 28, we ran 4 variants of Facenet01 which all had less
capacity.  In each case, the performance of the 4 variants was worse!   So
as of 8/29/16, we believe Hinton's doctrine of bigger networks are better
is true...

3.  We find no particular benefit (and sometimes significant harm) to
reducing the number of final fully connected layers from 3 down to 2.

4.  We see no obvious benefit to increasing the number of nodes within
fully connected layers from 256 to 512.

5.  We see no obvious benefit to decreasing from 3 max pooling layers down
to 1 max pool + 1 avg pool.

6.  We probably want to add additional convolution layers as close to the
first fully connected layer as possible.

7.  Globally replacing max pooling layers with average pooling layers
led to better male and worse female classification.

8.  On 9/7/16, we empirically found the following numbers of test images
had ZERO activations for all nodes within the following convolution layers
of Facenet 1:

Conv layer 1:  3325 / 4536 = 73.3%
Conv layer 2:     5 / 4536 =  0.1%
Conv layer 3:   768 / 4536 = 16.9%
Conv layer 4:     2 / 4536 =  0.04%

After seeing an 9% improvement in Facenet 1 classification of female faces,
we strongly suspect that we should ALWAYS resize training image chips so
that they fill up the input volume as much as possible!  Moreover, the
numbers of test images which have all zero activations within convolution
layers 1 and 3 are dramatically reduced:

Conv layer 1:  318 / 4892 = 6.5%
Conv layer 2:    3 / 4892 = 0.05%
Conv layer 3:   69 / 4892 = 1.4%
Conv layer 4:   54 / 4892 = 1.1%

9.  A 6-convolution layer model with batch normalization DEFINITELY yields
better results than a 4-convolutional layer model without batch
normalization!  We can also increase the learning rate by an order of
magnitude if we use batch normalization versus if we don't.  

-------------------------------------------------------------------

350K combined original + Adience + Iranian faces + non-faces used for training.

Gender classification, baseline set of O(20K) female + O(20K) male faces +
17K Adience faces + 300 Iranian females + O(90K) non-faces , base learning
rate = 0.003, accuracy layer added for training phase, Dropout added into
all layers of network.  Dropout probabilities of nodes being ignored start
small and increase to 0.5 toward end of network.  Gaussian initialize all
node weights. with sigma = 0.1 for first conv and last fc layers and sigma
= 0.01 for all other weight layers.  Weight decay = 0.0010.  6x
augmentation of our 40K faces, 2X augmentation of Adience and Iranian
female faces.  Imagery data broken apart into training, validation and
testing subsets.  Image chips smaller than 48x48 doubled in size.

mean_B_value: 38.5   #  mu_blue for O(350K) 96x96 training face image chips
mean_G_value: 41.9   #  mu_green for O(350K) 96x96 training face image chips
mean_R_value: 49.0   #  mu_red for O(350K) 96x96 training face image chips
  
-------------------------------------------------------------------
*.  Mon Aug 29 , Titan 1, morning, Facenet model 2e

Training started on Facenet model 2e around 6:20 am

  /data/caffe/faces/trained_models/Aug29_2e_T1/train_iter_262000.caffemodel
  caffe validation accuracy asymptotes to 0.882 with no overfitting

n = 0 n_layer_nodes = 96
n = 1 n_layer_nodes = 192
n = 2 n_layer_nodes = 224
n = 3 n_layer_nodes = 256
n = 4 n_layer_nodes = 256
n = 5 n_layer_nodes = 256
n = 6 n_layer_nodes = 3

New mean RGB values + new std for 0th and 6th layers compared to Facenet01.

frac_correct = 0.833481 frac_unsure = 0.0591873 frac_incorrect = 0.107332

frac_nonface_correct = 0.980411
frac_nonface_incorrect = 0.0195886

frac_male_correct = 0.857614
frac_male_unsure = 0.0373832
frac_male_incorrect = 0.105003

frac_female_correct = 0.718602
frac_female_unsure = 0.118483
frac_female_incorrect = 0.162915

Confusion matrix:

1008	14	7	0	
39	1560	152	68	
27	248	1213	200	
0	0	0	0	

These results are comparable to our best results from facenet_01 (as they
should be).

-------------------------------------------------------------------
*.  Mon Aug 29 , Titan 3, morning, Facenet model 2f

Training started on Facenet model 2f around 6:25 am

/data/caffe/faces/trained_models/Aug29_2f_T3/train_iter_212000.caffemodel 
  caffe validation accuracy asymptotes to 0.879 with no overfitting
  n_total_weights = 20616064 n_total_biases = 1923

n = 0 n_layer_nodes = 128
n = 1 n_layer_nodes = 256
n = 2 n_layer_nodes = 256
n = 3 n_layer_nodes = 256
n = 4 n_layer_nodes = 512
n = 5 n_layer_nodes = 512
n = 6 n_layer_nodes = 3

frac_correct = 0.826384 frac_unsure = 0.0677256 frac_incorrect = 0.10589

frac_nonface_correct = 0.989279
frac_nonface_incorrect = 0.0107212

frac_male_correct = 0.843321
frac_male_unsure = 0.0406817
frac_male_incorrect = 0.115998

frac_female_correct = 0.709123
frac_female_unsure = 0.138033
frac_female_incorrect = 0.152844

Confusion matrix:

1017	6	6	0	
43	1534	168	74	
24	234	1197	233	
0	0	0	0	

New mean RGB values + new std for 0th and 6th layers compared to Facenet01.

These results are not significantly better than those for model 2e.  Yet
the total number of parameters in this model is twice that in model 2e.
Visualization of maximal filter responses also shows no obvious benefit
over model 2e.

-------------------------------------------------------------------
*.  Tues, Aug 30, Titan 1, Facenet model 2g, 5th conv layer at beginning
Training started on Titan 1 at 6:20 am.  Training stopped on Titan 1 at 3
pm on Tuesday.

/data/caffe/faces/trained_models/Aug29_2g_T1/train_iter_58000.caffemodel

n_total_weights = 10658080 n_total_biases = 1379

n = 0 n_layer_nodes = 96
n = 1 n_layer_nodes = 96
n = 2 n_layer_nodes = 192
n = 3 n_layer_nodes = 224
n = 4 n_layer_nodes = 256
n = 5 n_layer_nodes = 256
n = 6 n_layer_nodes = 256
n = 7 n_layer_nodes = 3

Param layer p = 0 is named conv1a
Param layer p = 1 is named conv1b
Param layer p = 2 is named conv2
Param layer p = 3 is named conv3
Param layer p = 4 is named conv4
Param layer p = 5 is named fc5
Param layer p = 6 is named fc6
Param layer p = 7 is named fc7_faces

New mean RGB values + new std for 0th and 7th layers compared to Facenet01.

Visualization of maximal filter responses strongly suggests this is
a poor place to insert a 5th conv layer into the Facenet 01 network.

-------------------------------------------------------------------
*.  Tues, Aug 30, Titan 3, Facenet model 2h, 5th conv layer at end
Training started on Titan 3 at 6:20 am.  

/data/caffe/faces/trained_models/Aug29_2h_T3/train_iter_294000.caffemodel 
  caffe validation accuracy asymptotes to around 0.895 with perhaps 
  slight overfitting

  n_total_weights = 11,164,960 n_total_biases = 1539

n = 0 n_layer_nodes = 96
n = 1 n_layer_nodes = 192
n = 2 n_layer_nodes = 224
n = 3 n_layer_nodes = 256
n = 4 n_layer_nodes = 256
n = 5 n_layer_nodes = 256
n = 6 n_layer_nodes = 256
n = 7 n_layer_nodes = 3y

Param layer p = 0 is named conv1
Param layer p = 1 is named conv2
Param layer p = 2 is named conv3
Param layer p = 3 is named conv4a
Param layer p = 4 is named conv4b
Param layer p = 5 is named fc5
Param layer p = 6 is named fc6
Param layer p = 7 is named fc7_faces

Parameter layer p = 0
  Weights:  mu = -0.00174696 sigma = 0.0683134
  Biases:  mu = -0.00189756 sigma = 0.0397123

Parameter layer p = 1
  Weights:  mu = -0.00118577 sigma = 0.00993896
  Biases:  mu = -0.000865359 sigma = 0.0194823

Parameter layer p = 2
  Weights:  mu = -0.00120124 sigma = 0.00857981
  Biases:  mu = 0.00485925 sigma = 0.0279415

Parameter layer p = 3
  Weights:  mu = -0.000930226 sigma = 0.00745409
  Biases:  mu = 0.0227746 sigma = 0.0404812

Parameter layer p = 4
  Weights:  mu = -0.000525946 sigma = 0.00489332
  Biases:  mu = 0.068664 sigma = 0.07447

Parameter layer p = 5
  Weights:  mu = 1.46273e-05 sigma = 0.00248768
  Biases:  mu = 0.0201204 sigma = 0.0188627

Parameter layer p = 6
  Weights:  mu = 0.00170436 sigma = 0.00930591
  Biases:  mu = 0.0926569 sigma = 0.052462

Parameter layer p = 7
  Weights:  mu = -0.00202325 sigma = 0.0887146
  Biases:  mu = -5.7742e-07 sigma = 0.167106

frac_correct = 0.848258 frac_unsure = 0.0293339 frac_incorrect = 0.122408

frac_nonface_correct = 0.97371
frac_nonface_incorrect = 0.0262902

frac_male_correct = 0.842771
frac_male_unsure = 0.0203408
frac_male_incorrect = 0.136888

frac_female_correct = 0.777844
frac_female_unsure = 0.056872
frac_female_incorrect = 0.165284

Confusion matrix:

1002	27	0	0	
61	1533	188	37	
33	246	1313	96	
0	0	0	0	

Visualization of maximal filter responses strongly suggests this is
the best way to insert a 5th conv layer into the Face01 network!

***************************
As of 8/31/16, model 2h yields the best overall gender classification 
performance results!
***************************


-------------------------------------------------------------------
*.  Mon Aug 29 , Titan 1, morning, Facenet model 2i, 4 FC layers
Training started around 12:45 pm

/data/caffe/faces/trained_models/Aug29_2i_T1/train_iter_146000.caffemodel 
  caffe validation accuracy asymptotes to 0.883 with no overfitting
  n_total_weights = 10607520 n_total_biases = 1411

n = 0 n_layer_nodes = 96
n = 1 n_layer_nodes = 192
n = 2 n_layer_nodes = 224
n = 3 n_layer_nodes = 256
n = 4 n_layer_nodes = 256
n = 5 n_layer_nodes = 256
n = 6 n_layer_nodes = 128
n = 7 n_layer_nodes = 3

New mean RGB values + new std for 0th and 6th layers compared to Facenet01.

frac_correct = 0.846612 frac_unsure = 0.0326639 frac_incorrect = 0.120724

frac_nonface_correct = 0.974609
frac_nonface_incorrect = 0.0253906

frac_male_correct = 0.893348
frac_male_unsure = 0.0148433
frac_male_incorrect = 0.0918087

frac_female_correct = 0.718602
frac_female_unsure = 0.0716825
frac_female_incorrect = 0.209716

Confusion matrix:

1002	23	4	0	
33	1625	134	27	
20	334	1213	121	
0	0	0	0	

Male classification is better than baseline model 2e.  But female
classification is worse.  Visualization of maximal filter responses also
shows no obvious benefit over model 2e.

-------------------------------------------------------------------
*.  Mon Aug 29 , Titan 1, morning, Facenet model 2j, 1 2x2 max pool + 1 4x4
avg pool.  Training started around 12:45 pm

  /data/caffe/faces/trained_models/Aug29_2j_T3/train_iter_52000.caffemodel
  caffe validation accuracy approaches 0.862 with no overfitting after 14
epochs.  Probably has not yet asymptoted

n_total_weights = 10575136 n_total_biases = 1283

n = 0 n_layer_nodes = 96
n = 1 n_layer_nodes = 192
n = 2 n_layer_nodes = 224
n = 3 n_layer_nodes = 256
n = 4 n_layer_nodes = 256
n = 5 n_layer_nodes = 256
n = 6 n_layer_nodes = 3

New mean RGB values + new std for 0th and 6th layers compared to Facenet01.


frac_correct = 0.818082 frac_unsure = 0.0637266 frac_incorrect = 0.118192

frac_nonface_correct = 0.972763
frac_nonface_incorrect = 0.0272374

frac_male_correct = 0.840022
frac_male_unsure = 0.042331
frac_male_incorrect = 0.117647

frac_female_correct = 0.700237
frac_female_unsure = 0.125592
frac_female_incorrect = 0.174171

Confusion matrix:

1001	26	2	0	
43	1528	171	77	
27	267	1182	212	
0	0	0	0	

-------------------------------------------------------------------
*.  Tues Aug 30 , Titan 1, pm, Facenet model 2k, 5 conv layers, 2 before
final max pool
Training started on Titan 1 around 12:50 pm 
  /data/caffe/faces/trained_models/Aug30_2k_T1/train_iter_116000.caffemodel
  caffe validation accuracy asymptotes to 0.890 with no overfitting

n_total_weights = 11026720 n_total_biases = 1507

n = 0 n_layer_nodes = 96
n = 1 n_layer_nodes = 192
n = 2 n_layer_nodes = 224
n = 3 n_layer_nodes = 224
n = 4 n_layer_nodes = 256
n = 5 n_layer_nodes = 256
n = 6 n_layer_nodes = 256
n = 7 n_layer_nodes = 3

Param layer p = 0 is named conv1
Param layer p = 1 is named conv2
Param layer p = 2 is named conv3a
Param layer p = 3 is named conv3b
Param layer p = 4 is named conv4
Param layer p = 5 is named fc5
Param layer p = 6 is named fc6
Param layer p = 7 is named fc7_faces

frac_correct = 0.85119 frac_unsure = 0.0288801 frac_incorrect = 0.119929

frac_nonface_correct = 0.993197
frac_nonface_incorrect = 0.00680272

frac_male_correct = 0.846619
frac_male_unsure = 0.0241891
frac_male_incorrect = 0.129192

frac_female_correct = 0.76955
frac_female_unsure = 0.0515403
frac_female_incorrect = 0.17891

Confusion matrix:

1022	7	0	0	
50	1540	185	44	
28	274	1299	87	
0	0	0	0	

-------------------------------------------------------------------
*.  Tues Aug 30 , Titan 3, pm, Facenet model 2l, 6 conv layers, 3 after
final max pool, 3 FC layers
Training started on Titan 3 around 1 pm

  /data/caffe/faces/trained_models/Aug30_2l_T3/train_iter_154000.caffemodel
  caffe validation accuracy asymptotes to 0.900 with no overfitting

n_total_weights = 11,754,784 n_total_biases = 1795

Param layer p = 0 is named conv1
Param layer p = 1 is named conv2
Param layer p = 2 is named conv3
Param layer p = 3 is named conv4a
Param layer p = 4 is named conv4b
Param layer p = 5 is named conv4c
Param layer p = 6 is named fc5
Param layer p = 7 is named fc6
Param layer p = 8 is named fc7_faces

n = 0 n_layer_nodes = 96
n = 1 n_layer_nodes = 192
n = 2 n_layer_nodes = 224
n = 3 n_layer_nodes = 256
n = 4 n_layer_nodes = 256
n = 5 n_layer_nodes = 256
n = 6 n_layer_nodes = 256
n = 7 n_layer_nodes = 3

Parameter layer p = 0
  Weights:  mu = -0.00209242 sigma = 0.0678112
  Biases:  mu = -0.00280178 sigma = 0.0404695

Parameter layer p = 1
  Weights:  mu = -0.00148748 sigma = 0.010059
  Biases:  mu = -0.000521517 sigma = 0.0188167

Parameter layer p = 2
  Weights:  mu = -0.00137651 sigma = 0.0088032
  Biases:  mu = 0.00491522 sigma = 0.0268962

Parameter layer p = 3
  Weights:  mu = -0.00105041 sigma = 0.00775448
  Biases:  mu = 0.0180862 sigma = 0.034263

Parameter layer p = 4
  Weights:  mu = -0.000339101 sigma = 0.00456063
  Biases:  mu = 0.0493335 sigma = 0.0530716

Parameter layer p = 5
  Weights:  mu = 2.44271e-06 sigma = 0.00384558
  Biases:  mu = 0.038571 sigma = 0.043799

Parameter layer p = 6
  Weights:  mu = 3.72765e-05 sigma = 0.00245405
  Biases:  mu = 0.0134576 sigma = 0.0159878

Parameter layer p = 7
  Weights:  mu = 0.00123338 sigma = 0.00878365
  Biases:  mu = 0.0992461 sigma = 0.0760599

Parameter layer p = 8
  Weights:  mu = 0.00117658 sigma = 0.083619
  Biases:  mu = 1.04246e-06 sigma = 0.218129


frac_correct = 0.848291 frac_unsure = 0.031753 frac_incorrect = 0.119956

frac_nonface_correct = 0.970817
frac_nonface_incorrect = 0.0291829

frac_male_correct = 0.846619
frac_male_unsure = 0.0181418
frac_male_incorrect = 0.135239

frac_female_correct = 0.775474
frac_female_unsure = 0.0657583
frac_female_incorrect = 0.158768

Confusion matrix:

998	24	7	0	
62	1540	184	33	
36	232	1309	111	
0	0	0	0	


-------------------------------------------------------------------
*.  Tues Aug 30 , Titan 1, pm, Facenet model 2m, 6 conv layers, 3 after
final max pool, 2 FC layers
Training started on Titan 1 around 3 pm
 /data/caffe/faces/trained_models/Aug30_2m_T1/train_iter_116000.caffemodel

n_total_weights = 11,689,248 n_total_biases = 1539
n = 0 n_layer_nodes = 96
n = 1 n_layer_nodes = 192
n = 2 n_layer_nodes = 224
n = 3 n_layer_nodes = 256
n = 4 n_layer_nodes = 256
n = 5 n_layer_nodes = 256
n = 6 n_layer_nodes = 256
n = 7 n_layer_nodes = 3

Param layer p = 0 is named conv1
Param layer p = 1 is named conv2
Param layer p = 2 is named conv3
Param layer p = 3 is named conv4a
Param layer p = 4 is named conv4b
Param layer p = 5 is named conv4c
Param layer p = 6 is named fc5
Param layer p = 7 is named fc6_faces

Parameter layer p = 0
  Weights:  mu = -0.00154857 sigma = 0.0684329
  Biases:  mu = 0.00161334 sigma = 0.0377798

Parameter layer p = 1
  Weights:  mu = -0.00104881 sigma = 0.00954984
  Biases:  mu = -0.00138129 sigma = 0.0187513

Parameter layer p = 2
  Weights:  mu = -0.00111737 sigma = 0.00854115
  Biases:  mu = 0.000796193 sigma = 0.0274393

Parameter layer p = 3
  Weights:  mu = -0.00107437 sigma = 0.00790526
  Biases:  mu = 0.0182075 sigma = 0.0333297

Parameter layer p = 4
  Weights:  mu = -0.000356935 sigma = 0.00482978
  Biases:  mu = 0.0495673 sigma = 0.0663798

Parameter layer p = 5
  Weights:  mu = -0.000227623 sigma = 0.00402996
  Biases:  mu = 0.0488157 sigma = 0.0783737

Parameter layer p = 6
  Weights:  mu = 1.64021e-05 sigma = 0.00256568
  Biases:  mu = 0.023309 sigma = 0.0218018

Parameter layer p = 7
  Weights:  mu = -0.000268115 sigma = 0.0816619
  Biases:  mu = -5.96046e-07 sigma = 0.21269

frac_correct = 0.824697 frac_unsure = 0.0458655 frac_incorrect = 0.129438

frac_nonface_correct = 0.978599
frac_nonface_incorrect = 0.0214008

frac_male_correct = 0.860913
frac_male_unsure = 0.0236394
frac_male_incorrect = 0.115448

frac_female_correct = 0.691943
frac_female_unsure = 0.0977488
frac_female_incorrect = 0.210308

Confusion matrix:

1007	22	0	0	
82	1566	128	43	
54	301	1168	165	
0	0	0	0	

Model 2m with only 2 fully-connected layers yields much worse gender
classification results than its analog model 2l which has 3 fully-connected
layers.


-------------------------------------------------------------------
*.  Weds Aug 31 , Titan 1, am, Facenet model 2n, 6 conv layers,
conv3a+conv3b+conv4a+conv4b, 3 FC layers


Reset std devs
Training started on titan 1 around 6:35 am


-------------------------------------------------------------------
*.  Weds Aug 31 , Titan 3, am, Facenet model 2o, 6 conv layers,
conv3a+conv3b+conv4a+conv4b, 3 FC layers, leaky ReLUs

Reset std devs
Training started on titan 3 around 12:20 pm


-------------------------------------------------------------------
*.  Weds Aug 31 , Titan 3, am, Facenet model 2p, 6 conv layers,
conv3a+conv3b+conv4a+conv4b, 3 FC layers, AVE pool rather than MAX pool

Reset std devs
Training started on titan 3 around 6:50 am












-------------------------------------------------------------------
*.  Sun Aug 28 , Titan 1, morning, Facenet model 2a

Training started on Facenet 02a around 8:40 am

/data/caffe/faces/trained_models/Aug27_2a_T1/train_iter_72000.caffemodel 
  caffe validation accuracy asymptotes to 0.866 with no overfitting
  n_total_weights = 4813056 n_total_biases = 835
n = 0 n_layer_nodes = 64
n = 1 n_layer_nodes = 96
n = 2 n_layer_nodes = 128
n = 3 n_layer_nodes = 160
n = 4 n_layer_nodes = 192
n = 5 n_layer_nodes = 192
n = 6 n_layer_nodes = 3

frac_correct = 0.814831 frac_unsure = 0.0801148 frac_incorrect = 0.105054

frac_nonface_correct = 0.983398
frac_nonface_incorrect = 0.0166016

frac_male_correct = 0.823529
frac_male_unsure = 0.0522265
frac_male_incorrect = 0.124244

frac_female_correct = 0.703199
frac_female_unsure = 0.158768
frac_female_incorrect = 0.138033

Confusion matrix:

1011	17	1	0	
44	1498	182	95	
22	211	1187	268	
0	0	0	0	

-------------------------------------------------------------------
*.  Sun Aug 28, Titan 1, afternoon, larger Facenet model 2a

/data/caffe/faces/trained_models/Aug27_2a_larger_T1/train_iter_185000.caffemodel
  caffe validation accuracy asymptotes to 0.881 with no overfitting
  n_total_weights = 10463680 n_total_biases = 1219

frac_correct = 0.834656 frac_unsure = 0.0577601 frac_incorrect = 0.107584

frac_nonface_correct = 0.979592
frac_nonface_incorrect = 0.0204082

frac_male_correct = 0.850467
frac_male_unsure = 0.0406817
frac_male_incorrect = 0.108851

frac_female_correct = 0.729265
frac_female_unsure = 0.111374
frac_female_incorrect = 0.15936

Confusion matrix:

1008	19	2	0	
41	1547	157	74	
21	248	1231	188	
0	0	0	0	

-------------------------------------------------------------------
*.  Sun Aug 28 , Titan 3, morning, Facenet model 2b

Training started on Facenet 02b around 8:43 am

/data/caffe/faces/trained_models/Aug27_2b_T3/train_iter_101000.caffemodel 
  caffe validation accuracy asymptotes to 0.855 with no overfitting
  n_total_weights = 4776192 n_total_biases = 643

n = 0 n_layer_nodes = 64
n = 1 n_layer_nodes = 96
n = 2 n_layer_nodes = 128
n = 3 n_layer_nodes = 160
n = 4 n_layer_nodes = 192
n = 5 n_layer_nodes = 3

frac_correct = 0.796072 frac_unsure = 0.0856323 frac_incorrect = 0.118296

frac_nonface_correct = 0.979492
frac_nonface_incorrect = 0.0205078

frac_male_correct = 0.854865
frac_male_unsure = 0.0494777
frac_male_incorrect = 0.095657

frac_female_correct = 0.621445
frac_female_unsure = 0.17654
frac_female_incorrect = 0.202014

Confusion matrix:

1003	25	1	0	
55	1555	119	90	
26	315	1049	298	
0	0	0	0	

-------------------------------------------------------------------
*.  Sun Aug 28, Titan 3, afternoon, larger Facenet model 2b

/data/caffe/faces/trained_models/Aug27_2b_larger_T3/train_iter_216000.caffemodel
  caffe validation accuracy asymptotes to 0.871 with no overfitting
  n_total_weights = 10398144 n_total_biases = 963

n = 0 n_layer_nodes = 64
n = 1 n_layer_nodes = 128
n = 2 n_layer_nodes = 256
n = 3 n_layer_nodes = 256
n = 4 n_layer_nodes = 256
n = 5 n_layer_nodes = 3

frac_correct = 0.821492 frac_unsure = 0.0703883 frac_incorrect = 0.10812

frac_nonface_correct = 0.985366
frac_nonface_incorrect = 0.0146341

frac_male_correct = 0.847719
frac_male_unsure = 0.0362837
frac_male_incorrect = 0.115998

frac_female_correct = 0.69372
frac_female_unsure = 0.149882
frac_female_incorrect = 0.156398

Confusion matrix:

1013	15	1	0	
53	1542	158	66	
26	238	1171	253	
0	0	0	0	

-------------------------------------------------------------------
*.  Sun Aug 28, Titan 1, afternoon, Facenet model 2c

/data/caffe/faces/trained_models/Aug27_2c_T1/train_iter_92000.caffemodel
  caffe validation accuracy asymptotes to 0.864 with no overfitting
n_total_weights = 4813056 n_total_biases = 835

n = 0 n_layer_nodes = 64
n = 1 n_layer_nodes = 96
n = 2 n_layer_nodes = 128
n = 3 n_layer_nodes = 160
n = 4 n_layer_nodes = 192
n = 5 n_layer_nodes = 192
n = 6 n_layer_nodes = 3

frac_correct = 0.82183 frac_unsure = 0.0590959 frac_incorrect = 0.119074

frac_nonface_correct = 0.983463
frac_nonface_incorrect = 0.016537

frac_male_correct = 0.810335
frac_male_unsure = 0.0461792
frac_male_incorrect = 0.143485

frac_female_correct = 0.735782
frac_female_unsure = 0.109005
frac_female_incorrect = 0.155213

Confusion matrix:

1011	17	1	0	
49	1474	212	84	
28	234	1242	184	
0	0	0	0	

-------------------------------------------------------------------
*.  Sun Aug 28, Titan 3, afternoon, Facenet model 2d

/data/caffe/faces/trained_models/Aug27_2d_T3/train_iter_115000.caffemodel
  caffe validation accuracy asymptotes to 0.867 with no overfitting
  n_total_weights = 4776192 n_total_biases = 643

frac_correct = 0.819806 frac_unsure = 0.0657256 frac_incorrect = 0.114468

frac_nonface_correct = 0.983447
frac_nonface_incorrect = 0.0165531

frac_male_correct = 0.811985
frac_male_unsure = 0.0500275
frac_male_incorrect = 0.137988

frac_female_correct = 0.728673
frac_female_unsure = 0.12263
frac_female_incorrect = 0.148697

Confusion matrix:

1011	13	5	0	
64	1477	187	91	
34	217	1230	207	
0	0	0	0	

-------------------------------------------------------------------
Facenet1: 4 convolutions  

/data/caffe/faces/trained_models/Aug6_350K_96cap_T3/train_iter_702426.caffemodel 

n_total_weights = 10575136 n_total_biases = 1283

frac_correct = 0.840688 frac_unsure = 0.0582524 frac_incorrect = 0.101059

frac_nonface_correct = 0.993171
frac_nonface_incorrect = 0.00682927

frac_male_correct = 0.853766
frac_male_unsure = 0.0307861
frac_male_incorrect = 0.115448

frac_female_correct = 0.734005
frac_female_unsure = 0.123223
frac_female_incorrect = 0.142773

Confusion matrix:

1021	2	6	0	
51	1553	159	56	
25	216	1239	208	
0	0	0	0	

-------------------------------------------------------------------
5 convolutions  (terminated OK), Titan 1

  /data/caffe/faces/trained_models/Aug26_5conv_T1/train_iter_300000.caffemodel
  caffe validation accuracy asymptotes to 0.886, no overfitting
  n_total_weights = 4849920 n_total_biases = 899
	n = 0 n_layer_nodes = 64
	n = 1 n_layer_nodes = 64
	n = 2 n_layer_nodes = 96
	n = 3 n_layer_nodes = 128
	n = 4 n_layer_nodes = 160
	n = 5 n_layer_nodes = 192
	n = 6 n_layer_nodes = 192
	n = 7 n_layer_nodes = 3

n_nonface = 1023 n_male = 1819 n_female = 1688

n_correct = 3754
 n_nonface_correct = 1000 n_male_correct = 1538 n_female_correct = 1216

n_incorrect = 483
 n_nonface_incorrect = 23 n_male_incorrect = 197 n_female_incorrect = 263

 n_unsure = 293
 n_male_unsure = 84 n_female_unsure = 209

frac_correct = 0.828698 frac_unsure = 0.0646799 frac_incorrect = 0.106623

frac_nonface_correct = 0.977517
frac_nonface_incorrect = 0.0224829

frac_male_correct = 0.84552
frac_male_unsure = 0.0461792
frac_male_incorrect = 0.108301

frac_female_correct = 0.720379
frac_female_unsure = 0.123815
frac_female_incorrect = 0.155806

Confusion matrix:

1001	27	1	0	
44	1538	153	84	
22	241	1216	209	
0	0	0	0	

-------------------------------------------------------------------
6 convolutions  (terminated OK),  Titan 3

  /data/caffe/faces/trained_models/Aug26_6conv_T3/train_iter_273000.caffemodel
  caffe validation accuracy asymptotes to 0.883, no overfitting
  n_total_weights = 4932864 n_total_biases = 995

	n = 0 n_layer_nodes = 64
	n = 1 n_layer_nodes = 64
	n = 2 n_layer_nodes = 96
	n = 3 n_layer_nodes = 96
	n = 4 n_layer_nodes = 128
	n = 5 n_layer_nodes = 160
	n = 6 n_layer_nodes = 192
	n = 7 n_layer_nodes = 192
	n = 8 n_layer_nodes = 3

n_nonface = 1021 n_male = 1819 n_female = 1688

n_correct = 3789
 n_nonface_correct = 1006 n_male_correct = 1509 n_female_correct = 1274

n_incorrect = 474
 n_nonface_incorrect = 15 n_male_incorrect = 231 n_female_incorrect = 228

 n_unsure = 265
 n_male_unsure = 79 n_female_unsure = 186

frac_correct = 0.836793 frac_unsure = 0.0585247 frac_incorrect = 0.104682

frac_nonface_correct = 0.985309
frac_nonface_incorrect = 0.0146915

frac_male_correct = 0.829577
frac_male_unsure = 0.0434305
frac_male_incorrect = 0.126993

frac_female_correct = 0.754739
frac_female_unsure = 0.11019
frac_female_incorrect = 0.135071

Confusion matrix:

1010	17	2	0	
51	1509	180	79	
26	202	1274	186	
0	0	0	0	



-------------------------------------------------------------------
7 convolutions  (terminated OK)

  /data/caffe/faces/trained_models/Aug26_7conv_T1/train_iter_113221.caffemodel
  caffe validation accuracy asymptotes to 0.881, no overfitting
  Trained for only 30 epochs.  Accuracy might still have been
     slightly increasing at end of session
  initial 0.003 learning rate is too high!
  n_total_weights = 5080320 n_total_biases = 1123

n = 0 n_layer_nodes = 64
n = 1 n_layer_nodes = 64
n = 2 n_layer_nodes = 96
n = 3 n_layer_nodes = 96
n = 4 n_layer_nodes = 128
n = 5 n_layer_nodes = 128
n = 6 n_layer_nodes = 160
n = 7 n_layer_nodes = 192
n = 8 n_layer_nodes = 192
n = 9 n_layer_nodes = 3

n_nonface = 1025 n_male = 1819 n_female = 1688

n_correct = 3757
 n_nonface_correct = 1016 n_male_correct = 1487 n_female_correct = 1254


n_incorrect = 533
 n_nonface_incorrect = 9 n_male_incorrect = 258 n_female_incorrect = 266

 n_unsure = 242
 n_male_unsure = 74 n_female_unsure = 168

frac_correct = 0.828994 frac_unsure = 0.0533981 frac_incorrect = 0.117608

frac_nonface_correct = 0.99122
frac_nonface_incorrect = 0.00878049

frac_male_correct = 0.817482
frac_male_unsure = 0.0406817
frac_male_incorrect = 0.141836

frac_female_correct = 0.742891
frac_female_unsure = 0.0995261
frac_female_incorrect = 0.157583

Confusion matrix:

1018	10	1	0	
71	1487	187	74	
43	223	1254	168	
0	0	0	0	


........................................................
7 convolutions, 2 fc layers, 2 max pools + 1 avg pool (running as of 1:45
pm Sat) 

T3




........................................................
8 convolutions (failed to converge!)

........................................................
7 convolutions, 2 fc layers (running as of noontime Sat)



======================================
TODO:

*.  Break out all Magick++ methods in videofuncs and imagefuncs namespaces
into new magickfuncs namespace sitting in src/images/


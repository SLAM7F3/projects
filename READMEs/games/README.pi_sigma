==========================================================================
Training pi_sigma for TTT
==========================================================================
Last updated on 1/18/17; 1/19/17; 1/20/17; 1/21/17
==========================================================================

Best results so far:

66. T1:  Experiment with forcing player board state to always correspond to
agent value = +1

Experiment 66
Wed Jan 18 08:17:24 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 64
   layer = 3 n_nodes = 64
   n_weights = 20480 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 44458
n_test_samples = 5043
mini_batch_size = 100
n_mini_batches per epoch = 445
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 1
Process ID = 28193

After 712 epochs, test accuracy = 0.683 and training accuracy = 0.734
After 1370 epochs, test accuracy = 0.756 and training accuracy = 0.812
After 4058 epochs, test accuracy = 0.850 and training accuracy = 0.909

------------------------------------------------------
TERMINATED EXPERIMENTS
------------------------------------------------------

5.  m6700

Experiment 5
Mon Jan 16 10:28:40 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 65
   layer = 1 n_nodes = 32
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 64
   n_weights = 5152 (FC)
base_learning_rate = 0.0001; batch_size = 32
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.001
n_training_samples = 85135
n_test_samples = 9438
Max n_training_epochs = 1000
Leaky ReLU small slope = 0.01
Process ID = 5253

Bad loss behavior

10.  T1
H1 = 64, H2 = 32, H3 = 0
lambda = 1E-3
base_learning_rate = 3E-4

Bad loss behavior

11.  T1
H1 = 64, H2 = 32, H3 = 0
lambda = 1E-3
base_learning_rate = 1E-4

Bad loss behavior

12.  T1
H1 = 64, H2 = 32, H3 = 0
lambda = 1E-3
base_learning_rate = 3E-5

Bad loss behavior

13.  T3
H1 = 32, H2 = 32, H3 = 0
lambda = 1E-3
base_learning_rate = 3E-4

Bad loss behavior

14.  T3
H1 = 32, H2 = 32, H3 = 0
lambda = 1E-3
base_learning_rate = 1E-4

Bad loss behavior

15.  T3
H1 = 32, H2 = 32, H3 = 0
lambda = 1E-3
base_learning_rate = 3E-5

Bad loss behavior


16.  TM
H1 = 32, H2 = 32, H3 = 32
lambda = 1E-3
base_learning_rate = 3E-4

Bad loss behavior

17.  TM
H1 = 32, H2 = 32, H3 = 32
lambda = 1E-3
base_learning_rate = 1E-4

Bad loss behavior


18.  T1
H1 = 64, H2 = 32, H3 = 0
lambda = 1E-3
base_learning_rate = 1E-6

19.  T1
H1 = 64, H2 = 32, H3 = 0
lambda = 1E-3
base_learning_rate = 3E-7

20.  T1
H1 = 32, H2 = 32, H3 = 0
lambda = 1E-3
base_learning_rate = 1E-6

27.  T1
H1 = 64, H2 = 32, H3 = 0
lambda = 0
base_learning_rate = 3E-5

21.  T3
H1 = 32, H2 = 32, H3 = 0
lambda = 1E-3
base_learning_rate = 3E-7

22.  T3
H1 = 32, H2 = 32, H3 = 32
lambda = 1E-3
base_learning_rate = 1E-6

23.  T3
H1 = 32, H2 = 32, H3 = 32
lambda = 1E-3
base_learning_rate = 3E-7

28.  T1
H1 = 64, H2 = 64, H3 = 0
lambda = 0
base_learning_rate = 3E-5

24.  TM
H1 = 32, H2 = 32, H3 = 0
lambda = 0
base_learning_rate = 3E-5

25.  TM
H1 = 32, H2 = 32, H3 = 0
lambda = 0
base_learning_rate = 1E-4

26.  TM
H1 = 32, H2 = 32, H3 = 0
lambda = 0
base_learning_rate = 1E-5

30.  T1
max move rel to end = 1
H1 = H2 = 32
lambda = 0.01
base_learning_rate = 1E-

31.  T1
max move rel to end = 1
H1 = H2 = 32
lambda = 0.001
base_learning_rate = 3E-5

32.  T1
max move rel to end = 1
H1 = H2 = 32
lambda = 0.003
base_learning_rate = 3E-5

33.  T1
max move rel to end = 1
H1 = H2 = 32
lambda = 0.005
base_learning_rate = 1E-6

Loss basically stayed under control.  
testing accuracy = 0.0395; training accuracy = 0.075


34.  T1
max move rel to end = 1
H1 = 64; H2 = 32
lambda = 0.005
base_learning_rate = 1E-6

Experiment 34
Mon Jan 16 18:43:58 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 65
   layer = 1 n_nodes = 64
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 64
   n_weights = 8256 (FC)
base_learning_rate = 1e-06; batch_size = 32
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.005
n_training_samples = 4509
n_test_samples = 492
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 1
Process ID = 6954

Loss basically stayed under control.  
testing accuracy = 0.0569; training accuracy = 0.105

35.  T1
max move rel to end = 1
H1 = 32; H2 = 32; H3 = 32
lambda = 0.005
base_learning_rate = 1E-6

Loss basically stayed under control.  
testing accuracy = 0.0535; training accuracy = 0.0701


36.  T1
H1 = 64; H2 = 32; H3 = 0
lambda = 0.005
base_learning_rate = 3E-6
max move rel to end = 1


37.  T1
H1 = 64; H2 = 32; H3 = 0
lambda = 0.005
base_learning_rate = 1E-5
max move rel to end = 1

38.  T1
H1 = 64; H2 = 32; H3 = 0
lambda = 0.005
base_learning_rate = 3E-5
max move rel to end = 1

After 340+ epochs, test accuracy and training accuracy around 0.1


39.  T3
H1 = 128; H2 = 32; H3 = 0
lambda = 0.005
base_learning_rate = 3E-6
max move rel to end = 1

40.  T3
H1 = 128; H2 = 32; H3 = 0
lambda = 0.005
base_learning_rate = 1E-5
max move rel to end = 1

41.  T3
H1 = 128; H2 = 32; H3 = 0
lambda = 0.005
base_learning_rate = 3E-5
max move rel to end = 1


42.  T1
H1 = 64; H2 = 32; H3 = 0
lambda = 0.001
base_learning_rate = 1E-4
max move rel to end = 1

Experiment 42
Tue Jan 17 05:46:33 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 65
   layer = 1 n_nodes = 64
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 64
   n_weights = 8256 (FC)
base_learning_rate = 0.0001; batch_size = 32
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.001
n_training_samples = 31524
n_test_samples = 3477
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 1
Process ID = 2235

After 160 epochs, test accuracy = 0.252 and train accuracy = 0.291


43.  T3
H1 = 128; H2 = 32; H3 = 0
lambda = 0.001
base_learning_rate = 1E-4
max move rel to end = 1

After 1018 epochs, test accuracy = 0.395 & train accuracy = 0.442

44.  T1
H1 = 64; H2 = 64; H3 = 0
lambda = 0.005
base_learning_rate = 1E-5
max move rel to end = 1

45.  T3
H1 = 128; H2 = 64; H3 = 0
lambda = 0.005
base_learning_rate = 1E-5
max move rel to end = 1

After 101 epochs, test accuracy = 0.287 and training accuracy = 0.319
Loss diverging

46.  T1
H1 = 64; H2 = 32; H3 = 0
lambda = 0.001
base_learning_rate = 3E-4
max move rel to end = 1

Experiment 46
Tue Jan 17 06:22:52 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 65
   layer = 1 n_nodes = 64
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 64
   n_weights = 8256 (FC)
base_learning_rate = 0.0003; batch_size = 100
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.001
n_training_samples = 31524
n_test_samples = 3477
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 1
Process ID = 18761

After 190 epochs, test accuracy = 0.309 and train accuracy = 0.343


47.  T1
H1 = 64; H2 = 32; H3 = 0
lambda = 0.003
base_learning_rate = 3E-4
max move rel to end = 1

48.  T1
H1 = 64; H2 = 32; H3 = 0
lambda = 0.005
base_learning_rate = 3E-4
max move rel to end = 1

49.  T3
H1 = 128; H2 = 32; H3 = 0
lambda = 0.001
base_learning_rate = 3E-4
max move rel to end = 1

After 1288 epochs, test accuracy = 0.444 and train accuracy = 0.516

50.  T3
H1 = 128; H2 = 32; H3 = 0
lambda = 0.003
base_learning_rate = 3E-4
max move rel to end = 1

51.  T3
H1 = 128; H2 = 32; H3 = 0
lambda = 0.005
base_learning_rate = 3E-4
max move rel to end = 1

52.  T1
H1 = 64; H2 = 64; H3 = 0
lambda = 0.0003
base_learning_rate = 1E-3
max move rel to end = 1

After 1.6K epochs, test accuracy = 0.372

53.  T1
H1 = 64; H2 = 128; H3 = 0
lambda = 0.0003
base_learning_rate = 1E-3
max move rel to end = 1

After 1073 epochs, test accuracy = 0.352

54.  T3
H1 = 128; H2 = 64; H3 = 0
lambda = 0.0003
base_learning_rate = 3E-4
max move rel to end = 1

After 853 epochs, test accuracy = 0.400 and train accuracy = 0.512
After 3557 epochs, test accuracy = 0.519 and train accuracy = 0.613


55.  T3
H1 = 128; H2 = 128; H3 = 0
lambda = 0.0003
base_learning_rate = 3E-4
max move rel to end = 1
Experiment 55
Tue Jan 17 06:57:34 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 65
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 128
   layer = 3 n_nodes = 64
   n_weights = 32896 (FC)
base_learning_rate = 0.0003; batch_size = 100
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0003
n_training_samples = 31474
n_test_samples = 3527
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 1
Process ID = 19850

After 536 epochs, test accuracy = 0.398 and train accuracy = 0.542


58.  Repeat expt 54 from T3 with streamlined code
H1 = 128; H2 = 64; H3 = 0
lambda = 0.0003
base_learning_rate = 3E-4
max move rel to end = 1

Tue Jan 17 13:20:04 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 65
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 64
   layer = 3 n_nodes = 64
   n_weights = 20608 (FC)
base_learning_rate = 0.0003; batch_size = 100
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0003
n_training_samples = 44915
n_test_samples = 5085
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 1
Process ID = 17352

After 1673 epochs, testing accuracy = 0.522 and training accuracy = 0.587
After 2688 epcohs, testing accuracy = 0.577 and training accuracy = 0.638

59.  Repeat expt 55 from T3 with streamline code
H1 = 128; H2 = 128; H3 = 0
lambda = 0.0003
base_learning_rate = 3E-4
max move rel to end = 1


60.   Experiment 60 on Titan 1:
Tue Jan 17 15:49:40 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 65
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 64
   layer = 3 n_nodes = 64
   n_weights = 20608 (FC)
base_learning_rate = 0.0003; batch_size = 100
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0003
n_training_samples = 44915
n_test_samples = 5085
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 1
Process ID = 5356

After 2202 epochs, test accuracy = 0.569322 and training accuracy = 0.636
After 3.2K epochs, test accuracy = 0.578 and training accuracy = 0.664


60.  Repeat expt 54 from T3 with code with transposed weights matrix
H1 = 128; H2 = 64; H3 = 0
lambda = 0.0003
base_learning_rate = 3E-4
max move rel to end = 1

Experiment 60 on Titan 3:
Tue Jan 17 14:36:54 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 65
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 64
   layer = 3 n_nodes = 64
   n_weights = 20608 (FC)
base_learning_rate = 0.0003; batch_size = 100
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0003
n_training_samples = 44915
n_test_samples = 5085
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 1
Process ID = 29550


After 1566 epochs, testing accuracy = 0.515 and training accuracy = 0.582


61.  Repeat expt 55 from T3 with code with transposed weights matrix
H1 = 128; H2 = 128; H3 = 0
lambda = 0.0003
base_learning_rate = 3E-4
max move rel to end = 1

Results not as good as H1 = 64, H2 = 128!

62.  Repeat experiment 60 on Titan 1 after fixing serious bug in genvector

Experiment 62
Wed Jan 18 05:42:56 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 65
   layer = 1 n_nodes = 64
   layer = 2 n_nodes = 128
   layer = 3 n_nodes = 64
   n_weights = 20544 (FC)
base_learning_rate = 0.0003; batch_size = 100
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0003
n_training_samples = 44461
n_test_samples = 5040
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 1
Process ID = 9036

OK results


63.  Repeat experiment 60 on Titan 3 after fixing serious bug in genvector

Experiment 63
Wed Jan 18 05:44:00 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 65
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 64
   layer = 3 n_nodes = 64
   n_weights = 20608 (FC)
base_learning_rate = 0.0003; batch_size = 100
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0003
n_training_samples = 44915
n_test_samples = 5085
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 1
Process ID = 4901

Good results

64.  Repeat experiment 60 on Titan 1 after fixing serious bug in genvector
and streamlining regularization term

 Wed Jan 18 06:32:31 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 65
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 64
   layer = 3 n_nodes = 64
   n_weights = 20608 (FC)
base_learning_rate = 0.0003; batch_size = 100
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 44462
n_test_samples = 5039
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 1
Process ID = 21235

After 1047 epochs, test accuracy = 0.483 and training accuracy = 0.526

65.  Repeat experiment 60 on Titan 3 after fixing serious bug in genvector
and streamlining regularization term

Experiment 65
Wed Jan 18 06:33:33 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 65
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 64
   layer = 3 n_nodes = 64
   n_weights = 20608 (FC)
base_learning_rate = 0.0003; batch_size = 100
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 44462
n_test_samples = 5039
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 1
Process ID = 19380


66. T1:  Experiment with forcing player board state to always correspond to
agent value = +1

Experiment 66
Wed Jan 18 08:17:24 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 64
   layer = 3 n_nodes = 64
   n_weights = 20480 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 44458
n_test_samples = 5043
mini_batch_size = 100
n_mini_batches per epoch = 445
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 1
Process ID = 28193

After 712 epochs, test accuracy = 0.683 and training accuracy = 0.734
After 1370 epochs, test accuracy = 0.756 and training accuracy = 0.812
After 4058 epochs, test accuracy = 0.850 and training accuracy = 0.909
TERMINATED


67. T3: Experiment with forcing player board state to always correspond to
agent value = +1

Experiment 67
Wed Jan 18 08:18:42 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 64
   layer = 3 n_nodes = 64
   n_weights = 20480 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 44458
n_test_samples = 5043
mini_batch_size = 100
n_mini_batches per epoch = 445
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 1
Process ID = 19548

After 1229 epochs, testing accuracy = 0.720 and training accuracy = 0.783
After 3885 epochs, testing accuracy = 0.837 and training accuracy = 0.895

TERMINATED.

68.  T1: Wed Jan 18 12:16:07 2017
Neural net params:
   n_layers = 5
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 64
   layer = 2 n_nodes = 64
   layer = 3 n_nodes = 64
   layer = 4 n_nodes = 64
   n_weights = 16384 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 44442
n_test_samples = 5059
mini_batch_size = 100
n_mini_batches per epoch = 445
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 1
Process ID = 32443

After 739 epochs, test accuracy = 0.654 and training accuracy = 0.678
After 3885 epochs, test accuracy = 0.637 and training accuracy = 0.683
TERMINATED.

69.  T3: Experiment 69
Wed Jan 18 12:17:14 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 64
   layer = 3 n_nodes = 64
   n_weights = 20480 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 89026
n_test_samples = 9976
mini_batch_size = 100
n_mini_batches per epoch = 891
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 2
Process ID = 5365

After 290 epochs, test accuracy = 0.487 and train accuracy = 0.510
After 1566 epochs, test accuracy = 0.577 and train accuracy = 0.609
  Both curves have almost leveled out.

TERMINATED.

70.  T1
Experiment 70
Wed Jan 18 15:25:59 2017
Neural net params:
   n_layers = 5
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 64
   layer = 3 n_nodes = 32
   layer = 4 n_nodes = 64
   n_weights = 20480 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 89029
n_test_samples = 9973
mini_batch_size = 100
n_mini_batches per epoch = 891
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 2
Process ID = 22110

After 1294 epochs, test accuracy = 0.605 and training accuracy = 0.638
  Both curves are slowly heading upwards
  TERMINATED


71. T3: Experiment 71
Wed Jan 18 15:27:15 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 64
   layer = 3 n_nodes = 64
   n_weights = 20480 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 133418
n_test_samples = 15085
mini_batch_size = 100
n_mini_batches per epoch = 1335
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 3
Process ID = 24825

After 854 epochs, test accuracy = 0.377 and training accuracy = 0.405
  Both curves have peaked are are heading slightly downwards
TERMINATED.

72.  TM
Experiment 72
Wed Jan 18 15:28:04 2017
Neural net params:
   n_layers = 5
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 64
   layer = 3 n_nodes = 32
   layer = 4 n_nodes = 64
   n_weights = 20480 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 90389
n_test_samples = 10114
mini_batch_size = 100
n_mini_batches per epoch = 904
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 3
Process ID = 35017

After 713 epochs, val accuracy = 0.328 and train accuracy = 0.365
  Both curves are continuing to head upwards
After 1040 epochs, val accuracy = 0.368 and train accuracy = 0.406
TERMINATED

73.  TM Experiment 73
Wed Jan 18 15:29:08 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 128
   layer = 3 n_nodes = 64
   n_weights = 32768 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 90324
n_test_samples = 10179
mini_batch_size = 100
n_mini_batches per epoch = 904
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 3
Process ID = 35163

After 460 epochs, test accuracy = 0.381 and train accuracy = 0.419
  Both curves are slowly heading upwards
After 690 epochs, val accuracy = 0.405 and train accuracy = 0.445
WRONGLY TERMINATED


74.  T1 : Experiment 74
Thu Jan 19 05:25:47 2017
Neural net params:
   n_layers = 5
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 64
   layer = 3 n_nodes = 32
   layer = 4 n_nodes = 64
   n_weights = 20480 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 44460
n_test_samples = 5041
mini_batch_size = 100
n_mini_batches per epoch = 445
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 1
Process ID = 14185

After 1256 epochs, val accuracy = 0.769 and train accuracy = 0.822
TERMINATED.

75.  T1:  Experiment 75
Thu Jan 19 05:27:00 2017
Neural net params:
   n_layers = 5
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 64
   layer = 3 n_nodes = 32
   layer = 4 n_nodes = 64
   n_weights = 20480 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 178001
n_test_samples = 20003
mini_batch_size = 100
n_mini_batches per epoch = 1781
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 4
Process ID = 14199

After 297 epochs, val accuracy = 0.340 and train accuracy = 0.352
TERMINATED.

76. T3: Experiment 76
Thu Jan 19 05:28:31 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 128
   layer = 3 n_nodes = 64
   n_weights = 32768 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 88962
n_test_samples = 10040
mini_batch_size = 100
n_mini_batches per epoch = 890
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 2
Process ID = 30663

After 378 epochs, val accuracy = 0.535 and train accuracy = 0.571
After 567 epochs, val accuracy = 0.533 and train accuracy = 0.568
TERMINATED

77.  T1  Repeat expt 70 but with snapshots enabled

Experiment 77
Thu Jan 19 07:53:54 2017
Neural net params:
   n_layers = 5
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 64
   layer = 3 n_nodes = 32
   layer = 4 n_nodes = 64
   n_weights = 20480 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 89029
n_validation_samples = 9973
mini_batch_size = 100
n_mini_batches per epoch = 891
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 2
Process ID = 2639

After 378 epochs, val accuracy = 0.466 and train accuracy = 0.497
TERMINATED


78.  T3.  Repeat expt 69 with snapshots enabled

Experiment 78
Thu Jan 19 07:56:23 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 64
   layer = 3 n_nodes = 64
   n_weights = 20480 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 89026
n_validation_samples = 9976
mini_batch_size = 100
n_mini_batches per epoch = 891
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 2
Process ID = 16755

After 369 epochs, val accuracy = 0.506 and train accuracy = 0.538
TERMINATED


79.  T3:  Experiment 79  snapshots enabled
Thu Jan 19 07:58:05 2017
Neural net params:
   n_layers = 5
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 64
   layer = 3 n_nodes = 32
   layer = 4 n_nodes = 64
   n_weights = 20480 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 133413
n_validation_samples = 15090
mini_batch_size = 100
n_mini_batches per epoch = 1335
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 3
Process ID = 16792

After 222 epochs, val accuracy = 0.278 and train accuracy = 0.299
TERMINATED



80.  TM  Experiment 80
Thu Jan 19 11:58:32 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 128
   layer = 3 n_nodes = 64
   n_weights = 32768 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 133476
n_validation_samples = 15027
mini_batch_size = 100
n_mini_batches per epoch = 1335
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 3
Process ID = 367

After 81 epochs, val accuracy = 0.315 and train accuracy = 0.347
After 400 epochs, val accuracy = 0.422 and train accuracy = 0.440
  Curves slowly rising

TERMINATED

81.  TM  Experiment 81
Thu Jan 19 11:59:52 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 128
   layer = 3 n_nodes = 64
   n_weights = 32768 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 44528
n_validation_samples = 4973
mini_batch_size = 100
n_mini_batches per epoch = 446
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 1
Process ID = 428

After 244 epochs, val accuracy = 0.614 and train accuracy = 0.694
After 1226 epochs, val accuracy = 0.750 and train accuracy = 0.814

TERMINATED

82.  T3:  Experiment 82
Thu Jan 19 12:03:05 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 128
   layer = 3 n_nodes = 64
   n_weights = 32768 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 178011
n_validation_samples = 19993
mini_batch_size = 100
n_mini_batches per epoch = 1781
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 4
Process ID = 4916

After 83 epochs, val accuracy = 0.311 and train accuracy = 0.329
After 470 epochs, val accuracy = 0.384 and train accuracy = 0.403
Curves slowly decreasing
TERMINATED

83.  T3:  Experiment 83
Thu Jan 19 12:04:23 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 256
   layer = 2 n_nodes = 128
   layer = 3 n_nodes = 64
   n_weights = 57344 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 44589
n_validation_samples = 4912
mini_batch_size = 100
n_mini_batches per epoch = 446
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 1
Process ID = 4946

After 179 epochs, val accuracy = 0.586 and train accuracy = 0.739
After 1044 epochs, val accuracy = 0.682 and train accuracy = 0.794
After 2295 epochs, val accuracy = 0.705 and train accuracy = 0.816

TERMINATED.
84.  T3:  Experiment 84
Thu Jan 19 12:05:09 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 256
   layer = 2 n_nodes = 128
   layer = 3 n_nodes = 64
   n_weights = 57344 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 89004
n_validation_samples = 9998
mini_batch_size = 100
n_mini_batches per epoch = 891
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 2
Process ID = 4958

After 90 epochs, val accuracy = 0.462 and train accuracy = 0.527
After 90 epochs, val accuracy = 0.578 and train accuracy = 0.642
Curves peaked around epoch 300 and heading downwards afterwards

TERMINATED.

85.  T1 Experiment 85
Thu Jan 19 12:07:03 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 256
   layer = 2 n_nodes = 128
   layer = 3 n_nodes = 64
   n_weights = 57344 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 133554
n_validation_samples = 14949
mini_batch_size = 100
n_mini_batches per epoch = 1336
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 3
Process ID = 29152

After 67 epochs, val accuracy = 0.367 and train accuracy = 0.416
After 326 epochs, val accuracy = 0.493 and train accuracy = 0.525
  Curves nearly asymptoted

TERMINATED.


86.  T1 Experiment 86
Thu Jan 19 12:07:56 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 256
   layer = 2 n_nodes = 128
   layer = 3 n_nodes = 64
   n_weights = 57344 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 178060
n_validation_samples = 19944
mini_batch_size = 100
n_mini_batches per epoch = 1781
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 4
Process ID = 29168

After 50 epochs, val accuracy = 0.317 and train accuracy = 0.347
After 239 epochs, val accuracy = 0.512 and train accuracy = 0.544
  Curves both rising nontrivially
After 667 epochs, val accuracy = 0.457 and train accuracy = 0.486
TERMINATED.

87.  T1 Experiment 87
Thu Jan 19 12:10:03 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 256
   layer = 2 n_nodes = 256
   layer = 3 n_nodes = 64
   n_weights = 98304 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 88962
n_validation_samples = 10040
mini_batch_size = 100
n_mini_batches per epoch = 890
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 2
Process ID = 29190

After 56 epochs, val accuracy = 0.467 and train accuracy = 0.557
After 267 epochs, val accuracy = 0.587 and train accuracy = 0.684

88.  T1 Experiment 88
Thu Jan 19 12:10:30 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 256
   layer = 2 n_nodes = 256
   layer = 3 n_nodes = 64
   n_weights = 98304 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 133498
n_validation_samples = 15005
mini_batch_size = 100
n_mini_batches per epoch = 1335
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 3
Process ID = 29204

After 30 epochs, val accuracy = 0.337 and train accuracy = 0.396
After 178 epochs, val accuracy = 0.477 and train accuracy = 0.530
  Curves slowly rising
After 764 epochs, val accuracy = 0.647 and train accuracy = 0.718


91.  T1  Experiment 91
Fri Jan 20 06:46:19 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 256
   layer = 2 n_nodes = 256
   layer = 3 n_nodes = 64
   n_weights = 98304 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 321008
n_validation_samples = 35848
mini_batch_size = 100
n_mini_batches per epoch = 3211
Max n_training_epochs = 5000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 4
Process ID = 5458

After 121 epochs, val accuracy = 0.545 and train accuracy = 0.565


92.  T3:  Experiment 92
Fri Jan 20 06:40:40 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 256
   layer = 2 n_nodes = 256
   layer = 3 n_nodes = 64
   n_weights = 98304 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 321008
n_validation_samples = 35848
mini_batch_size = 100
n_mini_batches per epoch = 3211
Max n_training_epochs = 5000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 4
Process ID = 28469

After 77 epochs, val accuracy = 0.531 and train accuracy = 0.555

93.  T3.  Experiment 93
Fri Jan 20 06:41:49 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 256
   layer = 2 n_nodes = 256
   layer = 3 n_nodes = 64
   n_weights = 98304 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 401181
n_validation_samples = 44822
mini_batch_size = 100
n_mini_batches per epoch = 4012
Max n_training_epochs = 5000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 5
Process ID = 28485

After 64 epochs, val accuracy = 0.533 and train accuracy = 0.546


94.  T3  Experiment 94
Fri Jan 20 06:42:51 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 256
   layer = 2 n_nodes = 256
   layer = 3 n_nodes = 64
   n_weights = 98304 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 401181
n_validation_samples = 44822
mini_batch_size = 100
n_mini_batches per epoch = 4012
Max n_training_epochs = 5000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 5
Process ID = 28498

After 64 epochs, val accuracy = 0.533 and train accuracy = 0.546

89.  TM  Experiment 89
Fri Jan 20 06:46:38 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 256
   layer = 2 n_nodes = 256
   layer = 3 n_nodes = 64
   n_weights = 98304 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 80197
n_validation_samples = 9040
mini_batch_size = 100
n_mini_batches per epoch = 802
Max n_training_epochs = 5000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 1
Process ID = 2131

After 395 epochs, valiation accuracy = 0.785 and training accuracy = 0.832

90.  TM   Experiment 90
Fri Jan 20 06:48:09 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 256
   layer = 2 n_nodes = 256
   layer = 3 n_nodes = 64
   n_weights = 98304 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 240820
n_validation_samples = 26930
mini_batch_size = 100
n_mini_batches per epoch = 2409
Max n_training_epochs = 5000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 3
Process ID = 2284

After 123 epochs, valiation accuracy = 0.529 and training accuracy = 0.543

95.  T1 Experiment 95  with weight/bias symmetrization

Sat Jan 21 15:53:32 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 256
   layer = 2 n_nodes = 256
   layer = 3 n_nodes = 64
   n_weights = 98304 (FC)
include_bias_terms = 1
base_learning_rate = 0.001
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 80416
n_validation_samples = 8821
mini_batch_size = 100
n_mini_batches per epoch = 805
perm_symmetrize_weights_and_biases = 1
Max n_training_epochs = 3000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 1
Random seed = -42751
Process ID = 32523
TERMINATED.

96.  T1 Experiment 96 with weight/bias symmetrization

Sat Jan 21 15:53:42 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 256
   layer = 2 n_nodes = 256
   layer = 3 n_nodes = 64
   n_weights = 98304 (FC)
include_bias_terms = 1
base_learning_rate = 0.001
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 160641
n_validation_samples = 17705
mini_batch_size = 100
n_mini_batches per epoch = 1607
perm_symmetrize_weights_and_biases = 1
Max n_training_epochs = 3000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 2
Random seed = -42811
Process ID = 32532
TERMINATED

97.  T1: Experiment 97 with weight/bias symmetrization

Sat Jan 21 15:53:50 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 256
   layer = 2 n_nodes = 256
   layer = 3 n_nodes = 64
   n_weights = 98304 (FC)
include_bias_terms = 1
base_learning_rate = 0.001
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 240811
n_validation_samples = 26939
mini_batch_size = 100
n_mini_batches per epoch = 2409
perm_symmetrize_weights_and_biases = 1
Max n_training_epochs = 3000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 3
Random seed = -42818
Process ID = 32540
TERMINATED

98.  T1 Experiment 98 with weight/bias symmetrization

Sat Jan 21 15:54:03 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 256
   layer = 2 n_nodes = 256
   layer = 3 n_nodes = 64
   n_weights = 98304 (FC)
include_bias_terms = 1
base_learning_rate = 0.001
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 321324
n_validation_samples = 35532
mini_batch_size = 100
n_mini_batches per epoch = 3214
perm_symmetrize_weights_and_biases = 1
Max n_training_epochs = 3000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 4
Random seed = -42826
Process ID = 32548
DIED when TM went down

100.  T1  Experiment 100 with weight/bias symmetrization

Sat Jan 21 15:54:46 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 256
   layer = 2 n_nodes = 256
   layer = 3 n_nodes = 64
   n_weights = 98304 (FC)
include_bias_terms = 1
base_learning_rate = 0.001
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 481587
n_validation_samples = 53418
mini_batch_size = 100
n_mini_batches per epoch = 4816
perm_symmetrize_weights_and_biases = 1
Max n_training_epochs = 3000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 6
Random seed = -42867
Process ID = 32578
DIED when TM went down


101.  T3  Experiment 101 with weight/bias symmetrization

Sat Jan 21 15:58:21 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 256
   layer = 2 n_nodes = 256
   layer = 3 n_nodes = 64
   n_weights = 98304 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 80238
n_validation_samples = 8999
mini_batch_size = 100
n_mini_batches per epoch = 803
perm_symmetrize_weights_and_biases = 1
Max n_training_epochs = 3000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 1
Random seed = -43025
Process ID = 7180
Died when TM went down

102.  T3  Experiment 102 with weight/bias symmetrization
Sat Jan 21 15:58:35 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 256
   layer = 2 n_nodes = 256
   layer = 3 n_nodes = 64
   n_weights = 98304 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 160607
n_validation_samples = 17739
mini_batch_size = 100
n_mini_batches per epoch = 1607
perm_symmetrize_weights_and_biases = 1
Max n_training_epochs = 3000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 2
Random seed = -43100
Process ID = 7190
TERMINATED

103.  T3  Experiment 103 with weight/bias symmetrization

Sat Jan 21 15:58:43 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 256
   layer = 2 n_nodes = 256
   layer = 3 n_nodes = 64
   n_weights = 98304 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 240886
n_validation_samples = 26864
mini_batch_size = 100
n_mini_batches per epoch = 2409
perm_symmetrize_weights_and_biases = 1
Max n_training_epochs = 3000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 3
Random seed = -43109
Process ID = 7198
TERMINATED


104.  T3  Experiment 104 with weight/bias symmetrization

Sat Jan 21 16:01:25 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 256
   layer = 2 n_nodes = 256
   layer = 3 n_nodes = 64
   n_weights = 98304 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 321228
n_validation_samples = 35628
mini_batch_size = 100
n_mini_batches per epoch = 3213
perm_symmetrize_weights_and_biases = 1
Max n_training_epochs = 3000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 4
Random seed = -43118
Process ID = 7206
TERMINATED

107.  TM  Experiment 107 with weight/bias symmetrization
Sat Jan 21 16:03:57 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 256
   layer = 2 n_nodes = 256
   layer = 3 n_nodes = 64
   n_weights = 98304 (FC)
include_bias_terms = 1
base_learning_rate = 0.003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 80358
n_validation_samples = 8879
mini_batch_size = 100
n_mini_batches per epoch = 804
perm_symmetrize_weights_and_biases = 1
Max n_training_epochs = 3000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 1
Random seed = -43401
Process ID = 11106
DIED when TM went down

108.  TM  Experiment 108 with weight/bias symmetrization
Experiment 108
Sat Jan 21 16:04:06 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 256
   layer = 2 n_nodes = 256
   layer = 3 n_nodes = 64
   n_weights = 98304 (FC)
include_bias_terms = 1
base_learning_rate = 0.003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 160506
n_validation_samples = 17840
mini_batch_size = 100
n_mini_batches per epoch = 1606
perm_symmetrize_weights_and_biases = 1
Max n_training_epochs = 3000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 2
Random seed = -43432
Process ID = 11123
DIED when TM went down

109.  TM  Experiment 109 with weight/bias symmetrization
Sat Jan 21 16:04:20 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 256
   layer = 2 n_nodes = 256
   layer = 3 n_nodes = 64
   n_weights = 98304 (FC)
include_bias_terms = 1
base_learning_rate = 0.003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 240886
n_validation_samples = 26864
mini_batch_size = 100
n_mini_batches per epoch = 2409
perm_symmetrize_weights_and_biases = 1
Max n_training_epochs = 3000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 3
Random seed = -43440
Process ID = 11126
DIED when TM went down

110.  TM  Experiment 110 with weight/bias symmetrization
Sat Jan 21 16:04:28 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 256
   layer = 2 n_nodes = 256
   layer = 3 n_nodes = 64
   n_weights = 98304 (FC)
include_bias_terms = 1
base_learning_rate = 0.003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 320924
n_validation_samples = 35932
mini_batch_size = 100
n_mini_batches per epoch = 3210
perm_symmetrize_weights_and_biases = 1
Max n_training_epochs = 3000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 4
Random seed = -43454
Process ID = 11134
DIED when TM went down

111.  TM  Experiment 111 with weight/bias symmetrization
Sat Jan 21 16:04:38 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 256
   layer = 2 n_nodes = 256
   layer = 3 n_nodes = 64
   n_weights = 98304 (FC)
include_bias_terms = 1
base_learning_rate = 0.003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 401512
n_validation_samples = 44491
mini_batch_size = 100
n_mini_batches per epoch = 4016
perm_symmetrize_weights_and_biases = 1
Max n_training_epochs = 3000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 5
Random seed = -43462
Process ID = 11141
DIED when TM went down

112.  TM  Experiment 112 with weight/bias symmetrization

Sat Jan 21 16:04:51 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 256
   layer = 2 n_nodes = 256
   layer = 3 n_nodes = 64
   n_weights = 98304 (FC)
include_bias_terms = 1
base_learning_rate = 0.003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 481640
n_validation_samples = 53365
mini_batch_size = 100
n_mini_batches per epoch = 4817
perm_symmetrize_weights_and_biases = 1
Max n_training_epochs = 3000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 6
Random seed = -43476
Process ID = 11154
DIED when TM went down

------------------------------------------------------
RUNNING EXPERIMENTS
------------------------------------------------------

---------------------
Titan 1:
---------------------

99.  T1  Experiment 99 with weight/bias symmetrization

Sat Jan 21 15:54:24 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 256
   layer = 2 n_nodes = 256
   layer = 3 n_nodes = 64
   n_weights = 98304 (FC)
include_bias_terms = 1
base_learning_rate = 0.001
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 401383
n_validation_samples = 44620
mini_batch_size = 100
n_mini_batches per epoch = 4014
perm_symmetrize_weights_and_biases = 1
Max n_training_epochs = 3000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 5
Random seed = -42844
Process ID = 32566

115.  T1  Experiment 115 with weight/bias symmetrization; lambda = 0
Sun Jan 22 11:39:10 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 256
   layer = 2 n_nodes = 256
   layer = 3 n_nodes = 64
   n_weights = 98304 (FC)
include_bias_terms = 1
base_learning_rate = 0.001
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
n_training_samples = 401749
n_validation_samples = 44254
mini_batch_size = 100
n_mini_batches per epoch = 4018
perm_symmetrize_weights_and_biases = 1
Max n_training_epochs = 3000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 5
Random seed = -13883
Process ID = 433

116.  T1  Experiment 116 with weight/bias symmetrization; lambda = 0
Sun Jan 22 11:39:28 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 256
   layer = 2 n_nodes = 256
   layer = 3 n_nodes = 64
   n_weights = 98304 (FC)
include_bias_terms = 1
base_learning_rate = 0.001
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
n_training_samples = 481170
n_validation_samples = 53835
mini_batch_size = 100
n_mini_batches per epoch = 4812
perm_symmetrize_weights_and_biases = 1
Max n_training_epochs = 3000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 6
Random seed = -13954
Process ID = 443

117.  T1  Experiment 117 with NO symmetrization
Sun Jan 22 12:01:54 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 256
   layer = 2 n_nodes = 256
   layer = 3 n_nodes = 64
   n_weights = 98304 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 641737
n_validation_samples = 71400
mini_batch_size = 100
n_mini_batches per epoch = 6418
perm_symmetrize_weights_and_biases = 0
Max n_training_epochs = 3000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 8
Random seed = -15291
Process ID = 2279

118.  T1  Experiment 118 with NO symmetrization
Sun Jan 22 12:02:09 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 256
   layer = 2 n_nodes = 256
   layer = 3 n_nodes = 64
   n_weights = 98304 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 962605
n_validation_samples = 106955
mini_batch_size = 100
n_mini_batches per epoch = 9627
perm_symmetrize_weights_and_biases = 0
Max n_training_epochs = 3000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 12
Random seed = -15310
Process ID = 2288

119.  T1  Experiment 119 with NO symmetrization
Sun Jan 22 12:02:50 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 256
   layer = 2 n_nodes = 256
   layer = 3 n_nodes = 64
   n_weights = 98304 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 1283145
n_validation_samples = 142579
mini_batch_size = 100
n_mini_batches per epoch = 12832
perm_symmetrize_weights_and_biases = 0
Max n_training_epochs = 3000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 16
Random seed = -15337
Process ID = 2301

---------------------
Titan 3:
---------------------

105.  T3  Experiment 105 with weight/bias symmetrization
Sat Jan 21 15:59:04 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 256
   layer = 2 n_nodes = 256
   layer = 3 n_nodes = 64
   n_weights = 98304 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 401331
n_validation_samples = 44672
mini_batch_size = 100
n_mini_batches per epoch = 4014
perm_symmetrize_weights_and_biases = 1
Max n_training_epochs = 3000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 5
Random seed = -43126
Process ID = 7215

106.  T3  Experiment 106 with weight/bias symmetrization
Sat Jan 21 15:59:14 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 256
   layer = 2 n_nodes = 256
   layer = 3 n_nodes = 64
   n_weights = 98304 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 481717
n_validation_samples = 53288
mini_batch_size = 100
n_mini_batches per epoch = 4818
perm_symmetrize_weights_and_biases = 1
Max n_training_epochs = 3000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 6
Random seed = -43138
Process ID = 7223

117.  T3  Experiment 117 with weight/bias symmetrization; no L2 reg
Sun Jan 22 11:42:38 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 256
   layer = 2 n_nodes = 256
   layer = 3 n_nodes = 64
   n_weights = 98304 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
n_training_samples = 401089
n_validation_samples = 44914
mini_batch_size = 100
n_mini_batches per epoch = 4011
perm_symmetrize_weights_and_biases = 1
Max n_training_epochs = 3000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 5
Random seed = -14138
Process ID = 11828

118.  T3  Experiment 118 with weight/bias symmetrization; no L2 reg
Sun Jan 22 11:42:44 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 256
   layer = 2 n_nodes = 256
   layer = 3 n_nodes = 64
   n_weights = 98304 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
n_training_samples = 481601
n_validation_samples = 53404
mini_batch_size = 100
n_mini_batches per epoch = 4817
perm_symmetrize_weights_and_biases = 1
Max n_training_epochs = 3000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 6
Random seed = -14150
Process ID = 11836


---------------------
Thinkmate
---------------------

113.  TM  Experiment 113 with weight/bias symmetrization; no L2 reg
Sun Jan 22 11:33:18 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 256
   layer = 2 n_nodes = 256
   layer = 3 n_nodes = 64
   n_weights = 98304 (FC)
include_bias_terms = 1
base_learning_rate = 0.003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
n_training_samples = 401427
n_validation_samples = 44576
mini_batch_size = 100
n_mini_batches per epoch = 4015
perm_symmetrize_weights_and_biases = 1
Max n_training_epochs = 3000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 5
Random seed = -13576
Process ID = 35837

114.  TM  Experiment 113 with weight/bias symmetrization; no L2 reg
Sun Jan 22 11:33:38 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 256
   layer = 2 n_nodes = 256
   layer = 3 n_nodes = 64
   n_weights = 98304 (FC)
include_bias_terms = 1
base_learning_rate = 0.003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
n_training_samples = 481449
n_validation_samples = 53556
mini_batch_size = 100
n_mini_batches per epoch = 4815
perm_symmetrize_weights_and_biases = 1
Max n_training_epochs = 3000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 6
Random seed = -13593
Process ID = 35849

---------------------
m6700
---------------------

==========================================================================
Conclusions:

*.  As of 1 pm on Mon Jan 16, 2017, it looks like base learning rate must
be no larger than 1E6 to avoid horrible loss divergence behavior, at least
when lambda = 1E-3.

*.  H1 = 128, H2 = 64, lambda = 1E-4, BLR = 3E-4, switch colors yields 0.85
test accuracy for max move rel to game end = 1.

*. But H1 = 128, H2 = 64, H3 = 32, lambda = 1E-4, BLR = 3E-4, switch colors
may be better for max move rel to game end = 2 and 3.  

*.  As of noon on Thurs Jan 19, 2017, we've found that H1 = H2 = 128,
lambda = 1E-4, BLR = 3E-4, switch colors yields better validation
accuracies for max move rel to game end = 2 & 3 than H1 = 128, H2 = 64; H1
= 128, H2 = 64, H3 = 32.

*.  As of 5:40 am on Fri Jan 20, it looks like having hidden layers with
256 nodes yields better results than having just 128 nodes.

*.  As of Sat Jan 21, we definitely see that H1 = H2 = 256 yields better
validation accuracies than H1 = 256, H2 = 128 for max move rel to game end
= 1, 2, 3, 4, 5!



Interesting experiments:

H1 = 256, H2 = 256 with no symmetrization of weights or biases

  87.  end moves = 2
  88.  end moves = 3
  89.  end moves = 1
  90.  end moves = 3
  91.  end moves = 4
  92.  end moves = 4
  93.  end moves = 5
  94.  end moves = 5


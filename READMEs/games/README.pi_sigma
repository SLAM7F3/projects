==========================================================================
Training pi_sigma for TTT
==========================================================================
Last updated on 1/16/17; 1/17/17; 1/18/17; 1/19/17
==========================================================================

Best results so far:

66. T1:  Experiment with forcing player board state to always correspond to
agent value = +1

Experiment 66
Wed Jan 18 08:17:24 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 64
   layer = 3 n_nodes = 64
   n_weights = 20480 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 44458
n_test_samples = 5043
mini_batch_size = 100
n_mini_batches per epoch = 445
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 1
Process ID = 28193

After 712 epochs, test accuracy = 0.683 and training accuracy = 0.734
After 1370 epochs, test accuracy = 0.756 and training accuracy = 0.812
After 4058 epochs, test accuracy = 0.850 and training accuracy = 0.909

------------------------------------------------------
TERMINATED EXPERIMENTS
------------------------------------------------------

5.  m6700

Experiment 5
Mon Jan 16 10:28:40 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 65
   layer = 1 n_nodes = 32
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 64
   n_weights = 5152 (FC)
base_learning_rate = 0.0001; batch_size = 32
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.001
n_training_samples = 85135
n_test_samples = 9438
Max n_training_epochs = 1000
Leaky ReLU small slope = 0.01
Process ID = 5253

Bad loss behavior

10.  T1
H1 = 64, H2 = 32, H3 = 0
lambda = 1E-3
base_learning_rate = 3E-4

Bad loss behavior

11.  T1
H1 = 64, H2 = 32, H3 = 0
lambda = 1E-3
base_learning_rate = 1E-4

Bad loss behavior

12.  T1
H1 = 64, H2 = 32, H3 = 0
lambda = 1E-3
base_learning_rate = 3E-5

Bad loss behavior

13.  T3
H1 = 32, H2 = 32, H3 = 0
lambda = 1E-3
base_learning_rate = 3E-4

Bad loss behavior

14.  T3
H1 = 32, H2 = 32, H3 = 0
lambda = 1E-3
base_learning_rate = 1E-4

Bad loss behavior

15.  T3
H1 = 32, H2 = 32, H3 = 0
lambda = 1E-3
base_learning_rate = 3E-5

Bad loss behavior


16.  TM
H1 = 32, H2 = 32, H3 = 32
lambda = 1E-3
base_learning_rate = 3E-4

Bad loss behavior

17.  TM
H1 = 32, H2 = 32, H3 = 32
lambda = 1E-3
base_learning_rate = 1E-4

Bad loss behavior


18.  T1
H1 = 64, H2 = 32, H3 = 0
lambda = 1E-3
base_learning_rate = 1E-6

19.  T1
H1 = 64, H2 = 32, H3 = 0
lambda = 1E-3
base_learning_rate = 3E-7

20.  T1
H1 = 32, H2 = 32, H3 = 0
lambda = 1E-3
base_learning_rate = 1E-6

27.  T1
H1 = 64, H2 = 32, H3 = 0
lambda = 0
base_learning_rate = 3E-5

21.  T3
H1 = 32, H2 = 32, H3 = 0
lambda = 1E-3
base_learning_rate = 3E-7

22.  T3
H1 = 32, H2 = 32, H3 = 32
lambda = 1E-3
base_learning_rate = 1E-6

23.  T3
H1 = 32, H2 = 32, H3 = 32
lambda = 1E-3
base_learning_rate = 3E-7

28.  T1
H1 = 64, H2 = 64, H3 = 0
lambda = 0
base_learning_rate = 3E-5

24.  TM
H1 = 32, H2 = 32, H3 = 0
lambda = 0
base_learning_rate = 3E-5

25.  TM
H1 = 32, H2 = 32, H3 = 0
lambda = 0
base_learning_rate = 1E-4

26.  TM
H1 = 32, H2 = 32, H3 = 0
lambda = 0
base_learning_rate = 1E-5

30.  T1
max move rel to end = 1
H1 = H2 = 32
lambda = 0.01
base_learning_rate = 1E-

31.  T1
max move rel to end = 1
H1 = H2 = 32
lambda = 0.001
base_learning_rate = 3E-5

32.  T1
max move rel to end = 1
H1 = H2 = 32
lambda = 0.003
base_learning_rate = 3E-5

33.  T1
max move rel to end = 1
H1 = H2 = 32
lambda = 0.005
base_learning_rate = 1E-6

Loss basically stayed under control.  
testing accuracy = 0.0395; training accuracy = 0.075


34.  T1
max move rel to end = 1
H1 = 64; H2 = 32
lambda = 0.005
base_learning_rate = 1E-6

Experiment 34
Mon Jan 16 18:43:58 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 65
   layer = 1 n_nodes = 64
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 64
   n_weights = 8256 (FC)
base_learning_rate = 1e-06; batch_size = 32
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.005
n_training_samples = 4509
n_test_samples = 492
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 1
Process ID = 6954

Loss basically stayed under control.  
testing accuracy = 0.0569; training accuracy = 0.105

35.  T1
max move rel to end = 1
H1 = 32; H2 = 32; H3 = 32
lambda = 0.005
base_learning_rate = 1E-6

Loss basically stayed under control.  
testing accuracy = 0.0535; training accuracy = 0.0701


36.  T1
H1 = 64; H2 = 32; H3 = 0
lambda = 0.005
base_learning_rate = 3E-6
max move rel to end = 1


37.  T1
H1 = 64; H2 = 32; H3 = 0
lambda = 0.005
base_learning_rate = 1E-5
max move rel to end = 1

38.  T1
H1 = 64; H2 = 32; H3 = 0
lambda = 0.005
base_learning_rate = 3E-5
max move rel to end = 1

After 340+ epochs, test accuracy and training accuracy around 0.1


39.  T3
H1 = 128; H2 = 32; H3 = 0
lambda = 0.005
base_learning_rate = 3E-6
max move rel to end = 1

40.  T3
H1 = 128; H2 = 32; H3 = 0
lambda = 0.005
base_learning_rate = 1E-5
max move rel to end = 1

41.  T3
H1 = 128; H2 = 32; H3 = 0
lambda = 0.005
base_learning_rate = 3E-5
max move rel to end = 1


42.  T1
H1 = 64; H2 = 32; H3 = 0
lambda = 0.001
base_learning_rate = 1E-4
max move rel to end = 1

Experiment 42
Tue Jan 17 05:46:33 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 65
   layer = 1 n_nodes = 64
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 64
   n_weights = 8256 (FC)
base_learning_rate = 0.0001; batch_size = 32
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.001
n_training_samples = 31524
n_test_samples = 3477
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 1
Process ID = 2235

After 160 epochs, test accuracy = 0.252 and train accuracy = 0.291


43.  T3
H1 = 128; H2 = 32; H3 = 0
lambda = 0.001
base_learning_rate = 1E-4
max move rel to end = 1

After 1018 epochs, test accuracy = 0.395 & train accuracy = 0.442

44.  T1
H1 = 64; H2 = 64; H3 = 0
lambda = 0.005
base_learning_rate = 1E-5
max move rel to end = 1

45.  T3
H1 = 128; H2 = 64; H3 = 0
lambda = 0.005
base_learning_rate = 1E-5
max move rel to end = 1

After 101 epochs, test accuracy = 0.287 and training accuracy = 0.319
Loss diverging

46.  T1
H1 = 64; H2 = 32; H3 = 0
lambda = 0.001
base_learning_rate = 3E-4
max move rel to end = 1

Experiment 46
Tue Jan 17 06:22:52 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 65
   layer = 1 n_nodes = 64
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 64
   n_weights = 8256 (FC)
base_learning_rate = 0.0003; batch_size = 100
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.001
n_training_samples = 31524
n_test_samples = 3477
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 1
Process ID = 18761

After 190 epochs, test accuracy = 0.309 and train accuracy = 0.343


47.  T1
H1 = 64; H2 = 32; H3 = 0
lambda = 0.003
base_learning_rate = 3E-4
max move rel to end = 1

48.  T1
H1 = 64; H2 = 32; H3 = 0
lambda = 0.005
base_learning_rate = 3E-4
max move rel to end = 1

49.  T3
H1 = 128; H2 = 32; H3 = 0
lambda = 0.001
base_learning_rate = 3E-4
max move rel to end = 1

After 1288 epochs, test accuracy = 0.444 and train accuracy = 0.516

50.  T3
H1 = 128; H2 = 32; H3 = 0
lambda = 0.003
base_learning_rate = 3E-4
max move rel to end = 1

51.  T3
H1 = 128; H2 = 32; H3 = 0
lambda = 0.005
base_learning_rate = 3E-4
max move rel to end = 1

52.  T1
H1 = 64; H2 = 64; H3 = 0
lambda = 0.0003
base_learning_rate = 1E-3
max move rel to end = 1

After 1.6K epochs, test accuracy = 0.372

53.  T1
H1 = 64; H2 = 128; H3 = 0
lambda = 0.0003
base_learning_rate = 1E-3
max move rel to end = 1

After 1073 epochs, test accuracy = 0.352

54.  T3
H1 = 128; H2 = 64; H3 = 0
lambda = 0.0003
base_learning_rate = 3E-4
max move rel to end = 1

After 853 epochs, test accuracy = 0.400 and train accuracy = 0.512
After 3557 epochs, test accuracy = 0.519 and train accuracy = 0.613


55.  T3
H1 = 128; H2 = 128; H3 = 0
lambda = 0.0003
base_learning_rate = 3E-4
max move rel to end = 1
Experiment 55
Tue Jan 17 06:57:34 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 65
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 128
   layer = 3 n_nodes = 64
   n_weights = 32896 (FC)
base_learning_rate = 0.0003; batch_size = 100
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0003
n_training_samples = 31474
n_test_samples = 3527
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 1
Process ID = 19850

After 536 epochs, test accuracy = 0.398 and train accuracy = 0.542


58.  Repeat expt 54 from T3 with streamlined code
H1 = 128; H2 = 64; H3 = 0
lambda = 0.0003
base_learning_rate = 3E-4
max move rel to end = 1

Tue Jan 17 13:20:04 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 65
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 64
   layer = 3 n_nodes = 64
   n_weights = 20608 (FC)
base_learning_rate = 0.0003; batch_size = 100
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0003
n_training_samples = 44915
n_test_samples = 5085
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 1
Process ID = 17352

After 1673 epochs, testing accuracy = 0.522 and training accuracy = 0.587
After 2688 epcohs, testing accuracy = 0.577 and training accuracy = 0.638

59.  Repeat expt 55 from T3 with streamline code
H1 = 128; H2 = 128; H3 = 0
lambda = 0.0003
base_learning_rate = 3E-4
max move rel to end = 1


60.   Experiment 60 on Titan 1:
Tue Jan 17 15:49:40 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 65
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 64
   layer = 3 n_nodes = 64
   n_weights = 20608 (FC)
base_learning_rate = 0.0003; batch_size = 100
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0003
n_training_samples = 44915
n_test_samples = 5085
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 1
Process ID = 5356

After 2202 epochs, test accuracy = 0.569322 and training accuracy = 0.636
After 3.2K epochs, test accuracy = 0.578 and training accuracy = 0.664


60.  Repeat expt 54 from T3 with code with transposed weights matrix
H1 = 128; H2 = 64; H3 = 0
lambda = 0.0003
base_learning_rate = 3E-4
max move rel to end = 1

Experiment 60 on Titan 3:
Tue Jan 17 14:36:54 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 65
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 64
   layer = 3 n_nodes = 64
   n_weights = 20608 (FC)
base_learning_rate = 0.0003; batch_size = 100
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0003
n_training_samples = 44915
n_test_samples = 5085
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 1
Process ID = 29550


After 1566 epochs, testing accuracy = 0.515 and training accuracy = 0.582


61.  Repeat expt 55 from T3 with code with transposed weights matrix
H1 = 128; H2 = 128; H3 = 0
lambda = 0.0003
base_learning_rate = 3E-4
max move rel to end = 1

Results not as good as H1 = 64, H2 = 128!

62.  Repeat experiment 60 on Titan 1 after fixing serious bug in genvector

Experiment 62
Wed Jan 18 05:42:56 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 65
   layer = 1 n_nodes = 64
   layer = 2 n_nodes = 128
   layer = 3 n_nodes = 64
   n_weights = 20544 (FC)
base_learning_rate = 0.0003; batch_size = 100
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0003
n_training_samples = 44461
n_test_samples = 5040
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 1
Process ID = 9036

OK results


63.  Repeat experiment 60 on Titan 3 after fixing serious bug in genvector

Experiment 63
Wed Jan 18 05:44:00 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 65
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 64
   layer = 3 n_nodes = 64
   n_weights = 20608 (FC)
base_learning_rate = 0.0003; batch_size = 100
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0003
n_training_samples = 44915
n_test_samples = 5085
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 1
Process ID = 4901

Good results

64.  Repeat experiment 60 on Titan 1 after fixing serious bug in genvector
and streamlining regularization term

 Wed Jan 18 06:32:31 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 65
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 64
   layer = 3 n_nodes = 64
   n_weights = 20608 (FC)
base_learning_rate = 0.0003; batch_size = 100
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 44462
n_test_samples = 5039
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 1
Process ID = 21235

After 1047 epochs, test accuracy = 0.483 and training accuracy = 0.526

65.  Repeat experiment 60 on Titan 3 after fixing serious bug in genvector
and streamlining regularization term

Experiment 65
Wed Jan 18 06:33:33 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 65
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 64
   layer = 3 n_nodes = 64
   n_weights = 20608 (FC)
base_learning_rate = 0.0003; batch_size = 100
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 44462
n_test_samples = 5039
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 1
Process ID = 19380


66. T1:  Experiment with forcing player board state to always correspond to
agent value = +1

Experiment 66
Wed Jan 18 08:17:24 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 64
   layer = 3 n_nodes = 64
   n_weights = 20480 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 44458
n_test_samples = 5043
mini_batch_size = 100
n_mini_batches per epoch = 445
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 1
Process ID = 28193

After 712 epochs, test accuracy = 0.683 and training accuracy = 0.734
After 1370 epochs, test accuracy = 0.756 and training accuracy = 0.812
After 4058 epochs, test accuracy = 0.850 and training accuracy = 0.909
TERMINATED


67. T3: Experiment with forcing player board state to always correspond to
agent value = +1

Experiment 67
Wed Jan 18 08:18:42 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 64
   layer = 3 n_nodes = 64
   n_weights = 20480 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 44458
n_test_samples = 5043
mini_batch_size = 100
n_mini_batches per epoch = 445
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 1
Process ID = 19548

After 1229 epochs, testing accuracy = 0.720 and training accuracy = 0.783
After 3885 epochs, testing accuracy = 0.837 and training accuracy = 0.895

TERMINATED.

68.  T1: Wed Jan 18 12:16:07 2017
Neural net params:
   n_layers = 5
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 64
   layer = 2 n_nodes = 64
   layer = 3 n_nodes = 64
   layer = 4 n_nodes = 64
   n_weights = 16384 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 44442
n_test_samples = 5059
mini_batch_size = 100
n_mini_batches per epoch = 445
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 1
Process ID = 32443

After 739 epochs, test accuracy = 0.654 and training accuracy = 0.678
After 3885 epochs, test accuracy = 0.637 and training accuracy = 0.683
TERMINATED.

69.  T3: Experiment 69
Wed Jan 18 12:17:14 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 64
   layer = 3 n_nodes = 64
   n_weights = 20480 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 89026
n_test_samples = 9976
mini_batch_size = 100
n_mini_batches per epoch = 891
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 2
Process ID = 5365

After 290 epochs, test accuracy = 0.487 and train accuracy = 0.510
After 1566 epochs, test accuracy = 0.577 and train accuracy = 0.609
  Both curves have almost leveled out.

TERMINATED.

70.  T1
Experiment 70
Wed Jan 18 15:25:59 2017
Neural net params:
   n_layers = 5
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 64
   layer = 3 n_nodes = 32
   layer = 4 n_nodes = 64
   n_weights = 20480 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 89029
n_test_samples = 9973
mini_batch_size = 100
n_mini_batches per epoch = 891
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 2
Process ID = 22110

After 1294 epochs, test accuracy = 0.605 and training accuracy = 0.638
  Both curves are slowly heading upwards
  TERMINATED


71. T3: Experiment 71
Wed Jan 18 15:27:15 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 64
   layer = 3 n_nodes = 64
   n_weights = 20480 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 133418
n_test_samples = 15085
mini_batch_size = 100
n_mini_batches per epoch = 1335
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 3
Process ID = 24825

After 854 epochs, test accuracy = 0.377 and training accuracy = 0.405
  Both curves have peaked are are heading slightly downwards
TERMINATED.

72.  TM
Experiment 72
Wed Jan 18 15:28:04 2017
Neural net params:
   n_layers = 5
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 64
   layer = 3 n_nodes = 32
   layer = 4 n_nodes = 64
   n_weights = 20480 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 90389
n_test_samples = 10114
mini_batch_size = 100
n_mini_batches per epoch = 904
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 3
Process ID = 35017

After 713 epochs, val accuracy = 0.328 and train accuracy = 0.365
  Both curves are continuing to head upwards
After 1040 epochs, val accuracy = 0.368 and train accuracy = 0.406
TERMINATED

73.  TM Experiment 73
Wed Jan 18 15:29:08 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 128
   layer = 3 n_nodes = 64
   n_weights = 32768 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 90324
n_test_samples = 10179
mini_batch_size = 100
n_mini_batches per epoch = 904
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 3
Process ID = 35163

After 460 epochs, test accuracy = 0.381 and train accuracy = 0.419
  Both curves are slowly heading upwards
After 690 epochs, val accuracy = 0.405 and train accuracy = 0.445
WRONGLY TERMINATED


74.  T1 : Experiment 74
Thu Jan 19 05:25:47 2017
Neural net params:
   n_layers = 5
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 64
   layer = 3 n_nodes = 32
   layer = 4 n_nodes = 64
   n_weights = 20480 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 44460
n_test_samples = 5041
mini_batch_size = 100
n_mini_batches per epoch = 445
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 1
Process ID = 14185

After 1256 epochs, val accuracy = 0.769 and train accuracy = 0.822
TERMINATED.

75.  T1:  Experiment 75
Thu Jan 19 05:27:00 2017
Neural net params:
   n_layers = 5
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 64
   layer = 3 n_nodes = 32
   layer = 4 n_nodes = 64
   n_weights = 20480 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 178001
n_test_samples = 20003
mini_batch_size = 100
n_mini_batches per epoch = 1781
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 4
Process ID = 14199

After 297 epochs, val accuracy = 0.340 and train accuracy = 0.352
TERMINATED.

77.  T1  Repeat expt 70 but with snapshots enabled

Experiment 77
Thu Jan 19 07:53:54 2017
Neural net params:
   n_layers = 5
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 64
   layer = 3 n_nodes = 32
   layer = 4 n_nodes = 64
   n_weights = 20480 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 89029
n_validation_samples = 9973
mini_batch_size = 100
n_mini_batches per epoch = 891
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 2
Process ID = 2639

After 378 epochs, val accuracy = 0.466 and train accuracy = 0.497
TERMINATED


78.  T3.  Repeat expt 69 with snapshots enabled

Experiment 78
Thu Jan 19 07:56:23 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 64
   layer = 3 n_nodes = 64
   n_weights = 20480 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 89026
n_validation_samples = 9976
mini_batch_size = 100
n_mini_batches per epoch = 891
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 2
Process ID = 16755

After 369 epochs, val accuracy = 0.506 and train accuracy = 0.538
TERMINATED


79.  T3:  Experiment 79  snapshots enabled
Thu Jan 19 07:58:05 2017
Neural net params:
   n_layers = 5
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 64
   layer = 3 n_nodes = 32
   layer = 4 n_nodes = 64
   n_weights = 20480 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 133413
n_validation_samples = 15090
mini_batch_size = 100
n_mini_batches per epoch = 1335
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 3
Process ID = 16792

After 222 epochs, val accuracy = 0.278 and train accuracy = 0.299
TERMINATED

------------------------------------------------------
RUNNING EXPERIMENTS
------------------------------------------------------

---------------------
Titan 1:
---------------------

85.  T1 Experiment 85
Thu Jan 19 12:07:03 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 256
   layer = 2 n_nodes = 128
   layer = 3 n_nodes = 64
   n_weights = 57344 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 133554
n_validation_samples = 14949
mini_batch_size = 100
n_mini_batches per epoch = 1336
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 3
Process ID = 29152

86.  T1 Experiment 86
Thu Jan 19 12:07:56 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 256
   layer = 2 n_nodes = 128
   layer = 3 n_nodes = 64
   n_weights = 57344 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 178060
n_validation_samples = 19944
mini_batch_size = 100
n_mini_batches per epoch = 1781
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 4
Process ID = 29168

87.  T1 Experiment 87
Thu Jan 19 12:10:03 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 256
   layer = 2 n_nodes = 256
   layer = 3 n_nodes = 64
   n_weights = 98304 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 88962
n_validation_samples = 10040
mini_batch_size = 100
n_mini_batches per epoch = 890
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 2
Process ID = 29190

88.  T1 Experiment 88
Thu Jan 19 12:10:30 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 256
   layer = 2 n_nodes = 256
   layer = 3 n_nodes = 64
   n_weights = 98304 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 133498
n_validation_samples = 15005
mini_batch_size = 100
n_mini_batches per epoch = 1335
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 3
Process ID = 29204

---------------------
Titan 3:
---------------------

76. T3: Experiment 76
Thu Jan 19 05:28:31 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 128
   layer = 3 n_nodes = 64
   n_weights = 32768 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 88962
n_test_samples = 10040
mini_batch_size = 100
n_mini_batches per epoch = 890
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 2
Process ID = 30663

After 378 epochs, val accuracy = 0.535 and train accuracy = 0.571

82.  T3:  Experiment 82
Thu Jan 19 12:03:05 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 128
   layer = 3 n_nodes = 64
   n_weights = 32768 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 178011
n_validation_samples = 19993
mini_batch_size = 100
n_mini_batches per epoch = 1781
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 4
Process ID = 4916

83.  T3:  Experiment 83
Thu Jan 19 12:04:23 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 256
   layer = 2 n_nodes = 128
   layer = 3 n_nodes = 64
   n_weights = 57344 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 44589
n_validation_samples = 4912
mini_batch_size = 100
n_mini_batches per epoch = 446
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 1
Process ID = 4946

84.  T3:  Experiment 84
Thu Jan 19 12:05:09 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 256
   layer = 2 n_nodes = 128
   layer = 3 n_nodes = 64
   n_weights = 57344 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 89004
n_validation_samples = 9998
mini_batch_size = 100
n_mini_batches per epoch = 891
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 2
Process ID = 4958


---------------------
Thinkmate
---------------------


80.  TM  Experiment 80
Thu Jan 19 11:58:32 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 128
   layer = 3 n_nodes = 64
   n_weights = 32768 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 133476
n_validation_samples = 15027
mini_batch_size = 100
n_mini_batches per epoch = 1335
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 3
Process ID = 367

81.  TM  Experiment 81
Thu Jan 19 11:59:52 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 128
   layer = 3 n_nodes = 64
   n_weights = 32768 (FC)
include_bias_terms = 1
base_learning_rate = 0.0003
solver type = RMSPROP
   rmsprop_decay_rate = 0.95
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
n_training_samples = 44528
n_validation_samples = 4973
mini_batch_size = 100
n_mini_batches per epoch = 446
Max n_training_epochs = 10000
Leaky ReLU small slope = 0.01
Maximum move relative to game end = 1
Process ID = 428

---------------------
m6700
---------------------

==========================================================================
Conclusions:

*.  As of 1 pm on Mon Jan 16, 2017, it looks like base learning rate must
be no larger than 1E6 to avoid horrible loss divergence behavior, at least
when lambda = 1E-3.

*.  H1 = 128, H2 = 64, lambda = 1E-4, BLR = 3E-4, switch colors yields 0.85
test accuracy for max move rel to game end = 1.

*. But H1 = 128, H2 = 64, H3 = 32, lambda = 1E-4, BLR = 3E-4, switch colors
may be better for max move rel to game end = 2 and 3.  

*.  As of noon on Thurs Jan 19, 2017, we've found that H1 = H2 = 128,
lambda = 1E-4, BLR = 3E-4, switch colors yields better validation
accuracies for max move rel to game end = 2 & 3 than H1 = 128, H2 = 64; H1
= 128, H2 = 64, H3 = 32.

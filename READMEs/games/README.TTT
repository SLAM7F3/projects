==========================================================================
Deep Q learning notes for TTT
==========================================================================
Last updated on 1/14/17; 1/18/17; 1/19/17; 1/24/17
==========================================================================

Best results so far:

61.  T1: Opponent plays randomly: H1 = 64; H2 = 32; blr = 1E-6;
old weights period = 10K

Experiment 61
Fri Jan 13 15:51:20 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 64
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 64
   n_weights = 8192 (FC)
base_learning_rate = 1e-06; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.1
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 150000 episodes
Old weights period = 10000 episodes
n_max_episodes = 300000
Frame skip = -1
Starting episode for linear epsilon decay = 1000
Stopping episode for linear epsilon decay = 198000
Random seed = -51479
Process ID = 20018

win_frac = 0.835

32.  T1: Opponent plays randomly: H1 = H2 = 32; blr = 1E-6
Experiment 32
Fri Jan 13 13:09:14 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 32
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 64
   n_weights = 5120 (FC)
base_learning_rate = 1e-06; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.1
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 200000 episodes
Old weights period = 5000 episodes
n_max_episodes = 200000
Frame skip = -1
Starting episode for linear epsilon decay = 1000
Stopping episode for linear epsilon decay = 132000
Random seed = -41753
Process ID = 10263

Finished;  win frac = 0.839


------------------------------------------------------
TERMINATED EXPERIMENTS
------------------------------------------------------


17.  m6700: Opponent plays randomly: H1 = 64; H2 = 32; blr = 1E-5; old weights
period = 8 * replay mem capacity; min_eps = 0.025; randomly switch first
mover every turn; fixed AI move bug


Experiment 17
Sat Jan 14 08:34:21 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 64
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 64
   n_weights = 8192 (FC)
base_learning_rate = 1e-05; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.025
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 150000 episodes
Old weights period = 80000 episodes
n_max_episodes = 400000
Frame skip = -1
Starting episode for linear epsilon decay = 1000
Stopping episode for linear epsilon decay = 300000
Randomly switch starting player = 1
Random seed = -858
Process ID = 9282

18.  m6700: Opponent plays randomly: H1 = 64; H2 = 32; blr = 1E-4; old weights
period = 8 * replay mem capacity; min_eps = 0.025; randomly switch first
mover every turn; fixed AI move bug

Experiment 18
Sat Jan 14 08:44:59 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 64
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 64
   n_weights = 8192 (FC)
base_learning_rate = 0.0001; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.025
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 150000 episodes
Old weights period = 80000 episodes
n_max_episodes = 400000
Frame skip = -1
Starting episode for linear epsilon decay = 1000
Stopping episode for linear epsilon decay = 300000
Randomly switch starting player = 1
Random seed = -1496
Process ID = 12128

19.  m6700: Opponent plays randomly: H1 = 64; H2 = 32; blr = 13-4; old weights
period = 8 * replay mem capacity; min_eps = 0.025; randomly switch first
mover every turn; fixed AI move bug

Experiment 19
Sat Jan 14 09:21:49 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 64
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 64
   n_weights = 8192 (FC)
base_learning_rate = 0.0003; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.025
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 150000 episodes
Old weights period = 80000 episodes
n_max_episodes = 400000
Frame skip = -1
Starting episode for linear epsilon decay = 1000
Stopping episode for linear epsilon decay = 300000
Randomly switch starting player = 1
Random seed = -3706
Process ID = 21137




10.  T1: Opponent plays randomly: H1 = H2 = 32; blr = 1E-6

Experiment 10
Fri Jan 13 06:49:53 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 32
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 64
   n_weights = 5120 (FC)
base_learning_rate = 1e-06; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 5000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.1
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 200000 episodes
Old weights period = 5000 episodes
Frame skip = -1
Starting episode for linear epsilon decay = 1000
Stopping episode for linear epsilon decay = 132000
Random seed = -18991
Process ID = 20517

Base learning rate too small
win frac = 0.632
cum reward --> 0.713

11.  T1: Repeat expt 10

Experiment 11
Fri Jan 13 06:50:41 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 32
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 64
   n_weights = 5120 (FC)
base_learning_rate = 1e-06; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 5000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.1
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 200000 episodes
Old weights period = 5000 episodes
Frame skip = -1
Starting episode for linear epsilon decay = 1000
Stopping episode for linear epsilon decay = 132000
Random seed = -19038
Process ID = 21086

Base learning rate too small
win frac = 0.642
cum reward --> 0.727

12.  T1: Opponent plays randomly: H1 = 16; H2 = 32; blr = 1E-6

Experiment 12
Fri Jan 13 06:52:19 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 16
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 64
   n_weights = 3584 (FC)
base_learning_rate = 1e-06; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 5000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.1
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 200000 episodes
Old weights period = 5000 episodes
Frame skip = -1
Starting episode for linear epsilon decay = 1000
Stopping episode for linear epsilon decay = 132000
Random seed = -19137
Process ID = 23821

Base learning rate too small
win frac = 0.670
cum reward --> 0.813

13.  T1: Repeat expt 12

Experiment 13
Fri Jan 13 06:53:09 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 16
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 64
   n_weights = 3584 (FC)
base_learning_rate = 1e-06; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 5000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.1
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 200000 episodes
Old weights period = 5000 episodes
Frame skip = -1
Starting episode for linear epsilon decay = 1000
Stopping episode for linear epsilon decay = 132000
Random seed = -19187
Process ID = 25111

Base learning rate too small
win frac = 0.647
cum reward --> 0.823

14.  T1: Opponent plays randomly: H1 = 16; H2 = 16; blr = 1E-6

Experiment 14
Fri Jan 13 06:54:35 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 16
   layer = 2 n_nodes = 16
   layer = 3 n_nodes = 64
   n_weights = 2304 (FC)
base_learning_rate = 1e-06; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 5000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.1
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 200000 episodes
Old weights period = 5000 episodes
Frame skip = -1
Starting episode for linear epsilon decay = 1000
Stopping episode for linear epsilon decay = 132000
Random seed = -19272
Process ID = 28729

Base learning rate too small
win frac = 0.631
cum reward --> 0.654

15.  T1: Repeat expt 14

Experiment 15
Fri Jan 13 06:54:47 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 16
   layer = 2 n_nodes = 16
   layer = 3 n_nodes = 64
   n_weights = 2304 (FC)
base_learning_rate = 1e-06; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 5000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.1
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 200000 episodes
Old weights period = 5000 episodes
Frame skip = -1
Starting episode for linear epsilon decay = 1000
Stopping episode for linear epsilon decay = 132000
Random seed = -19286
Process ID = 29236

Base learning rate too small
win frac = 0.666
cum reward --> 0.808

20.  T1:  Opponent plays randomly: H1 = H2 = 16; blr = 1E-5

Experiment 20
Fri Jan 13 07:54:28 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 16
   layer = 2 n_nodes = 16
   layer = 3 n_nodes = 64
   n_weights = 2304 (FC)
base_learning_rate = 1e-05; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 5000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.1
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 200000 episodes
Old weights period = 5000 episodes
Frame skip = -1
Starting episode for linear epsilon decay = 1000
Stopping episode for linear epsilon decay = 132000
Random seed = -22867
Process ID = 11404

Base learning rate too large


21.  T1:  Opponent plays randomly: H1 = H2 = 16; blr = 1E-5

Experiment 21
Fri Jan 13 07:55:09 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 16
   layer = 2 n_nodes = 16
   layer = 3 n_nodes = 64
   n_weights = 2304 (FC)
base_learning_rate = 1e-05; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 5000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.1
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 200000 episodes
Old weights period = 5000 episodes
Frame skip = -1
Starting episode for linear epsilon decay = 1000
Stopping episode for linear epsilon decay = 132000
Random seed = -22908
Process ID = 11979

Base learning rate too large

22.  T1:  Opponent plays randomly: H1 = H2 = 16; blr = 3E-6

Experiment 22
Fri Jan 13 07:56:07 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 16
   layer = 2 n_nodes = 16
   layer = 3 n_nodes = 64
   n_weights = 2304 (FC)
base_learning_rate = 3e-06; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 5000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.1
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 200000 episodes
Old weights period = 5000 episodes
Frame skip = -1
Starting episode for linear epsilon decay = 1000
Stopping episode for linear epsilon decay = 132000
Random seed = -22966
Process ID = 14264

23.  T1:  Opponent plays randomly: H1 = H2 = 16; blr = 3E-6

Experiment 23
Fri Jan 13 07:56:09 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 16
   layer = 2 n_nodes = 16
   layer = 3 n_nodes = 64
   n_weights = 2304 (FC)
base_learning_rate = 3e-06; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 5000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.1
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 200000 episodes
Old weights period = 5000 episodes
Frame skip = -1
Starting episode for linear epsilon decay = 1000
Stopping episode for linear epsilon decay = 132000
Random seed = -22969
Process ID = 14381


24.  T1:  Opponent plays randomly:  H1 = H2 = 16; blr = 5E-6

Experiment 24
Fri Jan 13 08:08:36 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 16
   layer = 2 n_nodes = 16
   layer = 3 n_nodes = 64
   n_weights = 2304 (FC)
base_learning_rate = 5e-06; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 5000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.1
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 200000 episodes
Old weights period = 5000 episodes
Frame skip = -1
Starting episode for linear epsilon decay = 1000
Stopping episode for linear epsilon decay = 132000
Random seed = -23715
Process ID = 30184

25.  T1:  Opponent plays randomly:  H1 = H2 = 16; blr = 5E-6

Experiment 25
Fri Jan 13 08:08:39 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 16
   layer = 2 n_nodes = 16
   layer = 3 n_nodes = 64
   n_weights = 2304 (FC)
base_learning_rate = 5e-06; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 5000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.1
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 200000 episodes
Old weights period = 5000 episodes
Frame skip = -1
Starting episode for linear epsilon decay = 1000
Stopping episode for linear epsilon decay = 132000
Random seed = -23718
Process ID = 30216

16.  T3: Opponent plays randomly: H1 = H2 = 32; blr = 5E-6

Experiment 16
Fri Jan 13 08:17:03 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 32
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 64
   n_weights = 5120 (FC)
base_learning_rate = 5e-06; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.1
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 200000 episodes
Old weights period = 5000 episodes
Frame skip = -1
Starting episode for linear epsilon decay = 1000
Stopping episode for linear epsilon decay = 264000
Random seed = -24222
Process ID = 2794

17.  T3: Opponent plays randomly: H1 = H2 = 32; blr = 5E-6

Experiment 17
Fri Jan 13 08:17:06 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 32
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 64
   n_weights = 5120 (FC)
base_learning_rate = 5e-06; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.1
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 200000 episodes
Old weights period = 5000 episodes
Frame skip = -1
Starting episode for linear epsilon decay = 1000
Stopping episode for linear epsilon decay = 264000
Random seed = -24225
Process ID = 2802

18.  T3: Opponent plays randomly: H1 = H2 = 32; blr = 3E-6

Experiment 18
Fri Jan 13 08:18:42 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 32
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 64
   n_weights = 5120 (FC)
base_learning_rate = 3e-06; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.1
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 200000 episodes
Old weights period = 5000 episodes
Frame skip = -1
Starting episode for linear epsilon decay = 1000
Stopping episode for linear epsilon decay = 264000
Random seed = -24321
Process ID = 5621

19.  T3: Opponent plays randomly: H1 = H2 = 32; blr = 3E-6

Experiment 19
Fri Jan 13 08:18:46 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 32
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 64
   n_weights = 5120 (FC)
base_learning_rate = 3e-06; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.1
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 200000 episodes
Old weights period = 5000 episodes
Frame skip = -1
Starting episode for linear epsilon decay = 1000
Stopping episode for linear epsilon decay = 264000
Random seed = -24324
Process ID = 5801


26.  T1: Opponent plays randomly: H1 = H2 = 16; blr = 2E-6
Experiment 26
Fri Jan 13 10:07:40 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 16
   layer = 2 n_nodes = 16
   layer = 3 n_nodes = 64
   n_weights = 2304 (FC)
base_learning_rate = 2e-06; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.1
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 200000 episodes
Old weights period = 5000 episodes
Frame skip = -1
Starting episode for linear epsilon decay = 1000
Stopping episode for linear epsilon decay = 264000
Random seed = -30858
Process ID = 24208

27.  T1: Opponent plays randomly: H1 = H2 = 16; blr = 2E-6
Experiment 27
Fri Jan 13 10:07:43 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 16
   layer = 2 n_nodes = 16
   layer = 3 n_nodes = 64
   n_weights = 2304 (FC)
base_learning_rate = 2e-06; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.1
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 200000 episodes
Old weights period = 5000 episodes
Frame skip = -1
Starting episode for linear epsilon decay = 1000
Stopping episode for linear epsilon decay = 264000
Random seed = -30862
Process ID = 24216

28.  T1: Opponent plays randomly: H1 = H2 = 32; blr = 2E-6

Experiment 28
Fri Jan 13 10:08:43 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 32
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 64
   n_weights = 5120 (FC)
base_learning_rate = 2e-06; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.1
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 200000 episodes
Old weights period = 5000 episodes
Frame skip = -1
Starting episode for linear epsilon decay = 1000
Stopping episode for linear epsilon decay = 264000
Random seed = -30922
Process ID = 26685


29.  T1: Opponent plays randomly: H1 = H2 = 32; blr = 2E-6

Experiment 29
Fri Jan 13 10:08:46 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 32
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 64
   n_weights = 5120 (FC)
base_learning_rate = 2e-06; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.1
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 200000 episodes
Old weights period = 5000 episodes
Frame skip = -1
Starting episode for linear epsilon decay = 1000
Stopping episode for linear epsilon decay = 264000
Random seed = -30925
Process ID = 26780


30.  T1: Opponent plays randomly: H1 = H2 = 16; blr = 1E-6

Experiment 30
Fri Jan 13 11:59:27 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 16
   layer = 2 n_nodes = 16
   layer = 3 n_nodes = 64
   n_weights = 2304 (FC)
base_learning_rate = 1e-06; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.1
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 200000 episodes
Old weights period = 5000 episodes
n_max_episodes = 200000
Frame skip = -1
Starting episode for linear epsilon decay = 1000
Stopping episode for linear epsilon decay = 132000
Random seed = -37566
Process ID = 31422

31.  T1: Opponent plays randomly: H1 = H2 = 16; blr = 1E-6

Experiment 31
Fri Jan 13 11:59:30 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 16
   layer = 2 n_nodes = 16
   layer = 3 n_nodes = 64
   n_weights = 2304 (FC)
base_learning_rate = 1e-06; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.1
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 200000 episodes
Old weights period = 5000 episodes
n_max_episodes = 200000
Frame skip = -1
Starting episode for linear epsilon decay = 1000
Stopping episode for linear epsilon decay = 132000
Random seed = -37569
Process ID = 31430


32.  T1: Opponent plays randomly: H1 = H2 = 32; blr = 1E-6
Experiment 32
Fri Jan 13 13:09:14 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 32
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 64
   n_weights = 5120 (FC)
base_learning_rate = 1e-06; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.1
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 200000 episodes
Old weights period = 5000 episodes
n_max_episodes = 200000
Frame skip = -1
Starting episode for linear epsilon decay = 1000
Stopping episode for linear epsilon decay = 132000
Random seed = -41753
Process ID = 10263

Finished;  win frac = 0.839


33.  T1: Opponent plays randomly: H1 = H2 = 32; blr = 1E-6
Experiment 33
Fri Jan 13 13:09:17 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 32
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 64
   n_weights = 5120 (FC)
base_learning_rate = 1e-06; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.1
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 200000 episodes
Old weights period = 5000 episodes
n_max_episodes = 200000
Frame skip = -1
Starting episode for linear epsilon decay = 1000
Stopping episode for linear epsilon decay = 132000
Random seed = -41756
Process ID = 10271

Finished;  win frac = 0.835

38.  T1:  Opponent plays randomly; H1 = H2 = 64; blr = 1E-6
Experiment 38
Fri Jan 13 13:10:28 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 64
   layer = 2 n_nodes = 64
   layer = 3 n_nodes = 64
   n_weights = 12288 (FC)
base_learning_rate = 1e-06; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.1
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 200000 episodes
Old weights period = 5000 episodes
n_max_episodes = 200000
Frame skip = -1
Starting episode for linear epsilon decay = 1000
Stopping episode for linear epsilon decay = 132000
Random seed = -41826
Process ID = 13191

Finished;  definitely worse than H1 = H2 = 32

39.  T1:  Opponent plays randomly; H1 = H2 = 64; blr = 1E-6
Experiment 39
Fri Jan 13 13:10:31 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 64
   layer = 2 n_nodes = 64
   layer = 3 n_nodes = 64
   n_weights = 12288 (FC)
base_learning_rate = 1e-06; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.1
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 200000 episodes
Old weights period = 5000 episodes
n_max_episodes = 200000
Frame skip = -1
Starting episode for linear epsilon decay = 1000
Stopping episode for linear epsilon decay = 132000
Random seed = -41830
Process ID = 13200

Finished;  definitely worse than H1 = H2 = 32

42.  T1:  Opponent plays random:  H1 = H2 = 32;  blr = 3E-5
Experiment 42
Fri Jan 13 13:20:23 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 32
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 64
   n_weights = 5120 (FC)
base_learning_rate = 3e-05; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.1
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 200000 episodes
Old weights period = 5000 episodes
n_max_episodes = 200000
Frame skip = -1
Starting episode for linear epsilon decay = 1000
Stopping episode for linear epsilon decay = 132000
Random seed = -42422
Process ID = 26064

Bad loss behavior!

43.  T1:  Opponent plays random:  H1 = H2 = 32;  blr = 3E-5
Experiment 43
Fri Jan 13 13:20:26 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 32
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 64
   n_weights = 5120 (FC)
base_learning_rate = 3e-05; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.1
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 200000 episodes
Old weights period = 5000 episodes
n_max_episodes = 200000
Frame skip = -1
Starting episode for linear epsilon decay = 1000
Stopping episode for linear epsilon decay = 132000
Random seed = -42426
Process ID = 26072

Bad loss behavior!


34.  T3:  Opponent plays random: H1 = H2 = 32; blr = 3E-6
Experiment 34
Fri Jan 13 13:12:57 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 32
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 64
   n_weights = 5120 (FC)
base_learning_rate = 3e-06; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.1
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 200000 episodes
Old weights period = 5000 episodes
n_max_episodes = 200000
Frame skip = -1
Starting episode for linear epsilon decay = 1000
Stopping episode for linear epsilon decay = 132000
Random seed = -41976
Process ID = 14279

Bad loss behavior!

35.  T3:  Opponent plays random: H1 = H2 = 32; blr = 3E-6
Experiment 35
Fri Jan 13 13:12:59 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 32
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 64
   n_weights = 5120 (FC)
base_learning_rate = 3e-06; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.1
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 200000 episodes
Old weights period = 5000 episodes
n_max_episodes = 200000
Frame skip = -1
Starting episode for linear epsilon decay = 1000
Stopping episode for linear epsilon decay = 132000
Random seed = -41978
Process ID = 14287

Bad loss behavior!


36.  T3:  Opponent plays random: H1 = H2 = 64; blr = 3E-6
Experiment 36
Fri Jan 13 13:13:45 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 64
   layer = 2 n_nodes = 64
   layer = 3 n_nodes = 64
   n_weights = 12288 (FC)
base_learning_rate = 3e-06; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.1
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 200000 episodes
Old weights period = 5000 episodes
n_max_episodes = 200000
Frame skip = -1
Starting episode for linear epsilon decay = 1000
Stopping episode for linear epsilon decay = 132000
Random seed = -42024
Process ID = 15884

Bad loss behavior!

37.  T3:  Opponent plays random: H1 = H2 = 64; blr = 3E-6
Experiment 37
Fri Jan 13 13:13:50 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 64
   layer = 2 n_nodes = 64
   layer = 3 n_nodes = 64
   n_weights = 12288 (FC)
base_learning_rate = 3e-06; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.1
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 200000 episodes
Old weights period = 5000 episodes
n_max_episodes = 200000
Frame skip = -1
Starting episode for linear epsilon decay = 1000
Stopping episode for linear epsilon decay = 132000
Random seed = -42029
Process ID = 16178

Bad loss behavior!


40.  T3:  Opponent plays random: H1 = H2 = 32; blr = 1E-5
Experiment 40
Fri Jan 13 13:18:33 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 32
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 64
   n_weights = 5120 (FC)
base_learning_rate = 1e-05; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.1
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 200000 episodes
Old weights period = 5000 episodes
n_max_episodes = 200000
Frame skip = -1
Starting episode for linear epsilon decay = 1000
Stopping episode for linear epsilon decay = 132000
Random seed = -42312
Process ID = 24230

Bad loss behavior!

41.  T3:  Opponent plays random: H1 = H2 = 32; blr = 1E-5
Experiment 41
Fri Jan 13 13:18:36 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 32
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 64
   n_weights = 5120 (FC)
base_learning_rate = 1e-05; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.1
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 200000 episodes
Old weights period = 5000 episodes
n_max_episodes = 200000
Frame skip = -1
Starting episode for linear epsilon decay = 1000
Stopping episode for linear epsilon decay = 132000
Random seed = -42316
Process ID = 24324

Bad loss behavior!

44.  T1: Opponent plays randomly: H1 = H2 = 32; blr = 1E-6

Experiment 44
Fri Jan 13 14:15:47 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 32
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 64
   n_weights = 5120 (FC)
base_learning_rate = 1e-06; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.1
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 200000 episodes
Old weights period = 5000 episodes
n_max_episodes = 300000
Frame skip = -1
Starting episode for linear epsilon decay = 1000
Stopping episode for linear epsilon decay = 198000
Random seed = -45745
Process ID = 575

Loss starts to increase around episode 200K.
win frac = 0.831

45.  T1: Opponent plays randomly: H1 = H2 = 32; blr = 1E-6

Experiment 45
Fri Jan 13 14:15:49 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 32
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 64
   n_weights = 5120 (FC)
base_learning_rate = 1e-06; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.1
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 200000 episodes
Old weights period = 5000 episodes
n_max_episodes = 300000
Frame skip = -1
Starting episode for linear epsilon decay = 1000
Stopping episode for linear epsilon decay = 198000
Random seed = -45748
Process ID = 583

Loss starts to increase around episode 200K.
win frac = 0.798

46.  T1: Opponent plays randomly: H1 = H2 = 32; blr = 1E-6; old weights period = 10K

Experiment 46
Fri Jan 13 14:17:08 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 32
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 64
   n_weights = 5120 (FC)
base_learning_rate = 1e-06; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.1
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 200000 episodes
Old weights period = 10000 episodes
n_max_episodes = 300000
Frame skip = -1
Starting episode for linear epsilon decay = 1000
Stopping episode for linear epsilon decay = 198000
Random seed = -45826
Process ID = 3765

Loss starts to increase around episode 200K.
win frac = 0.841

47.  T1: Opponent plays randomly: H1 = H2 = 32; blr = 1E-6; old weights period = 10K

Experiment 47
Fri Jan 13 14:17:12 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 32
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 64
   n_weights = 5120 (FC)
base_learning_rate = 1e-06; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.1
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 200000 episodes
Old weights period = 10000 episodes
n_max_episodes = 300000
Frame skip = -1
Starting episode for linear epsilon decay = 1000
Stopping episode for linear epsilon decay = 198000
Random seed = -45830
Process ID = 3920

Loss starts to increase around episode 200K.
win frac = 0.814


48.  T1: Opponent plays randomly: H1 = 16; H2 = 32; blr = 1E-6;
old weights period = 5K

Experiment 47
Fri Jan 13 14:19:25 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 16
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 64
   n_weights = 3584 (FC)
base_learning_rate = 1e-06; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.1
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 200000 episodes
Old weights period = 5000 episodes
n_max_episodes = 300000
Frame skip = -1
Starting episode for linear epsilon decay = 1000
Stopping episode for linear epsilon decay = 198000
Random seed = -45962
Process ID = 9966

Loss starts to increase around episode 200K.
win frac = 0.822

49.  T1: Opponent plays randomly: H1 = 16; H2 = 32; blr = 1E-6;
old weights period = 5K

Experiment 49
Fri Jan 13 14:22:26 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 16
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 64
   n_weights = 3584 (FC)
base_learning_rate = 1e-06; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.1
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 200000 episodes
Old weights period = 5000 episodes
n_max_episodes = 300000
Frame skip = -1
Starting episode for linear epsilon decay = 1000
Stopping episode for linear epsilon decay = 198000
Random seed = -46145
Process ID = 16277

Loss starts to significantly increase around episode 200K.
win frac = 0.819



50.  T3: Opponent plays randomly: H1 = 32; H2 = 16; blr = 1E-6;
old weights period = 5K
Experiment 50
Fri Jan 13 14:23:48 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 32
   layer = 2 n_nodes = 16
   layer = 3 n_nodes = 64
   n_weights = 3584 (FC)
base_learning_rate = 1e-06; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.1
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 200000 episodes
Old weights period = 5000 episodes
n_max_episodes = 300000
Frame skip = -1
Starting episode for linear epsilon decay = 1000
Stopping episode for linear epsilon decay = 198000
Random seed = -46226
Process ID = 5427

Loss starts to increase around episode 200K.
win frac = 0.838


51.  T3: Opponent plays randomly: H1 = 32; H2 = 16; blr = 1E-6;
old weights period = 5K

Experiment 51
Fri Jan 13 14:23:50 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 32
   layer = 2 n_nodes = 16
   layer = 3 n_nodes = 64
   n_weights = 3584 (FC)
base_learning_rate = 1e-06; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.1
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 200000 episodes
Old weights period = 5000 episodes
n_max_episodes = 300000
Frame skip = -1
Starting episode for linear epsilon decay = 1000
Stopping episode for linear epsilon decay = 198000
Random seed = -46229
Process ID = 5435

Loss starts to increase around episode 200K.
win frac = 0.790


52.  T3: Opponent plays randomly: H1 = 16; H2 = 32; blr = 1E-6;
old weights period = 5K
Experiment 52
Fri Jan 13 14:25:22 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 16
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 64
   n_weights = 3584 (FC)
base_learning_rate = 1e-06; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.1
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 200000 episodes
Old weights period = 5000 episodes
n_max_episodes = 300000
Frame skip = -1
Starting episode for linear epsilon decay = 1000
Stopping episode for linear epsilon decay = 198000
Random seed = -46322
Process ID = 8327

Bad max Q behavior

53.  T3: Opponent plays randomly: H1 = 16; H2 = 32; blr = 1E-6;
old weights period = 5K


Experiment 53
Fri Jan 13 14:25:25 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 16
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 64
   n_weights = 3584 (FC)
base_learning_rate = 1e-06; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.1
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 200000 episodes
Old weights period = 5000 episodes
n_max_episodes = 300000
Frame skip = -1
Starting episode for linear epsilon decay = 1000
Stopping episode for linear epsilon decay = 198000
Random seed = -46324
Process ID = 8335

Bad max Q behavior

54.  T3: Opponent plays randomly: H1 = 16; H2 = 16; blr = 1E-6;
old weights period = 5K

Experiment 54
Fri Jan 13 14:26:34 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 16
   layer = 2 n_nodes = 16
   layer = 3 n_nodes = 64
   n_weights = 2304 (FC)
base_learning_rate = 1e-06; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.1
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 200000 episodes
Old weights period = 5000 episodes
n_max_episodes = 300000
Frame skip = -1
Starting episode for linear epsilon decay = 1000
Stopping episode for linear epsilon decay = 198000
Random seed = -46393
Process ID = 11646

Bad max Q behavior

55.  T3: Opponent plays randomly: H1 = 16; H2 = 16; blr = 1E-6;
old weights period = 5K
Experiment 55
Fri Jan 13 14:26:36 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 16
   layer = 2 n_nodes = 16
   layer = 3 n_nodes = 64
   n_weights = 2304 (FC)
base_learning_rate = 1e-06; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.1
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 200000 episodes
Old weights period = 5000 episodes
n_max_episodes = 300000
Frame skip = -1
Starting episode for linear epsilon decay = 1000
Stopping episode for linear epsilon decay = 198000
Random seed = -46396
Process ID = 11744

Bad max Q behavior

58.  T3: Opponent plays randomly: H1 = 32; H2 = 64; blr = 1E-6;
old weights period = 5K
Experiment 58
Fri Jan 13 14:45:22 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 32
   layer = 2 n_nodes = 64
   layer = 3 n_nodes = 64
   n_weights = 8192 (FC)
base_learning_rate = 1e-06; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.1
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 200000 episodes
Old weights period = 5000 episodes
n_max_episodes = 300000
Frame skip = -1
Starting episode for linear epsilon decay = 1000
Stopping episode for linear epsilon decay = 198000
Random seed = -47520
Process ID = 10850

After 184K episodes, win_frac = 0.738

59.  T3: Opponent plays randomly: H1 = 32; H2 = 64; blr = 1E-6;
old weights period = 5K

Experiment 59
Fri Jan 13 14:45:25 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 32
   layer = 2 n_nodes = 64
   layer = 3 n_nodes = 64
   n_weights = 8192 (FC)
base_learning_rate = 1e-06; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.1
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 200000 episodes
Old weights period = 5000 episodes
n_max_episodes = 300000
Frame skip = -1
Starting episode for linear epsilon decay = 1000
Stopping episode for linear epsilon decay = 198000
Random seed = -47524
Process ID = 10975

Bad loss behavior


60.  T1: Opponent plays randomly: H1 = 64; H2 = 32; blr = 1E-6;
old weights period = 10K

Experiment 60
Fri Jan 13 15:51:17 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 64
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 64
   n_weights = 8192 (FC)
base_learning_rate = 1e-06; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.1
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 150000 episodes
Old weights period = 10000 episodes
n_max_episodes = 300000
Frame skip = -1
Starting episode for linear epsilon decay = 1000
Stopping episode for linear epsilon decay = 198000
Random seed = -51475
Process ID = 20010

win_frac = 0.835

61.  T1: Opponent plays randomly: H1 = 64; H2 = 32; blr = 1E-6;
old weights period = 10K

Experiment 61
Fri Jan 13 15:51:20 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 64
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 64
   n_weights = 8192 (FC)
base_learning_rate = 1e-06; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.1
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 150000 episodes
Old weights period = 10000 episodes
n_max_episodes = 300000
Frame skip = -1
Starting episode for linear epsilon decay = 1000
Stopping episode for linear epsilon decay = 198000
Random seed = -51479
Process ID = 20018

win_frac = 0.835

62.  T1: Opponent plays randomly: H1 = 64; H2 = 32; H3 = 32; blr = 1E-6;
old weights period = 10K

Experiment 62
Fri Jan 13 15:53:17 2017
Neural net params:
   n_layers = 5
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 64
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 32
   layer = 4 n_nodes = 64
   n_weights = 9216 (FC)
base_learning_rate = 1e-06; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.1
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 150000 episodes
Old weights period = 10000 episodes
n_max_episodes = 300000
Frame skip = -1
Starting episode for linear epsilon decay = 1000
Stopping episode for linear epsilon decay = 198000
Random seed = -51596
Process ID = 24058

win_frac = 0.831

63.  T1: Opponent plays randomly: H1 = 64; H2 = 32; H3 = 32; blr = 1E-6;
old weights period = 10K

Experiment 63
Fri Jan 13 15:53:21 2017
Neural net params:
   n_layers = 5
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 64
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 32
   layer = 4 n_nodes = 64
   n_weights = 9216 (FC)
base_learning_rate = 1e-06; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.1
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 150000 episodes
Old weights period = 10000 episodes
n_max_episodes = 300000
Frame skip = -1
Starting episode for linear epsilon decay = 1000
Stopping episode for linear epsilon decay = 198000
Random seed = -51599
Process ID = 24096

win_frac = 0.844

56.  T3: Opponent plays randomly: H1 = 64; H2 = 32; blr = 1E-6;
old weights period = 5K
Experiment 56
Fri Jan 13 14:43:40 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 64
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 64
   n_weights = 8192 (FC)
base_learning_rate = 1e-06; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.1
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 200000 episodes
Old weights period = 5000 episodes
n_max_episodes = 300000
Frame skip = -1
Starting episode for linear epsilon decay = 1000
Stopping episode for linear epsilon decay = 198000
Random seed = -47418
Process ID = 6889

Loss starts to increase around episode 200K.
After 252 K episodes, win_frac = 0.821

57.  T3: Opponent plays randomly: H1 = 64; H2 = 32; blr = 1E-6;
old weights period = 5K
Experiment 57
Fri Jan 13 14:43:51 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 64
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 64
   n_weights = 8192 (FC)
base_learning_rate = 1e-06; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.1
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 200000 episodes
Old weights period = 5000 episodes
n_max_episodes = 300000
Frame skip = -1
Starting episode for linear epsilon decay = 1000
Stopping episode for linear epsilon decay = 198000
Random seed = -47430
Process ID = 7058

After 200K episodes, win_frac = 0.790
After 248K episodes, win_frac = 0.824

64.  T3: Opponent plays randomly: H1 = 64; H2 = 32; blr = 2E-6;
old weights period = 10K
Experiment 64
Fri Jan 13 15:55:35 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 64
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 64
   n_weights = 8192 (FC)
base_learning_rate = 2e-06; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.1
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 150000 episodes
Old weights period = 10000 episodes
n_max_episodes = 300000
Frame skip = -1
Starting episode for linear epsilon decay = 1000
Stopping episode for linear epsilon decay = 198000
Random seed = -51734
Process ID = 1445

win_frac = 0.739

65.  T3: Opponent plays randomly: H1 = 64; H2 = 32; blr = 2E-6;
old weights period = 10K

Experiment 65
Fri Jan 13 15:55:39 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 64
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 64
   n_weights = 8192 (FC)
base_learning_rate = 2e-06; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.1
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 150000 episodes
Old weights period = 10000 episodes
n_max_episodes = 300000
Frame skip = -1
Starting episode for linear epsilon decay = 1000
Stopping episode for linear epsilon decay = 198000
Random seed = -51737
Process ID = 1453

win_frac = 0.720

66.  T3: Opponent plays randomly: H1 = 64; H2 = 32; H3 = 32; blr = 2E-6;
old weights period = 10K
Experiment 66
Fri Jan 13 15:56:55 2017
Neural net params:
   n_layers = 5
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 64
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 32
   layer = 4 n_nodes = 64
   n_weights = 9216 (FC)
base_learning_rate = 2e-06; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.1
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 150000 episodes
Old weights period = 10000 episodes
n_max_episodes = 300000
Frame skip = -1
Starting episode for linear epsilon decay = 1000
Stopping episode for linear epsilon decay = 198000
Random seed = -51814
Process ID = 3705

win_frac = 0.728

67.  T3: Opponent plays randomly: H1 = 64; H2 = 32; H3 = 32; blr = 2E-6;
old weights period = 10K

Experiment 67
Fri Jan 13 15:56:58 2017
Neural net params:
   n_layers = 5
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 64
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 32
   layer = 4 n_nodes = 64
   n_weights = 9216 (FC)
base_learning_rate = 2e-06; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.1
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 150000 episodes
Old weights period = 10000 episodes
n_max_episodes = 300000
Frame skip = -1
Starting episode for linear epsilon decay = 1000
Stopping episode for linear epsilon decay = 198000
Random seed = -51817
Process ID = 3737

win_frac = 0.728

68.  TM: Opponent plays randomly: H1 = 128; H2 = 64; blr = 1E-6;
old weights period = 8 * replay mem capacity

win_frac --> 0.678

69.  TM: Opponent plays randomly: H1 = 128; H2 = 64; blr = 1E-6;
old weights period = 8 * replay mem capacity

win_frac --> 0.690

70.  T1: Opponent plays randomly: H1 = 128; H2 = 64; blr = 2E-6;
old weights period = 8 * replay mem capacity

win_frac --> 0.650

71.  T1: Opponent plays randomly: H1 = 128; H2 = 64; blr = 2E-6;
old weights period = 8 * replay mem capacity

win_frac --> 0.662

74.  TM: Opponent plays randomly: H1 = 64; H2 = 32; blr = 1E-5; old weights
period = 8 * replay mem capacity; min_eps = 0.025; randomly switch first
mover every turn; fixed AI move bug

75.  TM: Opponent plays randomly: H1 = 64; H2 = 32; blr = 1E-5; old weights
period = 8 * replay mem capacity; min_eps = 0.025; randomly switch first
mover every turn; fixed AI move bug

76.  T3: Opponent plays randomly: H1 = 64; H2 = 32; blr = 1E-4; old weights
period = 8 * replay mem capacity; min_eps = 0.025; randomly switch first
mover every turn; fixed AI move bug

77.  T3: Opponent plays randomly: H1 = 64; H2 = 32; blr = 1E-4; old weights
period = 8 * replay mem capacity; min_eps = 0.025; randomly switch first
mover every turn; fixed AI move bug

80.  T3: Opponent plays randomly: H1 = 64; H2 = 32; blr = 3E-4; old weights
period = 4 * replay mem capacity; min_eps = 0.025; do not switch first
player; update Q network every turn

81.  T3: Opponent plays randomly: H1 = 64; H2 = 32; blr = 3E-4; old weights
period = 4 * replay mem capacity; min_eps = 0.025; do not switch first
player; update Q network every turn


82.  T1: Opponent plays randomly: H1 = 64; H2 = 64; blr = 3E-4; old weights
period = 4 * replay mem capacity; min_eps = 0.025; do not switch first
player; update Q network every turn

83.  T1: Opponent plays randomly: H1 = 64; H2 = 64; blr = 3E-4; old weights
period = 4 * replay mem capacity; min_eps = 0.025; do not switch first
player; update Q network every turn

84.  TM: Opponent plays randomly: H1 = 64; H2 = 32; H3 = 32; blr = 3E-4;
old weights period = 4 * replay mem capacity; min_eps = 0.025; do not
switch first player; update Q network every turn

85.  TM: Opponent plays randomly: H1 = 64; H2 = 32; H3 = 32; blr = 3E-4;
old weights period = 4 * replay mem capacity; min_eps = 0.025; do not
switch first player; update Q network every turn


88.  T1: Opponent plays randomly: H1 = 64; H2 = 64; blr = 3E-4; old weights
period = 1 * replay mem capacity = 5K; min_eps = 0.025; do not switch first
player; update Q network every episode

89.  T1: Opponent plays randomly: H1 = 64; H2 = 64; blr = 3E-4; old weights
period = 1 * replay mem capacity = 5K; min_eps = 0.025; do not switch first
player; update Q network every episode

90.  T3: Opponent plays randomly: H1 = 64; H2 = 32; H3 = 32; blr = 3E-4;
old weights period = 1 * replay mem capacity = 5K; min_eps = 0.025; do not
switch first player; update Q network every episode

91.  T3: Opponent plays randomly: H1 = 64; H2 = 32; H3 = 32; blr = 3E-4;
old weights period = 1 * replay mem capacity = 5K; min_eps = 0.025; do not
switch first player; update Q network every episode

86.  TM: Opponent plays randomly: H1 = 64; H2 = 32; blr = 3E-4;
old weights period = 1 * replay mem capacity = 5K; min_eps = 0.025; do not
switch first player; update Q network every episode

87.  TM: Opponent plays randomly: H1 = 64; H2 = 32; blr = 3E-4;
old weights period = 1 * replay mem capacity = 5K; min_eps = 0.025; do not
switch first player; update Q network every episode



20.  m6700: Opponent plays randomly: H1 = 64; H2 = 32; blr = 3E-4; old weights
period = 4 * replay mem capacity; min_eps = 0.025; do not switch first
player; update Q network every turn

Experiment 20
Sat Jan 14 11:28:27 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 64
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 64
   n_weights = 8192 (FC)
base_learning_rate = 0.0003; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.025
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 150000 episodes
Old weights period = 40000 episodes
n_max_episodes = 400000
Frame skip = -1
Starting episode for linear epsilon decay = 100
Stopping episode for linear epsilon decay = 280000
Randomly switch starting player = 0
Random seed = -11306
Process ID = 17081

21.  m6700: Opponent plays randomly: H1 = 64; H2 = 64; blr = 3E-4; old weights
period = 4 * replay mem capacity; min_eps = 0.025; do not switch first
player; update Q network every turn

Experiment 21
Sat Jan 14 11:44:51 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 64
   layer = 2 n_nodes = 64
   layer = 3 n_nodes = 64
   n_weights = 12288 (FC)
base_learning_rate = 0.0003; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.025
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 150000 episodes
Old weights period = 40000 episodes
n_max_episodes = 400000
Frame skip = -1
Starting episode for linear epsilon decay = 100
Stopping episode for linear epsilon decay = 280000
Randomly switch starting player = 0
Random seed = -12287
Process ID = 19688


22.  m6700: Opponent plays randomly: H1 = 64; H2 = H3 = 32; 
blr = 3E-4; old weights
period = 4 * replay mem capacity; min_eps = 0.025; do not switch first
player; update Q network every turn

Experiment 22
Sat Jan 14 11:46:56 2017
Neural net params:
   n_layers = 5
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 64
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 32
   layer = 4 n_nodes = 64
   n_weights = 9216 (FC)
base_learning_rate = 0.0003; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.025
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 150000 episodes
Old weights period = 40000 episodes
n_max_episodes = 400000
Frame skip = -1
Starting episode for linear epsilon decay = 100
Stopping episode for linear epsilon decay = 280000
Randomly switch starting player = 0
Random seed = -12410
Process ID = 20761


23.  m6700: Opponent plays randomly: H1 = 64; H2 = 32; blr = 3E-4; old
weights period = replay mem capacity = 5K; min_eps = 0.025; do not switch
first player; update Q network every episode

Experiment 23
Sat Jan 14 13:27:16 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 64
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 64
   n_weights = 8192 (FC)
base_learning_rate = 0.0003; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 5000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.025
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 150000 episodes
Old weights period = 5000 episodes
n_max_episodes = 400000
Frame skip = -1
Starting episode for linear epsilon decay = 100
Stopping episode for linear epsilon decay = 280000
Randomly switch starting player = 0
Random seed = -18435
Process ID = 3858

Bad loss behavior

24.  m6700: Opponent plays randomly: H1 = 64; H2 = 64; blr = 3E-4; old
weights period = replay mem capacity = 5K; min_eps = 0.025; do not switch
first player; update Q network every episode

Experiment 24
Sat Jan 14 13:29:44 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 64
   layer = 2 n_nodes = 64
   layer = 3 n_nodes = 64
   n_weights = 12288 (FC)
base_learning_rate = 0.0003; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 5000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.025
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 150000 episodes
Old weights period = 5000 episodes
n_max_episodes = 400000
Frame skip = -1
Starting episode for linear epsilon decay = 100
Stopping episode for linear epsilon decay = 280000
Randomly switch starting player = 0
Random seed = -18583
Process ID = 5286

Bad loss behavior

25.  m6700: Opponent plays randomly: H1 = 64; H2 = 32; H3 = 32; blr = 3E-4;
old weights period = replay mem capacity = 5K; min_eps = 0.025; do not
switch first player; update Q network every episode
Sat Jan 14 13:31:14 2017
Neural net params:
   n_layers = 5
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 64
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 32
   layer = 4 n_nodes = 64
   n_weights = 9216 (FC)
base_learning_rate = 0.0003; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 5000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.025
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 150000 episodes
Old weights period = 5000 episodes
n_max_episodes = 400000
Frame skip = -1
Starting episode for linear epsilon decay = 100
Stopping episode for linear epsilon decay = 280000
Randomly switch starting player = 0
Random seed = -18672
Process ID = 6591

Bad loss behavior

27.  m6700: Opponent plays randomly: H1 = 64; H2 = 32; blr = 1E-4;
old weights period = 20K; replay mem capacity = 10K; min_eps = 0.025; do not
switch first player; update Q network every episode

28.  m6700: Opponent plays randomly: H1 = 64; H2 = 64; blr = 1E-4;
old weights period = 20K; replay mem capacity = 10K; min_eps = 0.025; do not
switch first player; update Q network every episode

29.  m6700: Opponent plays randomly: H1 = 64; H2 = 32; H3 = 32; blr = 1E-4;
old weights period = 20K; replay mem capacity = 10K; min_eps = 0.025; do
not switch first player; update Q network every episode

30.  m6700: Opponent plays randomly: H1 = 64; H2 = 32; blr = 1E-6;
old weights period = 10K; replay mem capacity = 10K; min_eps = 0.025; do
not switch first player; update Q network every episode

31.  m6700: Opponent plays randomly: H1 = 64; H2 = 32; blr = 3E-6;
old weights period = 10K; replay mem capacity = 10K; min_eps = 0.025; do
not switch first player; update Q network every episode

32.  m6700: Opponent plays randomly: H1 = 64; H2 = 32; blr = 1E-5;
old weights period = 10K; replay mem capacity = 10K; min_eps = 0.025; do
not switch first player; update Q network every episode


100. Experiment 100; random opponent play
Wed Jan 18 07:37:24 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 64
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 64
   n_weights = 8192 (FC)
   include_biases = 1
base_learning_rate = 0.0001; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.025
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 150000 episodes
Old weights period = 10000 episodes
n_max_episodes = 400000
Frame skip = -1
Starting episode for linear epsilon decay = 100
Stopping episode for linear epsilon decay = 280000
Randomly switch starting player = 0
Random seed = -53839
Process ID = 5978

Finished.  Win frac = 0.684

101.  Experiment 101
Wed Jan 18 07:42:11 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 64
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 64
   n_weights = 8192 (FC)
   include_biases = 1
base_learning_rate = 0.0001; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.025
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 150000 episodes
Old weights period = 10000 episodes
n_max_episodes = 400000
Frame skip = -1
Starting episode for linear epsilon decay = 100
Stopping episode for linear epsilon decay = 280000
Randomly switch starting player = 0
Random seed = -54129
Process ID = 30474

Finished: After 398K episodes, win frac = 0.681

102.  Experiment 102; random opponent play
Wed Jan 18 08:31:33 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 64
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 64
   n_weights = 8192 (FC)
   include_biases = 1
base_learning_rate = 0.0001; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.025
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 150000 episodes
Old weights period = 10000 episodes
n_max_episodes = 500000
Frame skip = -1
Starting episode for linear epsilon decay = 100
Stopping episode for linear epsilon decay = 250000
Randomly switch starting player = 0
Random seed = -57090
Process ID = 16321

After 498K episodes, win frac = 0.742.  Finished.

103.  TM:  Experiment 103; random opponent play
Wed Jan 18 11:42:33 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 64
   layer = 3 n_nodes = 64
   n_weights = 20480 (FC)
   include_biases = 1
base_learning_rate = 0.0001; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.0001
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.025
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 150000 episodes
Old weights period = 10000 episodes
n_max_episodes = 1000000
Frame skip = -1
Starting episode for linear epsilon decay = 100
Stopping episode for linear epsilon decay = 500000
Randomly switch starting player = 0
Random seed = -68551
Process ID = 36477

After 286K episodes, win frac = 0.567
After 958K episodes, win_frac = 0.729


104.   T1 Experiment 104: random opponent play
Wed Jan 18 11:48:45 2017
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 64
   layer = 3 n_nodes = 64
   n_weights = 20480 (FC)
   include_biases = 1
base_learning_rate = 0.0003; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
minimum epsilon = 0.025
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 150000 episodes
Old weights period = 10000 episodes
n_max_episodes = 1000000
Frame skip = -1
Starting episode for linear epsilon decay = 100
Stopping episode for linear epsilon decay = 500000
Randomly switch starting player = 0
Random seed = -68924
Process ID = 5705

After 428K episodes, win frac = 0.586

After 998 episodes, win frac = 0.749

------------------------------------------------------
RUNNING EXPERIMENTS
------------------------------------------------------

---------------------
Titan 1:
---------------------

105.  Experiment 105
Tue Jan 24 08:07:40 2017
output_subdir = ./experiments/qtrain/expt105/
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 256
   layer = 2 n_nodes = 256
   layer = 3 n_nodes = 64
   n_weights = 98304 (FC)
include_biases = 1
base_learning_rate = 0.0003
batch_size = 1
perm_symmetrize_weights_and_biases = 0
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 150000 episodes
Old weights period = 10000 episodes
n_max_episodes = 1000000
Frame skip = -1
Starting episode for linear epsilon decay = 100
Stopping episode for linear epsilon decay = 500000
Randomly switch starting player = 0
Random seed = -74056
Process ID = 11345

106.  Experiment 106
Tue Jan 24 08:09:38 2017
output_subdir = ./experiments/qtrain/expt106/
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 64
   layer = 1 n_nodes = 256
   layer = 2 n_nodes = 256
   layer = 3 n_nodes = 64
   n_weights = 98304 (FC)
include_biases = 1
base_learning_rate = 0.0003
batch_size = 1
perm_symmetrize_weights_and_biases = 0
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.95
n_actions = 64
Leaky ReLU small slope = 0.01
Learning rate decrease period = 150000 episodes
Old weights period = 10000 episodes
n_max_episodes = 1000000
Frame skip = -1
Starting episode for linear epsilon decay = 100
Stopping episode for linear epsilon decay = 500000
Randomly switch starting player = 0
Random seed = -74159
Process ID = 11461

---------------------
Titan 3:
---------------------

---------------------
Thinkmate
---------------------

---------------------
m6700
---------------------



==========================================================================
Conclusion: As of Sun Jan 15, 2017 we have found NO set of hyperparams for
which minimax Q learning of an agent against a randomly playing opponent
works even remotely decently.  So we give up on minimax Q learning for now.
==========================================================================
Conclusion: As of 1/19/17, we believe adding biases is useful for
Q-learning.  But it is still insufficient for agent to robustly play
against random moving opponent.

==========================================================================

Conclusion:  Base learning rate 1E-5 is too large.
Conclusion:  Base learning rate 3E-6 is too large
Conclusion:  Base learning rate 2E-6 is ?

Conclusion:  Base learning rate 1E-6 is OK

Conclusion:  Max Q percentile curves look better for H1=H2=32 than for
H1=H2=16

Conclusion:  Max Q percentile curves look better for H1=H2=32 than for
H1=32; H2=16 and better than H1 = H2 = 16

Conclusion:  Max Q percentile curves look better for H1=32 H2 = 64 has bad
loss behavior

Conclusion:  Use old weights period = 10K rather than 5K

Conclusion: H1 = 128; H2 = 64; blr = 1 or 2 E-6 yields POOR results

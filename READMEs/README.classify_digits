========================================================================
Classifying SVHN and MNIST digits via Caffe finetuning of VGG-16 model
========================================================================
Last updated 2/29/16; 3/1/16; 7/23/16; 7/25/16
========================================================================

In this README, we define the following root directory

	$root= ~/programs/c++/svn/projects/src/mains/

	$caffe = ~/software/caffe_custom/

	$datatext = /data/peter_stuff/imagery/syntext/

---------------------------------------
GENERATING SYNTHETIC TEXT TRAINING DATA
---------------------------------------

*.  Program $root/imagetext/CREATE_NUMBERS generates random large numbers
and exports them to an output text file.

*.  Program $root/imagetext/GENERATE_CHAR_IMAGES uses ImageMagick to
generate jpg images of synthetic letters and numbers.

The UV image is initially oriented in the YZ world-plane.  It is rotated
through az about the world z axis, el about the world y-axis and roll about
the the world x-axis.  The az, el and roll angles are random variables
selected from gaussian distributions with reasonable standard deviations to
simulate camera views of text in the wild.  The rotated synthetic character
image is subsequently projected back into the YZ world-plane.

Text characters are assigned random foreground and background colors.
Reasonable differences in hue, saturation and value are imposed between
foreground and background colorings.  Random linear shading is performed on
both foreground and background colors to simulate illumination variations.
Random gaussian noise is also added to the RGB color channels of every
pixel.

The projected image is cropped so that the rotated character fills most of
the projection. The cropped character is subsequently scaled so that its
height equals 32 pixels in size.  Finally, the output character image is
blurred by a gaussian with variable sigma value.

			 ./generate_char_images

*.  Program $root/imagetext/EXTEND_IMAGECHIPS imports image chips whose
pixel sizes are less than 256x256.  It calls ImageMagick and superposes the
image chips onto black backgrounds which are 256x256 in size.  The
trivially extended image chips are exported to an extended_chips
subdirectory.

Run program EXTEND_IMAGECHIPS from within ALL subdirectories holding both
text AND non-text image chips:

                        ./extend_imagechips

After extended image chips are created within a subdir of
$root/imagetext/training_data/synthetic_numbers/NNNNN/extended_chips/, move
all extended chips into a new subdir of $datatext.

  e.g. mv ./extended_chips/*.* /data/peter_stuff/imagery/syntext/numbers_Jan16

[ Note added on 7/23/16:  We learned the hard and painful way that the
default image --> LMDB converter used by Caffe appears to expect JPG rather
than PNG image chip input!]

*.  Program $root/imagetext/PREPARE_FINETUNING_INPUTS imports a set of text
and non-text image chips from a specified subdirectory.  It generates a
text file containing the image chip files' full pathnames vs their class
IDs.  Upper and lower case letters are assigned the same class IDs.
PREPARE_FINETUNING_INPUTS then shuffles the pairs of pathnames vs class IDs
multiple times.  The shuffled text file becomes an input to Caffe
fine-tuning of VGG-16/Alexnet networks.

			./prepare_finetuning_inputs

--------------------------
RUNNING CAFFE FINETUNING
--------------------------

*.  Within $caffe/experiments, unpack new_text_experiments.tgz.  Then
rename the unpacked "text/" subdir to a new folder containing today's date:

		e.g. mv text ./20160116_numbers

Be sure $caffe/experiments/DATED_TEXT_DIR/list/VGG16/dataset.txt points to
LATEST version of shuffled training/validation input imagery text file
within /data/peter_stuff/imagery/syntext !

*.  Create a new caffe run script within $caffe:

		e.g. copy finetune_20160115_numbers run_finetune_20160116_numbers

Be sure to alter its EXP line.  For example:

# Experiment
EXP=experiments/20160125_numbers


Then from within $caffe_experiments, chant

	run_finetune_20160116_numbers VGG16

in order to start finetuning neural network.

MAKE SURE TO CHECK THAT LOG FILE IS QUICKLY CREATED WITHIN SUBDIRECTORY
WHERE WE EXPECT IT TO GO !!!

*.  After caffe finetuning has essentially finished, copy
$caffe/experiments/DATED_TEXT_DIR/logs/bvlc_reference_caffenet/caffe.bin.INFO
into /data/caffe/faces/trained_models/DATED_SUBDIR.

Program $root/machine_learning/caffe/TUNING_PERFORMANCE imports a
caffe.bin.INFO file generated by caffe finetuning run on a GPU machine.  It
plots the loss function as a function of training epoch number for the
training set.  It also plots the accuracy value as a function of training
epoch number for the validation set.

			   tuning_performance

-------------------------------
PERFORMING CHARACTER INFERENCE
-------------------------------

*.  Place fine-tuned caffe models into subdirs of /data/caffe/digits/ . 

*.  Program $root/machine_learning/caffe/CAFFE_CLASSIFICATION is a variant
of the caffe example program classification.cpp.  It takes in a finetuned
caffe model along with folder containing a set of input test images.
CAFFE_CLASSIFICATION loops over each test image and prints out the labels
for the top N = 5 classes which the finetuned caffe model predicts for the
input image.


./caffe_classification \
/data/caffe/digits/test.prototxt \
/data/caffe/digits/test.caffemodel \
./data/ilsvrc12/imagenet_mean.binaryproto \
/data/caffe/digits/object_names.classes \
/data1/imagetext/training_data/svhn_image_chips/extra/

-------------------------------------
Text Classifier Performance: Letters
-------------------------------------

*.  On 1/16/2016, we started finetuning AlexNet on 640K training images and
validating on 160K images for letters.  Half of the training and validation
images contain no letter.  Each rescaled digit and non-text image ranges in
pixel size from 20x20 to 128x128.  Rescaled letters are pasted onto 256x256
black backgrounds.  Over 24 hours of training needed to finetune network
for just one training epoch.  As of 11 am on 1/18/2016, validation accuracy
is around 86% and appears to be very slowly incrasing.  Base learning rate
= 0.001 with SGD solver running on Thinkmate.

*.  On 1/18/2016, we started finetuning VGG16 on 216K training images and
54K validation images for letters.  Synthetic data set contains 5K examples
per letter * (26 upper-case + 26-lower case letters) + 10K non-letter
examples = 270K examples.  Training batch size = 64, validation batch size
= 16.

test_interval: 50
test_iter: 1077
base_lr: 0.001
lr_policy: "step"
gamma: 0.33
stepsize: 2000
display: 10
max_iter: 126803
momentum: 0.9
weight_decay: 0.0005

Asymptotic validation accuracy = 79.5% reached around 1 training epoch.
This is significantly *worse* than the 86% validation accuracy we
previously achieved for 640K training + 160K validation letter images (but
with square image chip sizes) using AlexNet

-------------------------------------
Text Classifier Performance: Digits
-------------------------------------

*.  On 1/16/2016, we generated a set of 400K training images and 100K
validation images for digits.  Half of the training and validation images
contain no digit.  Each rescaled digit and non-text image ranges in pixel
size from 20x20 to 128x128.  Rescaled digits are pasted onto 256x256 black
backgrounds.  Asymptotic validation accuracy = 92%.

*.  On 1/17/2016, we started finetuning AlexNet on the 800K synthetic
letter dataset with exactly the same parameters we had used on Titan1
except with the learning rate increased from 0.001 to 0.003.  After more
than 12 hours of training, both the training and validation curves look
terrible!  So learning rate = 0.003 is definitely too large!

*.  On 1/18/2016, we started finetuning VGG16 on 48K training images and
12K validation images for digits.  Synthetic data set contains 5K examples
per digit * 10 digits + 10K non-digit examples = 60K examples.  Training
batch size = 64, validation batch size = 16.  

base_lr: 0.001
lr_policy: "step"
gamma: 0.33
stepsize: 2000
display: 10
max_iter: 28300
momentum: 0.9
weight_decay: 0.0005

Asymptotic validation accuracy = 90% reached around 3 training epochs.

*.  On 1/19/2016, we started finetuning VGG16 on 96 traking images and 24K
validation images for digits.  Synthetic data set contains 10K examples per
digit * 10 digits + 20K non-digit examples = 120K examples.  Training batch
size = 64, validation batch size = 1.

test_interval: 50
test_iter: 479
base_lr: 0.001
lr_policy: "step"
gamma: 0.33
stepsize: 5000
display: 10
max_iter: 56442
momentum: 0.9
weight_decay: 0.0005

Asymptotic validation accuracy = 93% reached around 3 training epochs.

*.  On 1/25/16, we finetuned VGG-16 on a new set of O(10K per digit * 10
digits + 10K non-text) patches.  These O(110K) synthetic characters were
all greyscale, and their gravity locations were randomized.  Asymptotic
validation accuracy was around 85.3%.

We then tested the performance of the trained classifier on O(3K) of the
SVHN "extra" digit set:

n_correct = 2432 n_incorrect = 568 frac_correct = 0.8107
Confusion matrix:

0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	
5.0000	255.0000	4.0000	0.0000	0.0000	0.0000	0.0000	2.0000	0.0000	0.0000	0.0000	
33.0000	3.0000	222.0000	2.0000	1.0000	17.0000	3.0000	0.0000	9.0000	1.0000	3.0000	
13.0000	10.0000	1.0000	431.0000	0.0000	4.0000	6.0000	1.0000	2.0000	3.0000	6.0000	
19.0000	9.0000	2.0000	14.0000	247.0000	5.0000	51.0000	5.0000	16.0000	12.0000	2.0000	
13.0000	2.0000	12.0000	11.0000	0.0000	196.0000	2.0000	2.0000	2.0000	1.0000	1.0000	
12.0000	7.0000	2.0000	6.0000	1.0000	4.0000	303.0000	21.0000	0.0000	2.0000	1.0000	
7.0000	25.0000	0.0000	1.0000	1.0000	0.0000	0.0000	233.0000	0.0000	12.0000	1.0000	
15.0000	1.0000	31.0000	11.0000	1.0000	3.0000	1.0000	0.0000	188.0000	0.0000	2.0000	
7.0000	32.0000	0.0000	0.0000	1.0000	0.0000	1.0000	6.0000	0.0000	195.0000	4.0000	
7.0000	24.0000	0.0000	1.0000	0.0000	2.0000	1.0000	0.0000	0.0000	4.0000	162.0000	

*.  On 1/25/16, we performed a second round of V66-16 finetuning on a
refined set of O(10K per digit * 10 digits + 10K non-text) patches.  In
this second set, we intentionally reduced blurring for very small patches.
We also penalized background patches with high entropy.  These O(110K)
synthetic characters were all greyscale, and their gravity locations were
randomized.  Asymptotic validation accuracy was around 88.5%.

n_correct = 1907 n_incorrect = 1093 frac_correct = 0.6357
Confusion matrix:

0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	
38.0000	226.0000	0.0000	0.0000	0.0000	0.0000	0.0000	2.0000	0.0000	0.0000	0.0000	
160.0000	1.0000	113.0000	0.0000	1.0000	11.0000	2.0000	1.0000	4.0000	1.0000	0.0000	
82.0000	2.0000	0.0000	379.0000	1.0000	4.0000	0.0000	1.0000	1.0000	1.0000	6.0000	
95.0000	4.0000	0.0000	8.0000	191.0000	1.0000	50.0000	5.0000	6.0000	20.0000	2.0000	
99.0000	1.0000	6.0000	17.0000	2.0000	115.0000	0.0000	1.0000	0.0000	1.0000	0.0000	
47.0000	4.0000	2.0000	1.0000	8.0000	1.0000	250.0000	44.0000	1.0000	0.0000	1.0000	
40.0000	19.0000	0.0000	1.0000	3.0000	0.0000	0.0000	204.0000	0.0000	13.0000	0.0000	
102.0000	0.0000	21.0000	9.0000	0.0000	2.0000	0.0000	0.0000	118.0000	0.0000	1.0000	
68.0000	14.0000	0.0000	0.0000	0.0000	0.0000	1.0000	2.0000	0.0000	160.0000	1.0000	
24.0000	12.0000	0.0000	0.0000	1.0000	1.0000	1.0000	1.0000	0.0000	10.0000	151.0000	

---------------------

*.  On morning of 1/26/16, we finetuned VGG-16 on a set of O(60K) SVHN
digits. but with no non-character background patches.  Asymptotic
validation accuracy was around 92.5% !

n_correct = 2899 n_incorrect = 101 frac_correct = 0.9663
Confusion matrix:

0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	
0.0000	264.0000	0.0000	0.0000	1.0000	0.0000	0.0000	0.0000	0.0000	0.0000	1.0000	
0.0000	1.0000	273.0000	4.0000	2.0000	6.0000	4.0000	0.0000	2.0000	2.0000	0.0000	
0.0000	1.0000	0.0000	472.0000	0.0000	1.0000	1.0000	1.0000	1.0000	0.0000	0.0000	
0.0000	0.0000	1.0000	4.0000	364.0000	2.0000	8.0000	1.0000	1.0000	0.0000	1.0000	
0.0000	0.0000	2.0000	4.0000	1.0000	234.0000	0.0000	0.0000	0.0000	0.0000	1.0000	
0.0000	0.0000	2.0000	2.0000	3.0000	0.0000	351.0000	1.0000	0.0000	0.0000	0.0000	
0.0000	0.0000	0.0000	0.0000	0.0000	1.0000	1.0000	275.0000	0.0000	3.0000	0.0000	
0.0000	1.0000	6.0000	0.0000	3.0000	4.0000	0.0000	0.0000	239.0000	0.0000	0.0000	
0.0000	1.0000	0.0000	0.0000	3.0000	0.0000	2.0000	5.0000	0.0000	233.0000	2.0000	
0.0000	1.0000	1.0000	2.0000	1.0000	0.0000	1.0000	0.0000	0.0000	1.0000	194.0000	

On 2/4/16, we tested this SVHN-trained model on 2.5K mnist digits.  Its
performance is much worse than our synthetic-trained model for mnist!

n_correct = 1486 n_incorrect = 1014 frac_correct = 0.5944
Confusion matrix:

0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	
0.0000	94.0000	0.0000	50.0000	5.0000	0.0000	4.0000	11.0000	0.0000	41.0000	29.0000	
0.0000	3.0000	185.0000	4.0000	4.0000	27.0000	1.0000	9.0000	44.0000	0.0000	0.0000	
0.0000	0.0000	0.0000	141.0000	100.0000	0.0000	3.0000	0.0000	2.0000	0.0000	1.0000	
0.0000	0.0000	0.0000	1.0000	194.0000	0.0000	44.0000	0.0000	0.0000	0.0000	1.0000	
0.0000	0.0000	5.0000	9.0000	42.0000	132.0000	21.0000	3.0000	31.0000	6.0000	31.0000	
0.0000	0.0000	0.0000	3.0000	7.0000	0.0000	212.0000	4.0000	0.0000	0.0000	1.0000	
0.0000	0.0000	0.0000	104.0000	2.0000	0.0000	11.0000	136.0000	1.0000	0.0000	0.0000	
0.0000	1.0000	0.0000	15.0000	24.0000	0.0000	0.0000	1.0000	231.0000	1.0000	2.0000	
0.0000	0.0000	1.0000	2.0000	123.0000	0.0000	33.0000	12.0000	0.0000	49.0000	2.0000	
0.0000	0.0000	0.0000	12.0000	54.0000	0.0000	0.0000	1.0000	32.0000	33.0000	112.0000	

r = 0 class label = __other__  n_incorrect_classifications = 0
r = 1 class label = 0  n_incorrect_classifications = 140
r = 2 class label = 1  n_incorrect_classifications = 92
r = 3 class label = 2  n_incorrect_classifications = 106
r = 4 class label = 3  n_incorrect_classifications = 46
r = 5 class label = 4  n_incorrect_classifications = 148
r = 6 class label = 5  n_incorrect_classifications = 15
r = 7 class label = 6  n_incorrect_classifications = 118
r = 8 class label = 7  n_incorrect_classifications = 44
r = 9 class label = 8  n_incorrect_classifications = 173
r = 10 class label = 9  n_incorrect_classifications = 132

---------------------

*.  On afternoon of 1/26/16, we finetuned VGG-16 on a set of O(60K digits +
10K background) SVHN digits.  Asymptotic validation accuracy was around 93%!

Confusion matrix for 3000 SVHN test patches:

n_correct = 2887 n_incorrect = 113 frac_correct = 0.9623
Confusion matrix:

0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	
0.0000	264.0000	0.0000	0.0000	0.0000	0.0000	1.0000	0.0000	0.0000	0.0000	1.0000	
0.0000	0.0000	274.0000	3.0000	3.0000	6.0000	4.0000	0.0000	2.0000	2.0000	0.0000	
0.0000	0.0000	2.0000	468.0000	2.0000	3.0000	1.0000	0.0000	1.0000	0.0000	0.0000	
0.0000	0.0000	1.0000	3.0000	364.0000	1.0000	7.0000	0.0000	1.0000	4.0000	1.0000	
0.0000	0.0000	6.0000	1.0000	2.0000	231.0000	0.0000	1.0000	1.0000	0.0000	0.0000	
0.0000	0.0000	2.0000	3.0000	3.0000	1.0000	349.0000	0.0000	1.0000	0.0000	0.0000	
0.0000	3.0000	2.0000	0.0000	0.0000	0.0000	1.0000	266.0000	0.0000	8.0000	0.0000	
0.0000	0.0000	10.0000	1.0000	2.0000	1.0000	0.0000	0.0000	238.0000	0.0000	1.0000	
0.0000	0.0000	0.0000	0.0000	2.0000	0.0000	2.0000	2.0000	0.0000	240.0000	0.0000	
0.0000	0.0000	1.0000	2.0000	1.0000	1.0000	0.0000	0.0000	0.0000	3.0000	193.0000	

Confusion matrix for 3000 SVHN test patches + 595 non-text patches:
n_correct = 3230 n_incorrect = 365 frac_correct = 0.8985
Confusion matrix:

339.0000	6.0000	0.0000	71.0000	46.0000	86.0000	8.0000	3.0000	0.0000	36.0000	0.0000	
0.0000	266.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	
0.0000	6.0000	270.0000	0.0000	3.0000	7.0000	3.0000	0.0000	3.0000	2.0000	0.0000	
0.0000	0.0000	2.0000	468.0000	2.0000	1.0000	1.0000	0.0000	1.0000	2.0000	0.0000	
0.0000	1.0000	1.0000	3.0000	367.0000	1.0000	4.0000	0.0000	1.0000	3.0000	1.0000	
0.0000	1.0000	3.0000	0.0000	3.0000	233.0000	0.0000	0.0000	1.0000	0.0000	1.0000	
0.0000	0.0000	2.0000	3.0000	4.0000	0.0000	347.0000	1.0000	1.0000	1.0000	0.0000	
0.0000	4.0000	1.0000	0.0000	0.0000	1.0000	1.0000	271.0000	0.0000	2.0000	0.0000	
0.0000	0.0000	7.0000	0.0000	4.0000	3.0000	0.0000	0.0000	238.0000	1.0000	0.0000	
0.0000	2.0000	0.0000	0.0000	4.0000	0.0000	0.0000	2.0000	0.0000	238.0000	0.0000	
0.0000	1.0000	0.0000	1.0000	1.0000	2.0000	0.0000	0.0000	0.0000	3.0000	193.0000	

On 2/4/16, we tested this SVHN-trained model on 2.5K mnist digits.  Its
performance is much worse than our synthetic-trained model for mnist!

n_correct = 1155 n_incorrect = 1345 frac_correct = 0.4620
Confusion matrix:

0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	
75.0000	69.0000	0.0000	46.0000	16.0000	0.0000	3.0000	7.0000	0.0000	17.0000	1.0000	
33.0000	1.0000	98.0000	3.0000	2.0000	54.0000	1.0000	16.0000	69.0000	0.0000	0.0000	
2.0000	0.0000	0.0000	74.0000	169.0000	0.0000	0.0000	0.0000	1.0000	1.0000	0.0000	
1.0000	0.0000	0.0000	0.0000	229.0000	0.0000	10.0000	0.0000	0.0000	0.0000	0.0000	
1.0000	0.0000	2.0000	6.0000	133.0000	88.0000	13.0000	4.0000	23.0000	6.0000	4.0000	
0.0000	0.0000	0.0000	1.0000	32.0000	0.0000	190.0000	4.0000	0.0000	0.0000	0.0000	
2.0000	0.0000	0.0000	88.0000	5.0000	1.0000	8.0000	149.0000	1.0000	0.0000	0.0000	
1.0000	0.0000	0.0000	6.0000	45.0000	0.0000	0.0000	0.0000	222.0000	1.0000	0.0000	
0.0000	0.0000	1.0000	1.0000	163.0000	0.0000	11.0000	12.0000	2.0000	32.0000	0.0000	
0.0000	0.0000	0.0000	24.0000	164.0000	0.0000	1.0000	0.0000	24.0000	27.0000	4.0000	


r = 0 class label = __other__  n_incorrect_classifications = 0
r = 1 class label = 0  n_incorrect_classifications = 165
r = 2 class label = 1  n_incorrect_classifications = 179
r = 3 class label = 2  n_incorrect_classifications = 173
r = 4 class label = 3  n_incorrect_classifications = 11
r = 5 class label = 4  n_incorrect_classifications = 192
r = 6 class label = 5  n_incorrect_classifications = 37
r = 7 class label = 6  n_incorrect_classifications = 105
r = 8 class label = 7  n_incorrect_classifications = 53
r = 9 class label = 8  n_incorrect_classifications = 190
r = 10 class label = 9  n_incorrect_classifications = 240


---------------------

On 2/29/16, we finetuned VGG-16 on 50K mnist digits but with no
non-character background patches.  Asymptotic validation accuracy was
around 99.3!

10K held-out MNIST digits: n_correct = 9933 n_incorrect = 67 frac_correct = 0.9933
Confusion matrix:

0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	
0.0000	999.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	1.0000	1.0000	
0.0000	0.0000	1124.0000	0.0000	0.0000	0.0000	0.0000	0.0000	3.0000	0.0000	0.0000	
0.0000	0.0000	2.0000	981.0000	2.0000	0.0000	0.0000	0.0000	1.0000	3.0000	2.0000	
0.0000	0.0000	0.0000	0.0000	1024.0000	0.0000	3.0000	0.0000	2.0000	3.0000	0.0000	
0.0000	0.0000	1.0000	0.0000	0.0000	973.0000	0.0000	1.0000	2.0000	0.0000	3.0000	
0.0000	0.0000	0.0000	0.0000	1.0000	0.0000	858.0000	2.0000	0.0000	2.0000	0.0000	
0.0000	3.0000	0.0000	0.0000	0.0000	0.0000	2.0000	1007.0000	0.0000	1.0000	1.0000	
0.0000	0.0000	4.0000	0.0000	0.0000	3.0000	1.0000	0.0000	1060.0000	1.0000	1.0000	
0.0000	0.0000	0.0000	0.0000	1.0000	3.0000	4.0000	1.0000	1.0000	934.0000	0.0000	
0.0000	1.0000	1.0000	0.0000	0.0000	2.0000	0.0000	0.0000	0.0000	1.0000	973.0000	

r = 0 class label = __other__  n_incorrect_classifications = 0 frac_incorrect = -nan
r = 1 class label = 0  n_incorrect_classifications = 2 frac_incorrect = 0.0020
r = 2 class label = 1  n_incorrect_classifications = 3 frac_incorrect = 0.0027
r = 3 class label = 2  n_incorrect_classifications = 10 frac_incorrect = 0.0101
r = 4 class label = 3  n_incorrect_classifications = 8 frac_incorrect = 0.0078
r = 5 class label = 4  n_incorrect_classifications = 7 frac_incorrect = 0.0071
r = 6 class label = 5  n_incorrect_classifications = 5 frac_incorrect = 0.0058
r = 7 class label = 6  n_incorrect_classifications = 7 frac_incorrect = 0.0069
r = 8 class label = 7  n_incorrect_classifications = 10 frac_incorrect = 0.0093
r = 9 class label = 8  n_incorrect_classifications = 10 frac_incorrect = 0.0106
r = 10 class label = 9  n_incorrect_classifications = 5 frac_incorrect = 0.0051

But this mnist-trained model's performance on the 3K cleaned SVHN test
digits is bad!

n_correct = 583 n_incorrect = 2417 frac_correct = 0.1943
Confusion matrix:

0.0000	0.0000	84.0000	0.0000	0.0000	0.0000	26.0000	0.0000	0.0000	13.0000	0.0000	
0.0000	18.0000	151.0000	0.0000	0.0000	0.0000	40.0000	0.0000	8.0000	47.0000	0.0000	
0.0000	0.0000	213.0000	0.0000	0.0000	0.0000	26.0000	0.0000	29.0000	8.0000	0.0000	
0.0000	0.0000	186.0000	5.0000	0.0000	0.0000	53.0000	0.0000	4.0000	214.0000	0.0000	
0.0000	0.0000	132.0000	0.0000	3.0000	0.0000	98.0000	0.0000	3.0000	125.0000	0.0000	
0.0000	0.0000	144.0000	0.0000	0.0000	9.0000	31.0000	0.0000	19.0000	29.0000	0.0000	
0.0000	0.0000	99.0000	0.0000	0.0000	0.0000	165.0000	0.0000	4.0000	76.0000	0.0000	
0.0000	12.0000	84.0000	0.0000	0.0000	0.0000	30.0000	0.0000	4.0000	141.0000	0.0000	
0.0000	0.0000	111.0000	0.0000	0.0000	0.0000	64.0000	0.0000	26.0000	34.0000	0.0000	
0.0000	2.0000	73.0000	0.0000	0.0000	0.0000	17.0000	0.0000	1.0000	144.0000	0.0000	
0.0000	7.0000	72.0000	0.0000	0.0000	0.0000	24.0000	0.0000	1.0000	91.0000	0.0000	


r = 0 class label = __other__  n_incorrect_classifications = 123 frac_incorrect = 1.0000
r = 1 class label = 0  n_incorrect_classifications = 246 frac_incorrect = 0.9318
r = 2 class label = 1  n_incorrect_classifications = 63 frac_incorrect = 0.2283
r = 3 class label = 2  n_incorrect_classifications = 457 frac_incorrect = 0.9892
r = 4 class label = 3  n_incorrect_classifications = 358 frac_incorrect = 0.9917
r = 5 class label = 4  n_incorrect_classifications = 223 frac_incorrect = 0.9612
r = 6 class label = 5  n_incorrect_classifications = 179 frac_incorrect = 0.5203
r = 7 class label = 6  n_incorrect_classifications = 271 frac_incorrect = 1.0000
r = 8 class label = 7  n_incorrect_classifications = 209 frac_incorrect = 0.8894
r = 9 class label = 8  n_incorrect_classifications = 93 frac_incorrect = 0.3924
r = 10 class label = 9  n_incorrect_classifications = 195 frac_incorrect = 1.0000


*.  We are extensively reworking our entire synthetic text generation
pipeline as of Thurs, Jan 28, 2016.  We want to systematically test this
pipeline's performance.  

Jan 28 Test A: O(48K) 128x128 image chips with non-rotated digits (no
background class) with no gaussian noise or non-center gravity superposed
on constant color backgrounds.  Validation accuracy > 99.5% !!!

Jan 28 Test B: O(48K) 128x128 image chips with non-rotated digits (no
background class) with gaussian noise or non-center gravity superposed on
constant color backgrounds.  Validation accuracy > 99.5% !!!

Jan 28 Test C: O(48K) 128x128 image chips with non-rotated digits (no
background class) with gaussian noise and variable gravity superposed on
constant color backgrounds.  Validation accuracy > 99.0% !!!

Jan 28 Test D: O(48K) image chips with pixel sizes in interval [16,72] and
non-rotated digits (no background class) with gaussian noise and variable
gravity superposed on constant color backgrounds.  Asymptotic validation
accuracy around 92.5% reached by at least iteration 3000.

Jan 29 Test E: O(50K) 32x54 image chips with non-rotated digits (no
background class) with gaussian noise and variable gravity superposed on
constant color backgrounds.  Validation accuracy around 97.2% after (only)
800 training iterations.

Jan 29 Test F: O(50K) 32x54 image chips with non-rotated digits (no
background class) with gaussian noise, blurring and variable gravity
superposed on constant color backgrounds.  Validation accuracy around 98.5%
[97.8%] after 1600 [850] training iterations.

-------------------
Jan 29 Test G: O(60K) image chips with pixel widths ranging from 25 - 45,
non-rotated digits (no background class), gaussian noise, blurring and
variable gravity superposed on constant color backgrounds.  Validation
accuracy around 97% [95%] after 2500 [1400] training iterations.

Performance of this trained caffe model on 3K SVHN image chips (with no
non_char chips)

n_correct = 2532 n_incorrect = 468 frac_correct = 0.8440
Confusion matrix:

0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	
0.0000	243.0000	3.0000	0.0000	0.0000	3.0000	0.0000	11.0000	0.0000	4.0000	2.0000	
0.0000	1.0000	234.0000	0.0000	1.0000	45.0000	3.0000	2.0000	6.0000	2.0000	0.0000	
0.0000	2.0000	5.0000	408.0000	4.0000	6.0000	0.0000	10.0000	9.0000	23.0000	10.0000	
0.0000	5.0000	5.0000	4.0000	315.0000	6.0000	10.0000	10.0000	5.0000	17.0000	5.0000	
0.0000	0.0000	7.0000	7.0000	1.0000	213.0000	1.0000	0.0000	10.0000	2.0000	1.0000	
0.0000	2.0000	3.0000	3.0000	16.0000	7.0000	281.0000	43.0000	0.0000	3.0000	1.0000	
0.0000	14.0000	0.0000	0.0000	1.0000	4.0000	1.0000	243.0000	1.0000	15.0000	1.0000	
0.0000	0.0000	24.0000	5.0000	4.0000	10.0000	0.0000	3.0000	197.0000	1.0000	9.0000	
0.0000	14.0000	0.0000	0.0000	4.0000	0.0000	0.0000	4.0000	0.0000	222.0000	2.0000	
0.0000	7.0000	1.0000	0.0000	3.0000	1.0000	0.0000	3.0000	1.0000	9.0000	176.0000	

-------------------
Jan 29 Test G_90K: O(90K) image chips with pixel widths ranging from 25 - 45,
non-rotated digits (no background class), gaussian noise, blurring and
variable gravity superposed on constant color backgrounds.  Validation
accuracy around 97.5% after 3900 training iterations.

n_correct = 2604 n_incorrect = 396 frac_correct = 0.8680
Confusion matrix:

0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	
0.0000	235.0000	4.0000	0.0000	1.0000	4.0000	0.0000	14.0000	0.0000	4.0000	4.0000	
0.0000	0.0000	241.0000	0.0000	3.0000	36.0000	4.0000	2.0000	5.0000	2.0000	1.0000	
0.0000	0.0000	4.0000	429.0000	13.0000	5.0000	3.0000	4.0000	3.0000	9.0000	7.0000	
0.0000	3.0000	4.0000	6.0000	330.0000	4.0000	13.0000	3.0000	3.0000	10.0000	6.0000	
0.0000	0.0000	9.0000	12.0000	1.0000	206.0000	2.0000	2.0000	7.0000	1.0000	2.0000	
0.0000	1.0000	2.0000	4.0000	13.0000	6.0000	317.0000	13.0000	0.0000	3.0000	0.0000	
0.0000	6.0000	1.0000	0.0000	2.0000	5.0000	5.0000	249.0000	0.0000	10.0000	2.0000	
0.0000	0.0000	16.0000	14.0000	7.0000	11.0000	1.0000	1.0000	195.0000	0.0000	8.0000	
0.0000	9.0000	0.0000	0.0000	5.0000	1.0000	0.0000	3.0000	0.0000	224.0000	4.0000	
0.0000	5.0000	1.0000	0.0000	3.0000	5.0000	1.0000	1.0000	1.0000	6.0000	178.0000	

-------------------
Jan 30 Test H: O(90K) image chips with pixel widths selected according to
exponential distribution based upon fit to SVHN image chips.  Non-rotated
digits, no background class, gaussian noise, blurring, variable gravity,
constant color backgrounds.  Validation accuracy around 97.5% after 2640
training iterations.

n_correct = 2599 n_incorrect = 401 frac_correct = 0.8663
Confusion matrix:

0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	
0.0000	238.0000	3.0000	0.0000	0.0000	0.0000	0.0000	13.0000	1.0000	7.0000	4.0000	
0.0000	1.0000	232.0000	0.0000	1.0000	36.0000	3.0000	4.0000	15.0000	2.0000	0.0000	
0.0000	1.0000	1.0000	429.0000	20.0000	7.0000	1.0000	1.0000	6.0000	8.0000	3.0000	
0.0000	1.0000	2.0000	3.0000	344.0000	5.0000	10.0000	3.0000	4.0000	8.0000	2.0000	
0.0000	0.0000	4.0000	13.0000	1.0000	206.0000	1.0000	2.0000	12.0000	3.0000	0.0000	
0.0000	0.0000	2.0000	2.0000	21.0000	5.0000	310.0000	13.0000	2.0000	2.0000	2.0000	
0.0000	6.0000	0.0000	0.0000	10.0000	4.0000	7.0000	235.0000	1.0000	16.0000	1.0000	
0.0000	1.0000	19.0000	12.0000	5.0000	9.0000	0.0000	1.0000	205.0000	0.0000	1.0000	
0.0000	5.0000	0.0000	0.0000	10.0000	0.0000	0.0000	3.0000	0.0000	225.0000	3.0000	
0.0000	1.0000	1.0000	0.0000	7.0000	2.0000	0.0000	1.0000	2.0000	12.0000	175.0000	

-------------------
Jan 30 Test I: O(90K) image chips with pixel widths selected according to
exponential distribution based upon fit to SVHN image chips.  Non-rotated
digits, no background class, gaussian noise, blurring, variable gravity,
variable backgrounds, and min foreground pixel fraction.  Validation
accuracy around 97.5% after 1.5ish epochs.

n_correct = 2608 n_incorrect = 392 frac_correct = 0.8693
Confusion matrix:

0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	
0.0000	256.0000	5.0000	0.0000	0.0000	0.0000	0.0000	2.0000	1.0000	1.0000	1.0000	
0.0000	3.0000	264.0000	0.0000	1.0000	14.0000	2.0000	4.0000	4.0000	1.0000	1.0000	
0.0000	2.0000	4.0000	434.0000	7.0000	5.0000	1.0000	2.0000	1.0000	7.0000	14.0000	
0.0000	5.0000	7.0000	4.0000	325.0000	4.0000	17.0000	5.0000	3.0000	11.0000	1.0000	
0.0000	3.0000	21.0000	24.0000	0.0000	180.0000	2.0000	4.0000	4.0000	2.0000	2.0000	
0.0000	1.0000	3.0000	3.0000	13.0000	3.0000	320.0000	14.0000	1.0000	1.0000	0.0000	
0.0000	9.0000	2.0000	0.0000	6.0000	2.0000	7.0000	238.0000	0.0000	15.0000	1.0000	
0.0000	1.0000	41.0000	12.0000	4.0000	2.0000	1.0000	0.0000	190.0000	0.0000	2.0000	
0.0000	13.0000	0.0000	0.0000	7.0000	0.0000	0.0000	2.0000	0.0000	221.0000	3.0000	
0.0000	4.0000	1.0000	0.0000	6.0000	1.0000	0.0000	0.0000	1.0000	8.0000	180.0000	

-------------------
Jan 31 Test J: O(90K) image chips with pixel widths selected according to
exponential distribution based upon fit to SVHN image chips.  Rotated
digits, no background class, gaussian noise, blurring, variable gravity,
variable backgrounds, improved min foreground pixel fraction, minimum
foreground visibility requirement.  Validation accuracy around 97.8% after
1.5ish epochs.

n_correct = 2510 n_incorrect = 490 frac_correct = 0.8367
Confusion matrix:

0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	
0.0000	255.0000	8.0000	0.0000	0.0000	1.0000	0.0000	2.0000	0.0000	0.0000	0.0000	
0.0000	3.0000	263.0000	0.0000	1.0000	16.0000	3.0000	2.0000	4.0000	1.0000	1.0000	
0.0000	12.0000	8.0000	412.0000	3.0000	5.0000	6.0000	6.0000	3.0000	2.0000	20.0000	
0.0000	13.0000	16.0000	1.0000	291.0000	5.0000	24.0000	12.0000	8.0000	11.0000	1.0000	
0.0000	1.0000	44.0000	14.0000	1.0000	172.0000	1.0000	2.0000	3.0000	1.0000	3.0000	
0.0000	4.0000	8.0000	4.0000	3.0000	3.0000	315.0000	21.0000	0.0000	1.0000	0.0000	
0.0000	29.0000	0.0000	0.0000	2.0000	1.0000	0.0000	236.0000	0.0000	11.0000	1.0000	
0.0000	4.0000	72.0000	2.0000	1.0000	3.0000	1.0000	0.0000	169.0000	0.0000	1.0000	
0.0000	17.0000	2.0000	0.0000	3.0000	1.0000	1.0000	4.0000	0.0000	215.0000	3.0000	
0.0000	10.0000	4.0000	0.0000	1.0000	2.0000	0.0000	0.0000	0.0000	2.0000	182.0000	

r = 0 class label = __other__ 	n_incorrect_classifications = 0
r = 1 class label = 0 		n_incorrect_classifications = 11
r = 2 class label = 1 		n_incorrect_classifications = 31
r = 3 class label = 2 		n_incorrect_classifications = 65
r = 4 class label = 3 		n_incorrect_classifications = 91
r = 5 class label = 4 		n_incorrect_classifications = 70
r = 6 class label = 5 		n_incorrect_classifications = 44
r = 7 class label = 6 		n_incorrect_classifications = 44
r = 8 class label = 7 		n_incorrect_classifications = 84
r = 9 class label = 8 		n_incorrect_classifications = 31
r = 10 class label = 9 		n_incorrect_classifications = 19

-------------------
Feb 1 Test K: O(90K) image chips with pixel widths selected according to
exponential distribution based upon fit to SVHN image chips.  Rotated
digits, no background class, gaussian noise, blurring, variable gravity,
variable backgrounds, improved min foreground pixel fraction.  Less
stringent minimum foreground visibility requirement compared to Jan 31 Test
J.  Significantly more blurring compared to Jan 31 Test J.  Small fraction
of chars with strokes have foreground_RGB = background_RGB.  Random RGBs
now have max_s = 0.55, v_min = 0.15 and v_max = 0.85.  O(1K) of 90K
training chips are now labeled as "non-text".

Validation accuracy around 95.0% after 1.5ish epochs.

As of 2/2/16, we test the trained caffe model against our (partially)
cleaned set of SVHN digits.

n_correct = 2660 n_incorrect = 340 frac_correct = 0.8867
Confusion matrix:

68.0000	20.0000	4.0000	2.0000	3.0000	7.0000	2.0000	4.0000	0.0000	5.0000	7.0000	
0.0000	258.0000	3.0000	0.0000	0.0000	2.0000	0.0000	1.0000	0.0000	0.0000	0.0000	
0.0000	1.0000	236.0000	0.0000	0.0000	30.0000	2.0000	1.0000	3.0000	0.0000	3.0000	
0.0000	1.0000	2.0000	440.0000	0.0000	1.0000	0.0000	1.0000	1.0000	7.0000	9.0000	
1.0000	3.0000	0.0000	0.0000	301.0000	1.0000	12.0000	6.0000	9.0000	24.0000	4.0000	
0.0000	1.0000	13.0000	16.0000	1.0000	192.0000	1.0000	0.0000	5.0000	0.0000	4.0000	
1.0000	2.0000	0.0000	0.0000	0.0000	2.0000	305.0000	31.0000	0.0000	2.0000	1.0000	
0.0000	8.0000	0.0000	0.0000	1.0000	0.0000	0.0000	243.0000	0.0000	19.0000	0.0000	
0.0000	1.0000	14.0000	3.0000	2.0000	6.0000	0.0000	0.0000	202.0000	0.0000	7.0000	
0.0000	5.0000	0.0000	0.0000	1.0000	0.0000	0.0000	0.0000	0.0000	229.0000	2.0000	
0.0000	5.0000	0.0000	0.0000	0.0000	1.0000	0.0000	0.0000	0.0000	3.0000	186.0000	

r = 0 class label = __other__  n_incorrect_classifications = 54
r = 1 class label = 0  n_incorrect_classifications = 6
r = 2 class label = 1  n_incorrect_classifications = 40
r = 3 class label = 2  n_incorrect_classifications = 22
r = 4 class label = 3  n_incorrect_classifications = 60
r = 5 class label = 4  n_incorrect_classifications = 41
r = 6 class label = 5  n_incorrect_classifications = 39
r = 7 class label = 6  n_incorrect_classifications = 28
r = 8 class label = 7  n_incorrect_classifications = 33
r = 9 class label = 8  n_incorrect_classifications = 8
r = 10 class label = 9  n_incorrect_classifications = 9

------------------- 

Feb 2 Test L: O(90K) image chips + O(1.5K) ambiguous partial non-digit
chips with pixel widths selected according to exponential distribution
based upon fit to SVHN image chips.  Rotated digits; gaussian noise;
simulated solar shadowing; significant blurring; variable gravity; variable
backgrounds; improved min foreground pixel fraction.  Small fraction of
chars with strokes have foreground_RGB = background_RGB.  Random RGBs now
assigned using fitted exponential and gaussian distributions for S & V.

This dataset died twice on Titan1 on Tues, Feb 2 before 8:15 am.  We
restarted it a 3rd time after deleting a few entries at the end of
dataset.txt.  It died again.  We then tried running a second, independent
version of Test L.  It died again with an ominous GPU error message.  We
ended up rebooting titan1.  This seems to have fixed this GPU problem.

Validation accuracy around 95.5% after 1.5ish epochs.

SVHN digits: n_correct = 2676 n_incorrect = 324 frac_correct = 0.8920
Confusion matrix:

63.0000	17.0000	15.0000	4.0000	5.0000	3.0000	0.0000	4.0000	0.0000	3.0000	9.0000	
0.0000	260.0000	3.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	1.0000	
1.0000	1.0000	260.0000	2.0000	0.0000	6.0000	1.0000	1.0000	3.0000	0.0000	1.0000	
3.0000	3.0000	1.0000	432.0000	1.0000	4.0000	0.0000	0.0000	0.0000	2.0000	16.0000
1.0000	1.0000	1.0000	6.0000	299.0000	3.0000	24.0000	7.0000	8.0000	6.0000	5.0000	
1.0000	1.0000	13.0000	9.0000	0.0000	203.0000	0.0000	1.0000	1.0000	0.0000	3.0000	
3.0000	1.0000	0.0000	0.0000	0.0000	2.0000	317.0000	20.0000	0.0000	0.0000	1.0000	
2.0000	12.0000	0.0000	0.0000	2.0000	0.0000	1.0000	242.0000	0.0000	11.0000	1.0000	
3.0000	1.0000	22.0000	16.0000	1.0000	3.0000	0.0000	0.0000	184.0000	0.0000	5.0000	
0.0000	5.0000	0.0000	0.0000	1.0000	0.0000	0.0000	1.0000	0.0000	227.0000	3.0000	
0.0000	2.0000	1.0000	0.0000	0.0000	2.0000	0.0000	0.0000	0.0000	1.0000	189.0000	

r = 0 class label = __other__  n_incorrect_classifications = 60
r = 1 class label = 0  n_incorrect_classifications = 4
r = 2 class label = 1  n_incorrect_classifications = 16
r = 3 class label = 2  n_incorrect_classifications = 30
r = 4 class label = 3  n_incorrect_classifications = 62
r = 5 class label = 4  n_incorrect_classifications = 29
r = 6 class label = 5  n_incorrect_classifications = 27
r = 7 class label = 6  n_incorrect_classifications = 29
r = 8 class label = 7  n_incorrect_classifications = 51
r = 9 class label = 8  n_incorrect_classifications = 10
r = 10 class label = 9  n_incorrect_classifications = 6

..................
Second independent run of Feb 2 Test L:

Validation accuracy around 95.5% after 1.5ish epochs

SVHN digits: n_correct = 2695 n_incorrect = 305 frac_correct = 0.8983
Confusion matrix:

57.0000	21.0000	12.0000	4.0000	4.0000	7.0000	5.0000	3.0000	2.0000	3.0000	5.0000	
0.0000	257.0000	4.0000	0.0000	0.0000	0.0000	0.0000	1.0000	0.0000	1.0000	1.0000	
1.0000	1.0000	251.0000	1.0000	0.0000	14.0000	1.0000	0.0000	4.0000	0.0000	3.0000	
0.0000	1.0000	2.0000	440.0000	3.0000	3.0000	1.0000	1.0000	0.0000	1.0000	10.0000	
0.0000	1.0000	1.0000	2.0000	307.0000	3.0000	20.0000	6.0000	9.0000	10.0000	2.0000	
1.0000	1.0000	13.0000	22.0000	0.0000	190.0000	0.0000	1.0000	2.0000	0.0000	2.0000	
1.0000	1.0000	0.0000	0.0000	1.0000	2.0000	326.0000	12.0000	0.0000	0.0000	1.0000	
1.0000	9.0000	0.0000	0.0000	2.0000	0.0000	1.0000	252.0000	0.0000	6.0000	0.0000	
1.0000	0.0000	23.0000	5.0000	3.0000	2.0000	1.0000	0.0000	199.0000	0.0000	1.0000	
0.0000	2.0000	0.0000	0.0000	1.0000	1.0000	0.0000	3.0000	0.0000	228.0000	2.0000	
0.0000	2.0000	1.0000	0.0000	1.0000	2.0000	0.0000	0.0000	0.0000	1.0000	188.0000	


r = 0 class label = __other__  n_incorrect_classifications = 66
r = 1 class label = 0  n_incorrect_classifications = 7
r = 2 class label = 1  n_incorrect_classifications = 25
r = 3 class label = 2  n_incorrect_classifications = 22
r = 4 class label = 3  n_incorrect_classifications = 54
r = 5 class label = 4  n_incorrect_classifications = 42
r = 6 class label = 5  n_incorrect_classifications = 18
r = 7 class label = 6  n_incorrect_classifications = 19
r = 8 class label = 7  n_incorrect_classifications = 36
r = 9 class label = 8  n_incorrect_classifications = 9
r = 10 class label = 9  n_incorrect_classifications = 7

..................

Two independently trained caffe models for Feb 2L tests were both run on
the 3K SVHN test set.  We found that for 187 of 3K test images (6.2%), the
top predictions from the two models disagreed.

I:  Arithmetic average of two caffe models' scores:

n_correct = 2645 n_incorrect = 355 frac_correct = 0.8817
Confusion matrix:

0.0000	78.0000	13.0000	3.0000	4.0000	6.0000	3.0000	4.0000	2.0000	3.0000	7.0000	
0.0000	259.0000	3.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	1.0000	1.0000	
0.0000	2.0000	260.0000	1.0000	0.0000	6.0000	1.0000	0.0000	3.0000	0.0000	3.0000	
0.0000	3.0000	2.0000	438.0000	2.0000	4.0000	0.0000	1.0000	0.0000	1.0000	11.0000	
0.0000	2.0000	1.0000	2.0000	306.0000	3.0000	21.0000	6.0000	8.0000	9.0000	3.0000	
0.0000	2.0000	12.0000	17.0000	0.0000	196.0000	0.0000	1.0000	2.0000	0.0000	2.0000	
0.0000	2.0000	0.0000	0.0000	1.0000	2.0000	323.0000	14.0000	0.0000	0.0000	2.0000	
0.0000	11.0000	0.0000	0.0000	2.0000	0.0000	0.0000	251.0000	0.0000	7.0000	0.0000	
0.0000	2.0000	24.0000	8.0000	1.0000	2.0000	1.0000	0.0000	194.0000	0.0000	3.0000	
0.0000	3.0000	0.0000	0.0000	1.0000	1.0000	0.0000	3.0000	0.0000	227.0000	2.0000	
0.0000	1.0000	1.0000	0.0000	0.0000	1.0000	0.0000	0.0000	0.0000	1.0000	191.0000	

r = 0 class label = __other__  n_incorrect_classifications = 123
r = 1 class label = 0  n_incorrect_classifications = 5
r = 2 class label = 1  n_incorrect_classifications = 16
r = 3 class label = 2  n_incorrect_classifications = 24
r = 4 class label = 3  n_incorrect_classifications = 55
r = 5 class label = 4  n_incorrect_classifications = 36
r = 6 class label = 5  n_incorrect_classifications = 21
r = 7 class label = 6  n_incorrect_classifications = 20
r = 8 class label = 7  n_incorrect_classifications = 41
r = 9 class label = 8  n_incorrect_classifications = 10
r = 10 class label = 9  n_incorrect_classifications = 4

II:  Max average of two caffe models' scores:

n_correct = 2643 n_incorrect = 357 frac_correct = 0.8810
Confusion matrix:

0.0000	78.0000	13.0000	3.0000	4.0000	6.0000	3.0000	4.0000	2.0000	3.0000	7.0000	
0.0000	258.0000	3.0000	0.0000	0.0000	0.0000	0.0000	1.0000	0.0000	1.0000	1.0000	
0.0000	1.0000	260.0000	1.0000	0.0000	7.0000	1.0000	0.0000	3.0000	0.0000	3.0000	
0.0000	3.0000	2.0000	438.0000	1.0000	4.0000	0.0000	1.0000	0.0000	1.0000	12.0000	
0.0000	2.0000	1.0000	2.0000	305.0000	3.0000	21.0000	6.0000	8.0000	10.0000	3.0000	
0.0000	2.0000	12.0000	18.0000	0.0000	196.0000	0.0000	0.0000	2.0000	0.0000	2.0000	
0.0000	3.0000	0.0000	0.0000	1.0000	2.0000	324.0000	13.0000	0.0000	0.0000	1.0000	
0.0000	12.0000	0.0000	0.0000	2.0000	0.0000	0.0000	250.0000	0.0000	7.0000	0.0000	
0.0000	2.0000	23.0000	9.0000	1.0000	2.0000	1.0000	0.0000	194.0000	0.0000	3.0000	
0.0000	3.0000	0.0000	0.0000	1.0000	1.0000	0.0000	3.0000	0.0000	227.0000	2.0000	
0.0000	1.0000	1.0000	0.0000	0.0000	1.0000	0.0000	0.0000	0.0000	1.0000	191.0000	

r = 0 class label = __other__  n_incorrect_classifications = 123
r = 1 class label = 0  n_incorrect_classifications = 6
r = 2 class label = 1  n_incorrect_classifications = 16
r = 3 class label = 2  n_incorrect_classifications = 24
r = 4 class label = 3  n_incorrect_classifications = 56
r = 5 class label = 4  n_incorrect_classifications = 36
r = 6 class label = 5  n_incorrect_classifications = 20
r = 7 class label = 6  n_incorrect_classifications = 21
r = 8 class label = 7  n_incorrect_classifications = 41
r = 9 class label = 8  n_incorrect_classifications = 10
r = 10 class label = 9  n_incorrect_classifications = 4

------------------- 

Feb 3 Test M: O(120K) image chips + O(1.2K) ambiguous partial non-digit
chips with pixel widths selected according to exponential distribution
based upon fit to SVHN image chips.  Rotated digits; gaussian noise;
simulated solar shadowing; significant blurring; variable gravity; variable
backgrounds; improved min foreground pixel fraction.  Small fraction of
chars with strokes have foreground_RGB = background_RGB.  Random RGBs now
assigned using fitted exponential and gaussian distributions for S & V.
Random horizontal/vertical lines.  Random colored bboxes underneath text
chars.  Random roll < 7.5 degs.

Validation accuracy around 95.5% after 1ish epoch (3750 training iterations)

SVHN digits: n_correct = 2699 n_incorrect = 301 frac_correct = 0.8997
Confusion matrix:

43.0000	22.0000	15.0000	3.0000	7.0000	10.0000	7.0000	2.0000	3.0000	3.0000	8.0000	
0.0000	260.0000	4.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	
0.0000	0.0000	248.0000	0.0000	0.0000	23.0000	1.0000	0.0000	1.0000	0.0000	3.0000	
0.0000	1.0000	2.0000	437.0000	2.0000	5.0000	0.0000	1.0000	1.0000	4.0000	9.0000	
0.0000	0.0000	1.0000	2.0000	314.0000	3.0000	25.0000	4.0000	2.0000	6.0000	4.0000	
0.0000	0.0000	10.0000	8.0000	0.0000	210.0000	0.0000	0.0000	2.0000	0.0000	2.0000	
2.0000	1.0000	0.0000	0.0000	1.0000	2.0000	327.0000	11.0000	0.0000	0.0000	0.0000	
0.0000	9.0000	0.0000	0.0000	3.0000	0.0000	0.0000	251.0000	0.0000	8.0000	0.0000	
0.0000	0.0000	29.0000	3.0000	2.0000	4.0000	0.0000	0.0000	194.0000	0.0000	3.0000	
0.0000	3.0000	0.0000	0.0000	1.0000	0.0000	0.0000	3.0000	0.0000	228.0000	2.0000	
0.0000	4.0000	0.0000	0.0000	0.0000	2.0000	0.0000	0.0000	0.0000	2.0000	187.0000	

r = 0 class label = __other__  n_incorrect_classifications = 80
r = 1 class label = 0  n_incorrect_classifications = 4
r = 2 class label = 1  n_incorrect_classifications = 28
r = 3 class label = 2  n_incorrect_classifications = 25
r = 4 class label = 3  n_incorrect_classifications = 47
r = 5 class label = 4  n_incorrect_classifications = 22
r = 6 class label = 5  n_incorrect_classifications = 17
r = 7 class label = 6  n_incorrect_classifications = 20
r = 8 class label = 7  n_incorrect_classifications = 41
r = 9 class label = 8  n_incorrect_classifications = 9
r = 10 class label = 9  n_incorrect_classifications = 8

Validation accuracy around 95.5% after 1ish epoch (6000 training iterations)

SVHN digits: n_correct = 2718 n_incorrect = 282 frac_correct = 0.9060
Confusion matrix:

45.0000	21.0000	9.0000	6.0000	5.0000	10.0000	8.0000	1.0000	6.0000	3.0000	9.0000	
1.0000	259.0000	4.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	
0.0000	1.0000	241.0000	0.0000	1.0000	27.0000	1.0000	0.0000	2.0000	0.0000	3.0000	
1.0000	1.0000	1.0000	444.0000	1.0000	4.0000	0.0000	2.0000	1.0000	0.0000	7.0000	
0.0000	0.0000	1.0000	3.0000	319.0000	4.0000	18.0000	4.0000	3.0000	7.0000	2.0000	
0.0000	0.0000	6.0000	11.0000	0.0000	211.0000	0.0000	0.0000	2.0000	0.0000	2.0000	
1.0000	1.0000	0.0000	2.0000	1.0000	2.0000	328.0000	9.0000	0.0000	0.0000	0.0000	
1.0000	10.0000	0.0000	0.0000	3.0000	0.0000	0.0000	251.0000	0.0000	6.0000	0.0000	
1.0000	0.0000	23.0000	4.0000	2.0000	1.0000	0.0000	0.0000	204.0000	0.0000	0.0000	
0.0000	2.0000	0.0000	0.0000	1.0000	0.0000	0.0000	4.0000	0.0000	229.0000	1.0000	
0.0000	3.0000	0.0000	0.0000	1.0000	2.0000	0.0000	0.0000	0.0000	2.0000	187.0000	

r = 0 class label = __other__  n_incorrect_classifications = 78
r = 1 class label = 0  n_incorrect_classifications = 5
r = 2 class label = 1  n_incorrect_classifications = 35
r = 3 class label = 2  n_incorrect_classifications = 18
r = 4 class label = 3  n_incorrect_classifications = 42
r = 5 class label = 4  n_incorrect_classifications = 21
r = 6 class label = 5  n_incorrect_classifications = 16
r = 7 class label = 6  n_incorrect_classifications = 20
r = 8 class label = 7  n_incorrect_classifications = 31
r = 9 class label = 8  n_incorrect_classifications = 8
r = 10 class label = 9  n_incorrect_classifications = 8

------------------- 
Feb 3 overnight Test N: O(150K) image chips + O(1.2K)
ambiguous partial non-digit chips with pixel widths selected according to
exponential distribution based upon fit to SVHN image chips.  Rotated
digits; gaussian noise; simulated solar shadowing; significant blurring;
variable gravity; variable backgrounds; improved min foreground pixel
fraction.  Small fraction of chars with strokes have foreground_RGB =
background_RGB.  Random RGBs now assigned using fitted exponential and
gaussian distributions for S & V.  Random horizontal/vertical lines.
Random colored bboxes underneath text chars.  Random roll < 7.5 degs.

Validation accuracy around 96% after 1ish epoch (10530 training iterations)

9000 snapshot:
-------------

SVHN digits: n_correct = 2714 n_incorrect = 286 frac_correct = 0.9047
Confusion matrix:

52.0000	21.0000	8.0000	4.0000	7.0000	5.0000	6.0000	3.0000	5.0000	3.0000	9.0000	
1.0000	258.0000	4.0000	0.0000	0.0000	0.0000	0.0000	1.0000	0.0000	0.0000	0.0000	
1.0000	0.0000	252.0000	0.0000	1.0000	19.0000	1.0000	0.0000	0.0000	0.0000	2.0000	
0.0000	1.0000	3.0000	440.0000	0.0000	2.0000	1.0000	2.0000	1.0000	3.0000	9.0000	
0.0000	0.0000	1.0000	1.0000	307.0000	3.0000	23.0000	7.0000	6.0000	10.0000	3.0000	
0.0000	0.0000	6.0000	13.0000	0.0000	207.0000	0.0000	3.0000	1.0000	0.0000	2.0000	
1.0000	1.0000	0.0000	0.0000	0.0000	2.0000	329.0000	11.0000	0.0000	0.0000	0.0000	
0.0000	9.0000	0.0000	0.0000	2.0000	0.0000	0.0000	255.0000	0.0000	5.0000	0.0000	
1.0000	0.0000	28.0000	5.0000	1.0000	2.0000	0.0000	0.0000	198.0000	0.0000	0.0000	
0.0000	3.0000	0.0000	0.0000	2.0000	0.0000	0.0000	3.0000	0.0000	227.0000	2.0000	
0.0000	2.0000	0.0000	0.0000	0.0000	3.0000	0.0000	0.0000	0.0000	1.0000	189.0000	


r = 0 class label = __other__  n_incorrect_classifications = 71
r = 1 class label = 0  n_incorrect_classifications = 6
r = 2 class label = 1  n_incorrect_classifications = 24
r = 3 class label = 2  n_incorrect_classifications = 22
r = 4 class label = 3  n_incorrect_classifications = 54
r = 5 class label = 4  n_incorrect_classifications = 25
r = 6 class label = 5  n_incorrect_classifications = 15
r = 7 class label = 6  n_incorrect_classifications = 16
r = 8 class label = 7  n_incorrect_classifications = 37
r = 9 class label = 8  n_incorrect_classifications = 10
r = 10 class label = 9  n_incorrect_classifications = 6


2.5K MNIST digit test:

MNIST digits: n_correct = 2273 n_incorrect = 227 frac_correct = 0.9092
Confusion matrix:

0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	
0.0000	223.0000	0.0000	0.0000	0.0000	0.0000	0.0000	7.0000	0.0000	4.0000	0.0000	
1.0000	1.0000	266.0000	2.0000	2.0000	0.0000	1.0000	2.0000	1.0000	0.0000	1.0000	
0.0000	0.0000	0.0000	213.0000	14.0000	1.0000	3.0000	0.0000	4.0000	12.0000	0.0000	
0.0000	0.0000	0.0000	1.0000	229.0000	0.0000	7.0000	0.0000	1.0000	1.0000	1.0000	
0.0000	0.0000	2.0000	2.0000	0.0000	252.0000	0.0000	3.0000	2.0000	7.0000	12.0000	
0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	223.0000	4.0000	0.0000	0.0000	0.0000	
0.0000	0.0000	0.0000	11.0000	1.0000	0.0000	3.0000	237.0000	0.0000	2.0000	0.0000	
0.0000	0.0000	3.0000	41.0000	0.0000	2.0000	0.0000	0.0000	227.0000	0.0000	2.0000	
1.0000	0.0000	1.0000	2.0000	5.0000	1.0000	6.0000	5.0000	1.0000	198.0000	2.0000	
0.0000	2.0000	1.0000	7.0000	3.0000	0.0000	1.0000	0.0000	15.0000	10.0000	205.0000	

r = 0 class label = __other__  n_incorrect_classifications = 0
r = 1 class label = 0  n_incorrect_classifications = 11
r = 2 class label = 1  n_incorrect_classifications = 11
r = 3 class label = 2  n_incorrect_classifications = 34
r = 4 class label = 3  n_incorrect_classifications = 11
r = 5 class label = 4  n_incorrect_classifications = 28
r = 6 class label = 5  n_incorrect_classifications = 4
r = 7 class label = 6  n_incorrect_classifications = 17
r = 8 class label = 7  n_incorrect_classifications = 48
r = 9 class label = 8  n_incorrect_classifications = 24
r = 10 class label = 9  n_incorrect_classifications = 39

9750 snapshot: 
-------------

SVHN digits: n_correct = 2714 n_incorrect = 286 frac_correct = 0.9047
Confusion matrix:

54.0000	22.0000	7.0000	4.0000	7.0000	5.0000	5.0000	3.0000	5.0000	3.0000	8.0000	
1.0000	258.0000	4.0000	0.0000	0.0000	0.0000	0.0000	1.0000	0.0000	0.0000	0.0000	
1.0000	0.0000	251.0000	0.0000	1.0000	19.0000	1.0000	0.0000	1.0000	0.0000	2.0000	
0.0000	0.0000	3.0000	442.0000	0.0000	2.0000	1.0000	2.0000	1.0000	3.0000	8.0000	
0.0000	0.0000	2.0000	1.0000	304.0000	3.0000	25.0000	7.0000	6.0000	10.0000	3.0000	
0.0000	0.0000	6.0000	12.0000	0.0000	208.0000	0.0000	3.0000	1.0000	0.0000	2.0000	
1.0000	0.0000	0.0000	0.0000	0.0000	2.0000	328.0000	13.0000	0.0000	0.0000	0.0000	
0.0000	9.0000	0.0000	0.0000	2.0000	0.0000	0.0000	255.0000	0.0000	5.0000	0.0000	
1.0000	0.0000	27.0000	5.0000	1.0000	2.0000	0.0000	0.0000	198.0000	0.0000	1.0000	
0.0000	3.0000	0.0000	0.0000	1.0000	0.0000	0.0000	3.0000	0.0000	228.0000	2.0000	
0.0000	2.0000	0.0000	0.0000	0.0000	4.0000	0.0000	0.0000	0.0000	1.0000	188.0000	

r = 0 class label = __other__  n_incorrect_classifications = 69
r = 1 class label = 0  n_incorrect_classifications = 6
r = 2 class label = 1  n_incorrect_classifications = 25
r = 3 class label = 2  n_incorrect_classifications = 20
r = 4 class label = 3  n_incorrect_classifications = 57
r = 5 class label = 4  n_incorrect_classifications = 24
r = 6 class label = 5  n_incorrect_classifications = 16
r = 7 class label = 6  n_incorrect_classifications = 16
r = 8 class label = 7  n_incorrect_classifications = 37
r = 9 class label = 8  n_incorrect_classifications = 9
r = 10 class label = 9  n_incorrect_classifications = 7

10500 snapshot:
--------------
SVHN digits: n_correct = 2714 n_incorrect = 286 frac_correct = 0.9047
Confusion matrix:

54.0000	22.0000	7.0000	4.0000	7.0000	5.0000	5.0000	3.0000	5.0000	3.0000	8.0000	
1.0000	258.0000	4.0000	0.0000	0.0000	0.0000	0.0000	1.0000	0.0000	0.0000	0.0000	
1.0000	0.0000	251.0000	0.0000	1.0000	19.0000	1.0000	0.0000	1.0000	0.0000	2.0000	
0.0000	0.0000	3.0000	441.0000	0.0000	2.0000	1.0000	2.0000	1.0000	3.0000	9.0000	
0.0000	0.0000	1.0000	1.0000	307.0000	3.0000	22.0000	7.0000	7.0000	10.0000	3.0000	
0.0000	0.0000	6.0000	14.0000	0.0000	207.0000	0.0000	2.0000	1.0000	0.0000	2.0000	
1.0000	1.0000	0.0000	0.0000	0.0000	2.0000	329.0000	11.0000	0.0000	0.0000	0.0000	
0.0000	10.0000	0.0000	0.0000	2.0000	0.0000	0.0000	254.0000	0.0000	5.0000	0.0000	
1.0000	0.0000	28.0000	5.0000	1.0000	2.0000	0.0000	0.0000	198.0000	0.0000	0.0000	
0.0000	3.0000	0.0000	0.0000	2.0000	0.0000	0.0000	3.0000	0.0000	227.0000	2.0000	
0.0000	2.0000	0.0000	0.0000	0.0000	4.0000	0.0000	0.0000	0.0000	1.0000	188.0000	

r = 0 class label = __other__  n_incorrect_classifications = 69
r = 1 class label = 0  n_incorrect_classifications = 6
r = 2 class label = 1  n_incorrect_classifications = 25
r = 3 class label = 2  n_incorrect_classifications = 21
r = 4 class label = 3  n_incorrect_classifications = 54
r = 5 class label = 4  n_incorrect_classifications = 25
r = 6 class label = 5  n_incorrect_classifications = 15
r = 7 class label = 6  n_incorrect_classifications = 17
r = 8 class label = 7  n_incorrect_classifications = 37
r = 9 class label = 8  n_incorrect_classifications = 10
r = 10 class label = 9  n_incorrect_classifications = 7

------------------- 

First Feb 4 (morning) Test O: O(137K) image chips containing O(11K) background
samples + O(1.4K) ambiguous partial non-digit chips with pixel widths
selected according to exponential distribution based upon fit to SVHN image
chips.  Rotated digits; gaussian noise; simulated solar shadowing;
significant blurring; variable gravity; variable backgrounds; improved min
foreground pixel fraction.  Small fraction of chars with strokes have
foreground_RGB = background_RGB.  Random RGBs now assigned using fitted
exponential and gaussian distributions for S & V.  Random
horizontal/vertical lines.  Random colored bboxes underneath text chars.
Random roll < 7.5 degs.

Validation accuracy around 95.7 after 1ish epoch (5800 training iterations)

5500 snapshot:
-------------

SVHN digits: n_correct = 2729 n_incorrect = 271 frac_correct = 0.9097
Confusion matrix:

84.0000	15.0000	3.0000	3.0000	3.0000	3.0000	2.0000	3.0000	1.0000	2.0000	4.0000	
3.0000	257.0000	4.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	
8.0000	0.0000	250.0000	0.0000	1.0000	16.0000	1.0000	0.0000	0.0000	0.0000	0.0000	
1.0000	0.0000	2.0000	445.0000	1.0000	2.0000	0.0000	1.0000	1.0000	3.0000	6.0000	
4.0000	0.0000	1.0000	2.0000	303.0000	5.0000	27.0000	5.0000	5.0000	6.0000	3.0000	
2.0000	1.0000	8.0000	8.0000	0.0000	211.0000	0.0000	0.0000	1.0000	0.0000	1.0000	
4.0000	0.0000	0.0000	0.0000	1.0000	2.0000	324.0000	12.0000	0.0000	0.0000	1.0000	
3.0000	7.0000	0.0000	0.0000	3.0000	0.0000	1.0000	244.0000	0.0000	12.0000	1.0000	
3.0000	0.0000	22.0000	4.0000	1.0000	5.0000	1.0000	0.0000	196.0000	0.0000	3.0000	
0.0000	3.0000	0.0000	1.0000	2.0000	0.0000	0.0000	2.0000	0.0000	227.0000	2.0000	
0.0000	2.0000	1.0000	0.0000	0.0000	2.0000	1.0000	0.0000	0.0000	1.0000	188.0000	

r = 0 class label = __other__  n_incorrect_classifications = 39
r = 1 class label = 0  n_incorrect_classifications = 7
r = 2 class label = 1  n_incorrect_classifications = 26
r = 3 class label = 2  n_incorrect_classifications = 17
r = 4 class label = 3  n_incorrect_classifications = 58
r = 5 class label = 4  n_incorrect_classifications = 21
r = 6 class label = 5  n_incorrect_classifications = 20
r = 7 class label = 6  n_incorrect_classifications = 27
r = 8 class label = 7  n_incorrect_classifications = 39
r = 9 class label = 8  n_incorrect_classifications = 10
r = 10 class label = 9  n_incorrect_classifications = 7

MNIST digits:  n_correct = 2294 n_incorrect = 206 frac_correct = 0.9176
Confusion matrix:

0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	
0.0000	218.0000	0.0000	0.0000	0.0000	0.0000	0.0000	10.0000	0.0000	5.0000	1.0000	
0.0000	0.0000	264.0000	2.0000	0.0000	3.0000	2.0000	1.0000	3.0000	2.0000	0.0000	
0.0000	0.0000	1.0000	202.0000	25.0000	2.0000	2.0000	1.0000	5.0000	9.0000	0.0000	
0.0000	0.0000	0.0000	0.0000	232.0000	0.0000	5.0000	0.0000	1.0000	0.0000	2.0000	
0.0000	0.0000	2.0000	0.0000	0.0000	261.0000	0.0000	2.0000	0.0000	5.0000	10.0000	
0.0000	0.0000	0.0000	0.0000	1.0000	0.0000	221.0000	5.0000	0.0000	0.0000	0.0000	
0.0000	0.0000	1.0000	12.0000	0.0000	0.0000	1.0000	234.0000	0.0000	6.0000	0.0000	
0.0000	0.0000	3.0000	11.0000	2.0000	7.0000	0.0000	0.0000	249.0000	0.0000	3.0000	
0.0000	0.0000	1.0000	2.0000	3.0000	1.0000	4.0000	3.0000	2.0000	204.0000	2.0000	
0.0000	1.0000	1.0000	5.0000	2.0000	0.0000	1.0000	0.0000	17.0000	8.0000	209.0000	

r = 0 class label = __other__  n_incorrect_classifications = 0
r = 1 class label = 0  n_incorrect_classifications = 16
r = 2 class label = 1  n_incorrect_classifications = 13
r = 3 class label = 2  n_incorrect_classifications = 45
r = 4 class label = 3  n_incorrect_classifications = 8
r = 5 class label = 4  n_incorrect_classifications = 19
r = 6 class label = 5  n_incorrect_classifications = 6
r = 7 class label = 6  n_incorrect_classifications = 20
r = 8 class label = 7  n_incorrect_classifications = 26
r = 9 class label = 8  n_incorrect_classifications = 18
r = 10 class label = 9  n_incorrect_classifications = 35

......
Second Feb 4 (evening) Test O: O(137K) image chips containing O(11K) background
samples + O(3.7K) ambiguous partial non-digit chips with pixel widths
selected according to exponential distribution based upon fit to SVHN image
chips.  Rotated digits; gaussian noise; simulated solar shadowing;
significant blurring; variable gravity; variable backgrounds; improved min
foreground pixel fraction.  Small fraction of chars with strokes have
foreground_RGB = background_RGB.  Random RGBs now assigned using fitted
exponential and gaussian distributions for S & V.  Random
horizontal/vertical lines.  Random colored bboxes underneath text chars.
Random roll < 7.5 degs.

Validation accuracy around 95.7 after 1ish epoch (11680 training iterations)

11500 snapshot:
--------------

SVHN digits: n_correct = 2724 n_incorrect = 276 frac_correct = 0.9080
Confusion matrix:

91.0000	10.0000	3.0000	1.0000	1.0000	3.0000	7.0000	2.0000	1.0000	2.0000	2.0000	
2.0000	258.0000	4.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	
4.0000	0.0000	253.0000	0.0000	1.0000	13.0000	2.0000	0.0000	0.0000	0.0000	3.0000	
3.0000	0.0000	2.0000	442.0000	2.0000	4.0000	0.0000	1.0000	0.0000	3.0000	5.0000	
3.0000	1.0000	1.0000	2.0000	295.0000	3.0000	29.0000	9.0000	4.0000	11.0000	3.0000	
3.0000	0.0000	4.0000	11.0000	0.0000	209.0000	0.0000	4.0000	1.0000	0.0000	0.0000	
3.0000	2.0000	0.0000	1.0000	0.0000	2.0000	326.0000	10.0000	0.0000	0.0000	0.0000	
4.0000	5.0000	0.0000	0.0000	2.0000	0.0000	2.0000	249.0000	0.0000	9.0000	0.0000	
4.0000	0.0000	34.0000	8.0000	1.0000	2.0000	1.0000	0.0000	182.0000	0.0000	3.0000	
0.0000	3.0000	0.0000	0.0000	1.0000	0.0000	0.0000	2.0000	0.0000	229.0000	2.0000	
0.0000	1.0000	0.0000	0.0000	0.0000	2.0000	0.0000	0.0000	0.0000	2.0000	190.0000	

r = 0 class label = __other__  n_incorrect_classifications = 32
r = 1 class label = 0  n_incorrect_classifications = 6
r = 2 class label = 1  n_incorrect_classifications = 23
r = 3 class label = 2  n_incorrect_classifications = 20
r = 4 class label = 3  n_incorrect_classifications = 66
r = 5 class label = 4  n_incorrect_classifications = 23
r = 6 class label = 5  n_incorrect_classifications = 18
r = 7 class label = 6  n_incorrect_classifications = 22
r = 8 class label = 7  n_incorrect_classifications = 53
r = 9 class label = 8  n_incorrect_classifications = 8
r = 10 class label = 9  n_incorrect_classifications = 5

MNIST digits:  n_correct = 2269 n_incorrect = 231 frac_correct = 0.9076
Confusion matrix:

0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	
0.0000	219.0000	0.0000	0.0000	0.0000	0.0000	0.0000	12.0000	0.0000	1.0000	2.0000	
5.0000	2.0000	263.0000	2.0000	1.0000	2.0000	1.0000	1.0000	0.0000	0.0000	0.0000	
0.0000	0.0000	1.0000	199.0000	21.0000	2.0000	4.0000	0.0000	8.0000	12.0000	0.0000	
0.0000	0.0000	0.0000	1.0000	225.0000	0.0000	12.0000	0.0000	1.0000	0.0000	1.0000	
0.0000	0.0000	3.0000	0.0000	0.0000	263.0000	0.0000	1.0000	3.0000	3.0000	7.0000	
0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	222.0000	4.0000	0.0000	1.0000	0.0000	
0.0000	0.0000	0.0000	5.0000	2.0000	0.0000	6.0000	240.0000	0.0000	1.0000	0.0000	
1.0000	0.0000	9.0000	11.0000	0.0000	5.0000	0.0000	0.0000	246.0000	0.0000	3.0000	
1.0000	0.0000	1.0000	3.0000	7.0000	4.0000	10.0000	2.0000	5.0000	185.0000	4.0000	
0.0000	1.0000	3.0000	3.0000	1.0000	1.0000	1.0000	0.0000	21.0000	6.0000	207.0000	

r = 0 class label = __other__  n_incorrect_classifications = 0
r = 1 class label = 0  n_incorrect_classifications = 15
r = 2 class label = 1  n_incorrect_classifications = 14
r = 3 class label = 2  n_incorrect_classifications = 48
r = 4 class label = 3  n_incorrect_classifications = 15
r = 5 class label = 4  n_incorrect_classifications = 17
r = 6 class label = 5  n_incorrect_classifications = 5
r = 7 class label = 6  n_incorrect_classifications = 14
r = 8 class label = 7  n_incorrect_classifications = 29
r = 9 class label = 8  n_incorrect_classifications = 37
r = 10 class label = 9  n_incorrect_classifications = 37

......
Third Feb 4 (am of Feb 5) Test O: O(137K) image chips containing O(11K) background
samples + O(3.7K) ambiguous partial non-digit chips with pixel widths
selected according to exponential distribution based upon fit to SVHN image
chips.  Rotated digits; gaussian noise; simulated solar shadowing;
significant blurring; variable gravity; variable backgrounds; improved min
foreground pixel fraction.  Small fraction of chars with strokes have
foreground_RGB = background_RGB.  Random RGBs now assigned using fitted
exponential and gaussian distributions for S & V.  Random
horizontal/vertical lines.  Random colored bboxes underneath text chars.
Random roll < 7.5 degs.

Validation accuracy around 95.7% after 1ish epoch (6700 training iterations)

6500 snapshot:
--------------

SVHN digits: n_correct = 2745 n_incorrect = 255 frac_correct = 0.9150
Confusion matrix:

92.0000	7.0000	3.0000	0.0000	3.0000	1.0000	5.0000	1.0000	1.0000	4.0000	6.0000	
4.0000	255.0000	4.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	1.0000	
4.0000	0.0000	240.0000	0.0000	1.0000	29.0000	1.0000	0.0000	0.0000	0.0000	1.0000	
1.0000	0.0000	2.0000	437.0000	2.0000	2.0000	1.0000	1.0000	1.0000	1.0000	14.0000	
1.0000	0.0000	1.0000	3.0000	320.0000	2.0000	15.0000	6.0000	5.0000	6.0000	2.0000	
0.0000	0.0000	7.0000	13.0000	0.0000	211.0000	0.0000	0.0000	1.0000	0.0000	0.0000	
2.0000	1.0000	0.0000	1.0000	1.0000	2.0000	328.0000	8.0000	0.0000	0.0000	1.0000	
3.0000	5.0000	0.0000	0.0000	4.0000	0.0000	1.0000	251.0000	0.0000	7.0000	0.0000	
4.0000	0.0000	23.0000	5.0000	1.0000	3.0000	0.0000	0.0000	195.0000	0.0000	4.0000	
0.0000	3.0000	0.0000	0.0000	3.0000	0.0000	0.0000	3.0000	0.0000	227.0000	1.0000	
0.0000	2.0000	0.0000	0.0000	0.0000	3.0000	0.0000	0.0000	0.0000	1.0000	189.0000	

r = 0 class label = __other__  n_incorrect_classifications = 31
r = 1 class label = 0  n_incorrect_classifications = 9
r = 2 class label = 1  n_incorrect_classifications = 36
r = 3 class label = 2  n_incorrect_classifications = 25
r = 4 class label = 3  n_incorrect_classifications = 41
r = 5 class label = 4  n_incorrect_classifications = 21
r = 6 class label = 5  n_incorrect_classifications = 16
r = 7 class label = 6  n_incorrect_classifications = 20
r = 8 class label = 7  n_incorrect_classifications = 40
r = 9 class label = 8  n_incorrect_classifications = 10
r = 10 class label = 9  n_incorrect_classifications = 6

MNIST digits: n_correct = 2259 n_incorrect = 241 frac_correct = 0.9036
Confusion matrix:

0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	
0.0000	218.0000	0.0000	0.0000	0.0000	0.0000	0.0000	9.0000	0.0000	5.0000	2.0000	
17.0000	0.0000	247.0000	2.0000	1.0000	3.0000	3.0000	0.0000	2.0000	1.0000	1.0000	
0.0000	0.0000	1.0000	210.0000	17.0000	1.0000	2.0000	2.0000	6.0000	8.0000	0.0000	
0.0000	0.0000	0.0000	0.0000	228.0000	0.0000	9.0000	0.0000	1.0000	1.0000	1.0000	
0.0000	0.0000	2.0000	1.0000	0.0000	267.0000	1.0000	0.0000	0.0000	4.0000	5.0000	
0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	222.0000	5.0000	0.0000	0.0000	0.0000	
0.0000	0.0000	0.0000	5.0000	2.0000	0.0000	3.0000	243.0000	0.0000	1.0000	0.0000	
1.0000	0.0000	5.0000	33.0000	1.0000	6.0000	0.0000	0.0000	227.0000	0.0000	2.0000	
1.0000	0.0000	1.0000	2.0000	2.0000	1.0000	2.0000	6.0000	1.0000	203.0000	3.0000	
0.0000	0.0000	1.0000	25.0000	5.0000	1.0000	1.0000	0.0000	10.0000	7.0000	194.0000	

r = 0 class label = __other__  n_incorrect_classifications = 0
r = 1 class label = 0  n_incorrect_classifications = 16
r = 2 class label = 1  n_incorrect_classifications = 30
r = 3 class label = 2  n_incorrect_classifications = 37
r = 4 class label = 3  n_incorrect_classifications = 12
r = 5 class label = 4  n_incorrect_classifications = 13
r = 6 class label = 5  n_incorrect_classifications = 5
r = 7 class label = 6  n_incorrect_classifications = 11
r = 8 class label = 7  n_incorrect_classifications = 48
r = 9 class label = 8  n_incorrect_classifications = 19
r = 10 class label = 9  n_incorrect_classifications = 50

......
Fourth Feb 4 (evening of Feb 5) Test O: O(137K) image chips containing O(11K) background
samples + O(3.7K) ambiguous partial non-digit chips with pixel widths
selected according to exponential distribution based upon fit to SVHN image
chips.  Rotated digits; gaussian noise; simulated solar shadowing;
significant blurring; variable gravity; variable backgrounds; improved min
foreground pixel fraction.  Small fraction of chars with strokes have
foreground_RGB = background_RGB.  Random RGBs now assigned using fitted
exponential and gaussian distributions for S & V.  Random
horizontal/vertical lines.  Random colored bboxes underneath text chars.
Random roll < 7.5 degs.

Validation accuracy around 95.7% after 1ish epoch (30K+ training iterations)

30500 snapshot:
--------------

SVHN digits: n_correct = 2736 n_incorrect = 264 frac_correct = 0.9120
Confusion matrix:

90.0000	8.0000	4.0000	2.0000	3.0000	1.0000	5.0000	1.0000	1.0000	3.0000	5.0000	
5.0000	254.0000	4.0000	0.0000	0.0000	0.0000	0.0000	1.0000	0.0000	0.0000	0.0000	
1.0000	0.0000	250.0000	0.0000	0.0000	20.0000	1.0000	0.0000	2.0000	0.0000	2.0000	
3.0000	2.0000	4.0000	441.0000	1.0000	1.0000	0.0000	1.0000	0.0000	3.0000	6.0000	
3.0000	1.0000	1.0000	3.0000	311.0000	3.0000	18.0000	5.0000	7.0000	8.0000	1.0000	
2.0000	0.0000	6.0000	13.0000	0.0000	208.0000	0.0000	2.0000	1.0000	0.0000	0.0000	
2.0000	1.0000	0.0000	2.0000	1.0000	2.0000	326.0000	10.0000	0.0000	0.0000	0.0000	
3.0000	8.0000	0.0000	0.0000	3.0000	0.0000	1.0000	248.0000	0.0000	8.0000	0.0000	
3.0000	0.0000	29.0000	4.0000	1.0000	5.0000	0.0000	0.0000	192.0000	0.0000	1.0000	
0.0000	1.0000	0.0000	0.0000	2.0000	0.0000	0.0000	4.0000	0.0000	229.0000	1.0000	
0.0000	2.0000	0.0000	0.0000	0.0000	5.0000	0.0000	0.0000	0.0000	1.0000	187.0000	

r = 0 class label = __other__  n_incorrect_classifications = 33
r = 1 class label = 0  n_incorrect_classifications = 10
r = 2 class label = 1  n_incorrect_classifications = 26
r = 3 class label = 2  n_incorrect_classifications = 21
r = 4 class label = 3  n_incorrect_classifications = 50
r = 5 class label = 4  n_incorrect_classifications = 24
r = 6 class label = 5  n_incorrect_classifications = 18
r = 7 class label = 6  n_incorrect_classifications = 23
r = 8 class label = 7  n_incorrect_classifications = 43
r = 9 class label = 8  n_incorrect_classifications = 8
r = 10 class label = 9  n_incorrect_classifications = 8

MNIST digits: n_correct = 2277 n_incorrect = 223 frac_correct = 0.9108
Confusion matrix:

0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	
0.0000	214.0000	0.0000	1.0000	0.0000	0.0000	0.0000	10.0000	0.0000	4.0000	5.0000	
4.0000	1.0000	263.0000	1.0000	1.0000	3.0000	1.0000	2.0000	1.0000	0.0000	0.0000	
1.0000	0.0000	1.0000	216.0000	12.0000	1.0000	3.0000	0.0000	3.0000	10.0000	0.0000	
0.0000	0.0000	0.0000	2.0000	222.0000	0.0000	13.0000	0.0000	1.0000	1.0000	1.0000	
0.0000	0.0000	1.0000	0.0000	0.0000	255.0000	0.0000	0.0000	2.0000	9.0000	13.0000	
0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	223.0000	4.0000	0.0000	0.0000	0.0000	
0.0000	0.0000	0.0000	7.0000	2.0000	0.0000	3.0000	241.0000	0.0000	1.0000	0.0000	
1.0000	0.0000	4.0000	30.0000	0.0000	3.0000	0.0000	0.0000	235.0000	0.0000	2.0000	
1.0000	0.0000	1.0000	3.0000	3.0000	1.0000	6.0000	4.0000	0.0000	202.0000	1.0000	
0.0000	1.0000	2.0000	10.0000	4.0000	0.0000	1.0000	0.0000	14.0000	6.0000	206.0000	

r = 0 class label = __other__  n_incorrect_classifications = 0
r = 1 class label = 0  n_incorrect_classifications = 20
r = 2 class label = 1  n_incorrect_classifications = 14
r = 3 class label = 2  n_incorrect_classifications = 31
r = 4 class label = 3  n_incorrect_classifications = 18
r = 5 class label = 4  n_incorrect_classifications = 25
r = 6 class label = 5  n_incorrect_classifications = 4
r = 7 class label = 6  n_incorrect_classifications = 13
r = 8 class label = 7  n_incorrect_classifications = 40
r = 9 class label = 8  n_incorrect_classifications = 20
r = 10 class label = 9  n_incorrect_classifications = 38

......
Fifth Feb 4 (morning of Feb 7) Test O: O(137K) image chips containing O(11K) background
samples + O(3.7K) ambiguous partial non-digit chips with pixel widths
selected according to exponential distribution based upon fit to SVHN image
chips.  Rotated digits; gaussian noise; simulated solar shadowing;
significant blurring; variable gravity; variable backgrounds; improved min
foreground pixel fraction.  Small fraction of chars with strokes have
foreground_RGB = background_RGB.  Random RGBs now assigned using fitted
exponential and gaussian distributions for S & V.  Random
horizontal/vertical lines.  Random colored bboxes underneath text chars.
Random roll < 7.5 degs.

Validation accuracy around 96.0% after 2ish epoch (18K+ training iterations)

18500 snapshot:
--------------

SVHN digits: n_correct = 2724 n_incorrect = 276 frac_correct = 0.9080
Confusion matrix:

88.0000	13.0000	3.0000	1.0000	5.0000	2.0000	0.0000	4.0000	1.0000	0.0000	6.0000	
2.0000	259.0000	3.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	
4.0000	0.0000	255.0000	0.0000	1.0000	13.0000	1.0000	0.0000	0.0000	0.0000	2.0000	
2.0000	0.0000	4.0000	440.0000	0.0000	2.0000	1.0000	1.0000	1.0000	1.0000	10.0000	
4.0000	0.0000	3.0000	2.0000	301.0000	3.0000	27.0000	7.0000	6.0000	7.0000	1.0000	
6.0000	0.0000	11.0000	10.0000	0.0000	202.0000	0.0000	2.0000	1.0000	0.0000	0.0000	
4.0000	1.0000	0.0000	1.0000	1.0000	2.0000	322.0000	13.0000	0.0000	0.0000	0.0000	
3.0000	5.0000	0.0000	0.0000	2.0000	0.0000	0.0000	256.0000	0.0000	5.0000	0.0000	
5.0000	0.0000	32.0000	8.0000	1.0000	1.0000	0.0000	0.0000	186.0000	0.0000	2.0000	
0.0000	3.0000	0.0000	1.0000	2.0000	0.0000	0.0000	4.0000	0.0000	225.0000	2.0000	
0.0000	0.0000	1.0000	0.0000	0.0000	2.0000	0.0000	0.0000	0.0000	2.0000	190.0000	

r = 0 class label = __other__  n_incorrect_classifications = 35
r = 1 class label = 0  n_incorrect_classifications = 5
r = 2 class label = 1  n_incorrect_classifications = 21
r = 3 class label = 2  n_incorrect_classifications = 22
r = 4 class label = 3  n_incorrect_classifications = 60
r = 5 class label = 4  n_incorrect_classifications = 30
r = 6 class label = 5  n_incorrect_classifications = 22
r = 7 class label = 6  n_incorrect_classifications = 15
r = 8 class label = 7  n_incorrect_classifications = 49
r = 9 class label = 8  n_incorrect_classifications = 12
r = 10 class label = 9  n_incorrect_classifications = 5

MNIST digits:  n_correct = 2249 n_incorrect = 251 frac_correct = 0.8996
Confusion matrix:

0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	
0.0000	212.0000	0.0000	0.0000	0.0000	0.0000	0.0000	18.0000	0.0000	3.0000	1.0000	
3.0000	0.0000	265.0000	1.0000	1.0000	3.0000	1.0000	1.0000	2.0000	0.0000	0.0000	
1.0000	1.0000	1.0000	215.0000	8.0000	2.0000	3.0000	0.0000	5.0000	11.0000	0.0000	
0.0000	0.0000	0.0000	2.0000	220.0000	0.0000	15.0000	0.0000	1.0000	1.0000	1.0000	
1.0000	0.0000	2.0000	0.0000	0.0000	252.0000	0.0000	2.0000	2.0000	7.0000	14.0000	
0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	219.0000	7.0000	1.0000	0.0000	0.0000	
0.0000	0.0000	1.0000	6.0000	0.0000	0.0000	0.0000	246.0000	0.0000	1.0000	0.0000	
1.0000	0.0000	7.0000	30.0000	1.0000	7.0000	0.0000	0.0000	228.0000	0.0000	1.0000	
1.0000	0.0000	1.0000	2.0000	1.0000	2.0000	0.0000	9.0000	1.0000	201.0000	4.0000	
0.0000	1.0000	1.0000	17.0000	0.0000	1.0000	1.0000	0.0000	23.0000	9.0000	191.0000	


r = 0 class label = __other__  n_incorrect_classifications = 0
r = 1 class label = 0  n_incorrect_classifications = 22
r = 2 class label = 1  n_incorrect_classifications = 12
r = 3 class label = 2  n_incorrect_classifications = 32
r = 4 class label = 3  n_incorrect_classifications = 20
r = 5 class label = 4  n_incorrect_classifications = 28
r = 6 class label = 5  n_incorrect_classifications = 8
r = 7 class label = 6  n_incorrect_classifications = 8
r = 8 class label = 7  n_incorrect_classifications = 47
r = 9 class label = 8  n_incorrect_classifications = 21
r = 10 class label = 9  n_incorrect_classifications = 53

---------------------------------------------------------------------------------
Simple voting scheme for 5 model ensemble:

n_correct = 2746 n_incorrect = 254 frac_correct = 0.915333
Confusion matrix:

91	11	3	1	2	1	5	1	1	3	4	
2	259	3	0	0	0	0	0	0	0	0	
4	0	251	0	1	18	1	0	0	0	1	
2	1	4	439	0	2	0	1	1	2	10	
3	1	1	2	312	3	18	6	6	8	1	
2	0	8	12	0	207	0	2	1	0	0	
2	1	0	1	1	2	328	9	0	0	0	
3	7	0	0	3	0	0	252	0	6	0	
5	0	28	5	1	4	0	0	190	0	2	
0	2	0	0	3	0	0	3	0	228	1	
0	2	0	0	0	3	0	0	0	1	189	

r = 0 class label = __other__  n_incorrect_classifications = 32
r = 1 class label = 0  n_incorrect_classifications = 5
r = 2 class label = 1  n_incorrect_classifications = 25
r = 3 class label = 2  n_incorrect_classifications = 23
r = 4 class label = 3  n_incorrect_classifications = 49
r = 5 class label = 4  n_incorrect_classifications = 25
r = 6 class label = 5  n_incorrect_classifications = 16
r = 7 class label = 6  n_incorrect_classifications = 19
r = 8 class label = 7  n_incorrect_classifications = 45
r = 9 class label = 8  n_incorrect_classifications = 9
r = 10 class label = 9  n_incorrect_classifications = 6

-------------------
Votes = sum of class scores (geometric mean) scheme for 5 model ensemble is
a total disaster!

Votes = sum of class scores (arithmetic mean) scheme for 5 model ensemble:

SVHN digits: n_correct = 2745 n_incorrect = 255 frac_correct = 0.915
Confusion matrix:

90	11	3	1	2	2	5	1	1	3	4	
2	259	3	0	0	0	0	0	0	0	0	
3	0	252	0	1	18	1	0	0	0	1	
2	0	3	439	0	1	1	1	1	2	12	
3	1	1	2	311	3	18	6	7	8	1	
2	0	8	12	0	207	0	2	1	0	0	
2	1	0	1	1	2	327	10	0	0	0	
3	7	0	0	2	0	0	253	0	6	0	
5	0	28	5	1	4	0	0	190	0	2	
0	2	0	0	3	0	0	3	0	228	1	
0	2	0	0	0	3	0	0	0	1	189	

r = 0 class label = __other__  n_incorrect_classifications = 33
r = 1 class label = 0  n_incorrect_classifications = 5
r = 2 class label = 1  n_incorrect_classifications = 24
r = 3 class label = 2  n_incorrect_classifications = 23
r = 4 class label = 3  n_incorrect_classifications = 50
r = 5 class label = 4  n_incorrect_classifications = 25
r = 6 class label = 5  n_incorrect_classifications = 17
r = 7 class label = 6  n_incorrect_classifications = 18
r = 8 class label = 7  n_incorrect_classifications = 45
r = 9 class label = 8  n_incorrect_classifications = 9
r = 10 class label = 9  n_incorrect_classifications = 6


Vote for class with maximum score across all 5 classifiers' predictions:

SVHN digits: n_correct = 2752 n_incorrect = 248 frac_correct = 0.917333
Confusion matrix:

89	10	3	1	3	2	4	2	1	3	5	
3	258	3	0	0	0	0	0	0	0	0	
4	0	254	0	1	15	1	0	0	0	1	
2	0	3	439	0	1	1	1	1	2	12	
3	0	2	2	310	3	20	6	5	8	2	
2	0	7	12	0	209	0	1	1	0	0	
3	1	0	0	1	2	329	8	0	0	0	
3	6	0	0	2	0	0	254	0	6	0	
5	0	26	4	1	4	0	0	193	0	2	
0	2	0	0	3	0	0	4	0	227	1	
0	2	0	0	0	2	0	0	0	1	190	

r = 0 class label = __other__  n_incorrect_classifications = 34
r = 1 class label = 0  n_incorrect_classifications = 6
r = 2 class label = 1  n_incorrect_classifications = 22
r = 3 class label = 2  n_incorrect_classifications = 23
r = 4 class label = 3  n_incorrect_classifications = 51
r = 5 class label = 4  n_incorrect_classifications = 23
r = 6 class label = 5  n_incorrect_classifications = 15
r = 7 class label = 6  n_incorrect_classifications = 17
r = 8 class label = 7  n_incorrect_classifications = 42
r = 9 class label = 8  n_incorrect_classifications = 10
r = 10 class label = 9  n_incorrect_classifications = 5

MNIST digits: n_correct = 2279 n_incorrect = 221 frac_correct = 0.9116
Confusion matrix:

0	0	0	0	0	0	0	0	0	0	0	
0	216	0	0	0	0	0	13	0	4	1	
5	0	264	0	1	3	0	1	2	0	1	
0	0	1	218	9	1	3	0	5	10	0	
0	0	0	1	225	0	11	0	1	1	1	
0	0	2	1	0	261	0	0	0	5	11	
0	0	0	0	0	0	221	6	0	0	0	
0	0	0	6	1	0	1	245	0	1	0	
1	0	5	30	0	6	0	0	232	0	1	
1	0	1	2	2	1	3	7	0	201	4	
0	0	2	14	2	1	1	0	21	7	196	

r = 0 class label = __other__  n_incorrect_classifications = 0
r = 1 class label = 0  n_incorrect_classifications = 18
r = 2 class label = 1  n_incorrect_classifications = 13
r = 3 class label = 2  n_incorrect_classifications = 29
r = 4 class label = 3  n_incorrect_classifications = 15
r = 5 class label = 4  n_incorrect_classifications = 19
r = 6 class label = 5  n_incorrect_classifications = 6
r = 7 class label = 6  n_incorrect_classifications = 9
r = 8 class label = 7  n_incorrect_classifications = 43
r = 9 class label = 8  n_incorrect_classifications = 21
r = 10 class label = 9  n_incorrect_classifications = 48

---------------------------------------------------------------------------------

Feb 12 Handwriting Test:  Incorporation of synthetic handwriting
hard negatives into new training set.  

Testing results on 50K synthetic handwriting chips:

n_correct = 48942 n_incorrect = 1057 frac_correct = 0.9789
Confusion matrix:

6164.0000	3.0000	9.0000	2.0000	2.0000	4.0000	2.0000	4.0000	8.0000	1.0000	3.0000	
6.0000	3861.0000	1.0000	5.0000	6.0000	4.0000	6.0000	32.0000	1.0000	18.0000	17.0000	
12.0000	4.0000	3985.0000	5.0000	0.0000	18.0000	3.0000	6.0000	45.0000	4.0000	19.0000	
5.0000	5.0000	8.0000	4353.0000	8.0000	3.0000	5.0000	5.0000	22.0000	6.0000	6.0000	
6.0000	5.0000	1.0000	11.0000	4312.0000	4.0000	59.0000	3.0000	10.0000	33.0000	6.0000	
5.0000	4.0000	13.0000	11.0000	1.0000	4275.0000	1.0000	7.0000	13.0000	1.0000	19.0000	
6.0000	6.0000	9.0000	4.0000	38.0000	3.0000	4393.0000	21.0000	4.0000	11.0000	10.0000	
1.0000	27.0000	3.0000	5.0000	0.0000	7.0000	12.0000	4433.0000	1.0000	12.0000	4.0000	
5.0000	2.0000	50.0000	9.0000	4.0000	14.0000	6.0000	4.0000	4221.0000	1.0000	26.0000	
5.0000	23.0000	8.0000	7.0000	23.0000	1.0000	12.0000	17.0000	4.0000	4538.0000	10.0000	
7.0000	10.0000	15.0000	3.0000	4.0000	34.0000	4.0000	2.0000	13.0000	14.0000	4407.0000	

Testing results on 3K MNIST chips:

n_correct = 2206 n_incorrect = 294 frac_correct = 0.8824
Confusion matrix:

0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	
0.0000	216.0000	0.0000	0.0000	0.0000	1.0000	0.0000	12.0000	0.0000	4.0000	1.0000	
0.0000	0.0000	266.0000	3.0000	0.0000	1.0000	1.0000	3.0000	3.0000	0.0000	0.0000	
0.0000	0.0000	0.0000	181.0000	41.0000	2.0000	7.0000	0.0000	13.0000	3.0000	0.0000	
0.0000	0.0000	0.0000	0.0000	230.0000	0.0000	5.0000	0.0000	2.0000	2.0000	1.0000	
0.0000	0.0000	4.0000	0.0000	0.0000	267.0000	1.0000	1.0000	1.0000	2.0000	4.0000	
0.0000	0.0000	0.0000	0.0000	4.0000	0.0000	217.0000	6.0000	0.0000	0.0000	0.0000	
0.0000	0.0000	2.0000	12.0000	0.0000	1.0000	1.0000	238.0000	0.0000	0.0000	0.0000	
0.0000	0.0000	16.0000	21.0000	1.0000	6.0000	1.0000	0.0000	227.0000	0.0000	3.0000	
0.0000	0.0000	1.0000	4.0000	8.0000	6.0000	23.0000	2.0000	4.0000	172.0000	2.0000	
0.0000	1.0000	4.0000	5.0000	5.0000	3.0000	1.0000	0.0000	19.0000	14.0000	192.0000	

r = 0 class label = __other__  n_incorrect_classifications = 0
r = 1 class label = 0  n_incorrect_classifications = 18
r = 2 class label = 1  n_incorrect_classifications = 11
r = 3 class label = 2  n_incorrect_classifications = 66
r = 4 class label = 3  n_incorrect_classifications = 10
r = 5 class label = 4  n_incorrect_classifications = 13
r = 6 class label = 5  n_incorrect_classifications = 10
r = 7 class label = 6  n_incorrect_classifications = 16
r = 8 class label = 7  n_incorrect_classifications = 48
r = 9 class label = 8  n_incorrect_classifications = 50
r = 10 class label = 9  n_incorrect_classifications = 52


Conclusions drawn early on Sun, Feb 14:  
 1.  Incorporating hard nevatives into just handwriting synthetic training set
     may lead to a slight improvement in synthetic training error
 2.  But testing of MNIST with just handwriting synthetic chips leads to
     significantly *WORSE* results than with all-fonts synthetic chips!


......  

Feb 14 (morning of Feb 14) Test P on Titan 3: O(137K) image chips containing
O(11K) background samples + O(3.7K) ambiguous partial non-digit chips +
O(1.5K) hard synthetic handwriting negatives.  Pixel widths selected
according to exponential distribution based upon fit to SVHN image chips;
handwriting font chips have smaller pixel widths.  Rotated digits (less
rotation for handwriting fonts); gaussian noise; simulated solar shadowing
(less shadowing for handwriting fonts); significant blurring (less blurring
for handwriting fonts); variable gravity; variable backgrounds (fewer
variable backgrounds for handwriting fonts); improved min foreground pixel
fraction.  Small fraction of chars with strokes have foreground_RGB =
background_RGB.  Random RGBs now assigned using fitted exponential and
gaussian distributions for S & V (lower S values for handwriting fonts).
Random horizontal/vertical lines.  Random colored bboxes underneath text
chars (fewer bboxes for handwriting fonts).  Random roll < 7.5 degs (less
3D rotation for handwriting fonts).

Validation accuracy around 96% after 2ish epochs.

SVHN digits: n_correct = 2717 n_incorrect = 283 frac_correct = 0.9057
Confusion matrix:

87.0000	8.0000	4.0000	1.0000	0.0000	4.0000	7.0000	3.0000	1.0000	5.0000	3.0000	
2.0000	259.0000	2.0000	0.0000	0.0000	1.0000	0.0000	0.0000	0.0000	0.0000	0.0000	
3.0000	0.0000	253.0000	0.0000	0.0000	16.0000	1.0000	0.0000	1.0000	0.0000	2.0000	
1.0000	1.0000	3.0000	442.0000	0.0000	6.0000	0.0000	0.0000	0.0000	3.0000	6.0000	
2.0000	1.0000	1.0000	3.0000	303.0000	5.0000	23.0000	10.0000	4.0000	6.0000	3.0000	
1.0000	0.0000	8.0000	10.0000	0.0000	210.0000	0.0000	0.0000	1.0000	1.0000	1.0000	
2.0000	2.0000	0.0000	1.0000	0.0000	3.0000	315.0000	21.0000	0.0000	0.0000	0.0000	
2.0000	10.0000	0.0000	0.0000	1.0000	0.0000	1.0000	249.0000	0.0000	8.0000	0.0000	
3.0000	0.0000	35.0000	4.0000	5.0000	3.0000	0.0000	0.0000	184.0000	0.0000	1.0000	
0.0000	4.0000	0.0000	0.0000	2.0000	0.0000	1.0000	2.0000	0.0000	226.0000	2.0000	
0.0000	2.0000	2.0000	0.0000	0.0000	2.0000	0.0000	0.0000	0.0000	0.0000	189.0000	

r = 0 class label = __other__  n_incorrect_classifications = 36
r = 1 class label = 0  n_incorrect_classifications = 5
r = 2 class label = 1  n_incorrect_classifications = 23
r = 3 class label = 2  n_incorrect_classifications = 20
r = 4 class label = 3  n_incorrect_classifications = 58
r = 5 class label = 4  n_incorrect_classifications = 22
r = 6 class label = 5  n_incorrect_classifications = 29
r = 7 class label = 6  n_incorrect_classifications = 22
r = 8 class label = 7  n_incorrect_classifications = 51
r = 9 class label = 8  n_incorrect_classifications = 11
r = 10 class label = 9  n_incorrect_classifications = 6

MNIST digits: n_correct = 2276 n_incorrect = 224 frac_correct = 0.9104
Confusion matrix:

0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	
0.0000	221.0000	0.0000	0.0000	0.0000	0.0000	0.0000	6.0000	0.0000	4.0000	3.0000	
0.0000	0.0000	266.0000	2.0000	1.0000	2.0000	3.0000	1.0000	1.0000	0.0000	1.0000	
0.0000	0.0000	2.0000	218.0000	14.0000	2.0000	1.0000	0.0000	3.0000	5.0000	2.0000	
0.0000	0.0000	0.0000	1.0000	231.0000	0.0000	4.0000	0.0000	1.0000	1.0000	2.0000	
0.0000	0.0000	3.0000	0.0000	0.0000	257.0000	0.0000	0.0000	3.0000	4.0000	13.0000	
0.0000	0.0000	0.0000	0.0000	1.0000	0.0000	221.0000	5.0000	0.0000	0.0000	0.0000	
0.0000	0.0000	1.0000	10.0000	0.0000	0.0000	3.0000	239.0000	0.0000	1.0000	0.0000	
0.0000	0.0000	7.0000	27.0000	1.0000	3.0000	0.0000	0.0000	232.0000	0.0000	5.0000	
1.0000	0.0000	1.0000	2.0000	5.0000	1.0000	9.0000	4.0000	2.0000	196.0000	1.0000	
0.0000	0.0000	2.0000	6.0000	4.0000	1.0000	0.0000	0.0000	22.0000	14.0000	195.0000	

r = 0 class label = __other__  n_incorrect_classifications = 0
r = 1 class label = 0  n_incorrect_classifications = 13
r = 2 class label = 1  n_incorrect_classifications = 11
r = 3 class label = 2  n_incorrect_classifications = 29
r = 4 class label = 3  n_incorrect_classifications = 9
r = 5 class label = 4  n_incorrect_classifications = 23
r = 6 class label = 5  n_incorrect_classifications = 6
r = 7 class label = 6  n_incorrect_classifications = 15
r = 8 class label = 7  n_incorrect_classifications = 43
r = 9 class label = 8  n_incorrect_classifications = 26
r = 10 class label = 9  n_incorrect_classifications = 49

......  

Feb 14 (afternoon of Feb 14) Test Q on Titan 7: O(137K) image chips
containing O(11K) background samples + O(3.7K) ambiguous partial non-digit
chips + O(1.5K) synthetic handwriting hard negatives + O(1K) all-synthetic
digit hard negatives.  Pixel widths selected according to exponential
distribution based upon fit to SVHN image chips; handwriting font chips
have smaller pixel widths.  Rotated digits (less rotation for handwriting
fonts); gaussian noise; simulated solar shadowing (less shadowing for
handwriting fonts); significant blurring (less blurring for handwriting
fonts); variable gravity; variable backgrounds (fewer variable backgrounds
for handwriting fonts); improved min foreground pixel fraction.  Small
fraction of chars with strokes have foreground_RGB = background_RGB.
Random RGBs now assigned using fitted exponential and gaussian
distributions for S & V (lower S values for handwriting fonts).  Random
horizontal/vertical lines.  Random colored bboxes underneath text chars
(fewer bboxes for handwriting fonts).  Random roll < 7.5 degs (less 3D
rotation for handwriting fonts).

Validation accuracy around 95.9% after 1ish epoch

SVHN digits: n_correct = 2731 n_incorrect = 269 frac_correct = 0.9103
Confusion matrix:

88.0000	9.0000	4.0000	1.0000	3.0000	4.0000	5.0000	2.0000	0.0000	4.0000	3.0000	
2.0000	259.0000	3.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	
0.0000	0.0000	252.0000	1.0000	0.0000	17.0000	2.0000	0.0000	2.0000	0.0000	2.0000	
1.0000	0.0000	4.0000	439.0000	0.0000	5.0000	1.0000	0.0000	1.0000	2.0000	9.0000	
2.0000	1.0000	1.0000	2.0000	303.0000	4.0000	26.0000	7.0000	5.0000	7.0000	3.0000	
2.0000	0.0000	8.0000	10.0000	0.0000	207.0000	0.0000	3.0000	1.0000	0.0000	1.0000	
2.0000	1.0000	0.0000	0.0000	0.0000	2.0000	329.0000	10.0000	0.0000	0.0000	0.0000	
2.0000	7.0000	0.0000	0.0000	1.0000	1.0000	1.0000	250.0000	0.0000	9.0000	0.0000	
2.0000	0.0000	36.0000	2.0000	1.0000	2.0000	1.0000	0.0000	190.0000	0.0000	1.0000	
0.0000	5.0000	0.0000	0.0000	1.0000	0.0000	2.0000	2.0000	0.0000	225.0000	2.0000	
0.0000	2.0000	0.0000	0.0000	0.0000	4.0000	0.0000	0.0000	0.0000	0.0000	189.0000	

r = 0 class label = __other__  n_incorrect_classifications = 35
r = 1 class label = 0  n_incorrect_classifications = 5
r = 2 class label = 1  n_incorrect_classifications = 24
r = 3 class label = 2  n_incorrect_classifications = 23
r = 4 class label = 3  n_incorrect_classifications = 58
r = 5 class label = 4  n_incorrect_classifications = 25
r = 6 class label = 5  n_incorrect_classifications = 15
r = 7 class label = 6  n_incorrect_classifications = 21
r = 8 class label = 7  n_incorrect_classifications = 45
r = 9 class label = 8  n_incorrect_classifications = 12
r = 10 class label = 9  n_incorrect_classifications = 6

MNIST digits:  n_correct = 2277 n_incorrect = 223 frac_correct = 0.9108
Confusion matrix:

0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	0.0000	
0.0000	225.0000	0.0000	0.0000	0.0000	0.0000	0.0000	6.0000	0.0000	3.0000	0.0000	
0.0000	0.0000	269.0000	2.0000	0.0000	1.0000	3.0000	1.0000	1.0000	0.0000	0.0000	
1.0000	2.0000	4.0000	202.0000	22.0000	2.0000	1.0000	0.0000	5.0000	7.0000	1.0000	
0.0000	0.0000	0.0000	1.0000	234.0000	0.0000	1.0000	0.0000	1.0000	2.0000	1.0000	
0.0000	0.0000	2.0000	0.0000	0.0000	261.0000	1.0000	0.0000	2.0000	5.0000	9.0000	
0.0000	0.0000	0.0000	1.0000	4.0000	0.0000	217.0000	5.0000	0.0000	0.0000	0.0000	
0.0000	0.0000	1.0000	16.0000	0.0000	1.0000	2.0000	233.0000	0.0000	1.0000	0.0000	
0.0000	0.0000	9.0000	11.0000	1.0000	3.0000	0.0000	0.0000	248.0000	0.0000	3.0000	
1.0000	0.0000	1.0000	2.0000	10.0000	3.0000	7.0000	4.0000	1.0000	190.0000	3.0000	
0.0000	0.0000	1.0000	7.0000	3.0000	1.0000	1.0000	0.0000	21.0000	12.0000	198.0000	

r = 0 class label = __other__  n_incorrect_classifications = 0
r = 1 class label = 0  n_incorrect_classifications = 9
r = 2 class label = 1  n_incorrect_classifications = 8
r = 3 class label = 2  n_incorrect_classifications = 45
r = 4 class label = 3  n_incorrect_classifications = 6
r = 5 class label = 4  n_incorrect_classifications = 19
r = 6 class label = 5  n_incorrect_classifications = 10
r = 7 class label = 6  n_incorrect_classifications = 21
r = 8 class label = 7  n_incorrect_classifications = 27
r = 9 class label = 8  n_incorrect_classifications = 32
r = 10 class label = 9  n_incorrect_classifications = 46


On Feb 23, we modified the train_base.prototxt file so that the learning
rates for the 8th layer containing 11 rather than 1000 classification nodes
are 10 times larger than those for layers 1 - 7.  We then reran training
tests R1 - R6 (but with base learning rate held constant at 0.0005).  

R1: n_correct = 2675 n_incorrect = 325 frac_correct = 0.8917
R2: n_correct = 2752 n_incorrect = 248 frac_correct = 0.9173
R3: n_correct = 2736 n_incorrect = 264 frac_correct = 0.9120
R4: n_correct = 2718 n_incorrect = 282 frac_correct = 0.9060
R5: n_correct = 2718 n_incorrect = 282 frac_correct = 0.9060
R6: n_correct = 2741 n_incorrect = 259 frac_correct = 0.9137

On Feb 16, we didn't specify any particular learning rate multiplier for
the 8th layer (or any other layers for that matter).  In this case,
individual trained caffe model digit classification results for training
tests R1 - R6 were (with base learning rate held constant at 0.0005):

R1: n_correct = 2721 n_incorrect = 279 frac_correct = 0.9070
R2: n_correct = 2718 n_incorrect = 282 frac_correct = 0.9060
R3: n_correct = 2727 n_incorrect = 273 frac_correct = 0.9090
R4: n_correct = 2733 n_incorrect = 267 frac_correct = 0.9110
R5: n_correct = 2733 n_incorrect = 267 frac_correct = 0.9110
R6: n_correct = 2737 n_incorrect = 263 frac_correct = 0.9123

(Probably R4 and R5 were repeated in both sets of experiments above).  But
there doesn't appear to be any statistically significant difference in
classification performance results whether or not we accelerate the
learning rate in the last layer or not...

=============================================================================

Lessons Learned:

1.  More training data appears to yield improved validation accuracy

2.  Don't perform learning rate stepping down too quickly.

3.  VGG16 appears to be substantially better for classification finetuning
than AlexNet.

4.  On Thinkmate, we have observed via nvidia-smi that "X" can use more
than 2 GB of GPU memory!  

5.  Once asymptotic validation accuracy is reached (around 1.5ish epochs),
different synthetic snapshots appear to yield very similar performance on
3K SVHN test set.

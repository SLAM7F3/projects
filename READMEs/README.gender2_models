==========================================================================
Best gender detection models among Face02 networks
==========================================================================
Last updated on 9/15/16; 9/16/16; 9/17/16; 9/19/16
==========================================================================

*.  Sat, Sep 10 , Titan 1, afternoon, Facenet model 2e, 4 conv layers

New set of O(386K) training image chips which have much less black padding
than in the past.  Manually reset standard deviations for weight fillers
based upon previous Model 2e trained weights.  Using newly updated mean RGB
values.  Training set has been significantly cleaned to remove spurious
classification labels.

Base learning rate = 0.003
stepsize = 25K
gamma = 0.5
weight_decay = 0.001
  
Training started on Facenet model 2e around 5 pm.

/data/caffe/faces/trained_models/Sep10_2e_T1/train_iter_300000.caffemodel 
  caffe validation accuracy asymptotes to around 0.894 after approximately
44 training epochs with some small but definite overfitting

n_total_weights = 10,575,136 n_total_biases = 1283

Parameter layer p = 0
  Weights:  mu = 0.000414118 sigma = 0.0693051
  Biases:  mu = 0.0157053 sigma = 0.0325258

Parameter layer p = 1
  Weights:  mu = -0.00128091 sigma = 0.00962133
  Biases:  mu = 0.00387631 sigma = 0.0256235

Parameter layer p = 2
  Weights:  mu = -0.00115404 sigma = 0.00813202
  Biases:  mu = -0.00536314 sigma = 0.0515413

Parameter layer p = 3
  Weights:  mu = -0.00110636 sigma = 0.00793817
  Biases:  mu = 0.0494581 sigma = 0.143864

Parameter layer p = 4
  Weights:  mu = -9.88699e-05 sigma = 0.00257214
  Biases:  mu = 0.015505 sigma = 0.0615438

Parameter layer p = 5
  Weights:  mu = 0.000194972 sigma = 0.0107389
  Biases:  mu = 0.114255 sigma = 0.0513329

Parameter layer p = 6
  Weights:  mu = 2.32585e-05 sigma = 0.0936726
  Biases:  mu = -7.13665e-07 sigma = 0.0731815

frac_correct = 0.856587 frac_unsure = 0.0230809 frac_incorrect = 0.120332

frac_nonface_correct = 0.999055
frac_nonface_incorrect = 0.00094518

frac_male_correct = 0.808696
frac_male_unsure = 0.0297101
frac_male_incorrect = 0.161594

frac_female_correct = 0.796897
frac_female_unsure = 0.0338505
frac_female_incorrect = 0.169252

Confusion matrix:

1058	1	0	0	  = 1059
128	1116	95	41	  = 1380
95	145	1130	48	  = 1418
0	0	0	0	


male_score_threshold = 0.52 female_score_threshold = 0.52


We still observe some variability in confusion matrix results!!!

------------------------------------------------------------------- 
*. Sat, Sep 10, Titan 3, Facenet model 2r, 6 conv layers,
conv3a+conv3b+conv4a+conv4b, 3 FC layers, batch normalization after all
layers.  Increase learning rate to 0.03 and increase learning rate step
from 25K to 10K.

Training started on titan 3 on Sep 10 around 5 pm

  /data/caffe/faces/trained_models/Sep10_2r_T1/train_iter_182000.caffemodel
  caffe validation accuracy asymptotes to 0.919; very little overfitting

n_layers = 47

Layer l = 0 layer_name = input
Layer l = 1 layer_name = conv1      1
Layer l = 2 layer_name = bn1        2
Layer l = 3 layer_name = scale1
Layer l = 4 layer_name = relu1      1
Layer l = 5 layer_name = drop1
Layer l = 6 layer_name = maxpool1
Layer l = 7 layer_name = conv2      1
Layer l = 8 layer_name = bn2        2
Layer l = 9 layer_name = scale2
Layer l = 10 layer_name = relu2
Layer l = 11 layer_name = drop2
Layer l = 12 layer_name = maxpool2
Layer l = 13 layer_name = conv3a
Layer l = 14 layer_name = bn3a
Layer l = 15 layer_name = scale3a
Layer l = 16 layer_name = relu3a
Layer l = 17 layer_name = drop3a
Layer l = 18 layer_name = conv3b
Layer l = 19 layer_name = bn3b
Layer l = 20 layer_name = scale3b
Layer l = 21 layer_name = relu3b
Layer l = 22 layer_name = drop3b
Layer l = 23 layer_name = maxpool3
Layer l = 24 layer_name = conv4a
Layer l = 25 layer_name = bn4a
Layer l = 26 layer_name = scale4a
Layer l = 27 layer_name = relu4a
Layer l = 28 layer_name = drop4a
Layer l = 29 layer_name = conv4b
Layer l = 30 layer_name = bn4b
Layer l = 31 layer_name = scale4b
Layer l = 32 layer_name = relu4b
Layer l = 33 layer_name = drop4b
Layer l = 34 layer_name = fc5
Layer l = 35 layer_name = bn5
Layer l = 36 layer_name = scale5
Layer l = 37 layer_name = relu5
Layer l = 38 layer_name = drop5
Layer l = 39 layer_name = fc6
Layer l = 40 layer_name = bn6
Layer l = 41 layer_name = scale6
Layer l = 42 layer_name = relu6
Layer l = 43 layer_name = drop6
Layer l = 44 layer_name = fc7_faces
Layer l = 45 layer_name = prob
Layer l = 46 layer_name = output


model_2r

Number of layers containing calculated parameters = 49
n_total_weights = 11,616,544

p = 0 param_blob.num_axes() = 4 param_blob.shape_string() = 96 3 3 3 (2592)
p = 1 param_blob.num_axes() = 1 param_blob.shape_string() = 96 (96)
p = 2 param_blob.num_axes() = 1 param_blob.shape_string() = 96 (96)
p = 3 param_blob.num_axes() = 1 param_blob.shape_string() = 1 (1)
p = 4 param_blob.num_axes() = 1 param_blob.shape_string() = 96 (96)
p = 5 param_blob.num_axes() = 1 param_blob.shape_string() = 96 (96)
p = 6 param_blob.num_axes() = 4 param_blob.shape_string() = 192 96 3 3 (165888)
p = 7 param_blob.num_axes() = 1 param_blob.shape_string() = 192 (192)
p = 8 param_blob.num_axes() = 1 param_blob.shape_string() = 192 (192)
p = 9 param_blob.num_axes() = 1 param_blob.shape_string() = 1 (1)
p = 10 param_blob.num_axes() = 1 param_blob.shape_string() = 192 (192)
p = 11 param_blob.num_axes() = 1 param_blob.shape_string() = 192 (192)
p = 12 param_blob.num_axes() = 4 param_blob.shape_string() = 224 192 3 3 (387072)
p = 13 param_blob.num_axes() = 1 param_blob.shape_string() = 224 (224)
p = 14 param_blob.num_axes() = 1 param_blob.shape_string() = 224 (224)
p = 15 param_blob.num_axes() = 1 param_blob.shape_string() = 1 (1)
p = 16 param_blob.num_axes() = 1 param_blob.shape_string() = 224 (224)
p = 17 param_blob.num_axes() = 1 param_blob.shape_string() = 224 (224)
p = 18 param_blob.num_axes() = 4 param_blob.shape_string() = 224 224 3 3 (451584)
p = 19 param_blob.num_axes() = 1 param_blob.shape_string() = 224 (224)
p = 20 param_blob.num_axes() = 1 param_blob.shape_string() = 224 (224)
p = 21 param_blob.num_axes() = 1 param_blob.shape_string() = 1 (1)
p = 22 param_blob.num_axes() = 1 param_blob.shape_string() = 224 (224)
p = 23 param_blob.num_axes() = 1 param_blob.shape_string() = 224 (224)
p = 24 param_blob.num_axes() = 4 param_blob.shape_string() = 256 224 3 3 (516096)
p = 25 param_blob.num_axes() = 1 param_blob.shape_string() = 256 (256)
p = 26 param_blob.num_axes() = 1 param_blob.shape_string() = 256 (256)
p = 27 param_blob.num_axes() = 1 param_blob.shape_string() = 1 (1)
p = 28 param_blob.num_axes() = 1 param_blob.shape_string() = 256 (256)
p = 29 param_blob.num_axes() = 1 param_blob.shape_string() = 256 (256)
p = 30 param_blob.num_axes() = 4 param_blob.shape_string() = 256 256 3 3 (589824)
p = 31 param_blob.num_axes() = 1 param_blob.shape_string() = 256 (256)
p = 32 param_blob.num_axes() = 1 param_blob.shape_string() = 256 (256)
p = 33 param_blob.num_axes() = 1 param_blob.shape_string() = 1 (1)
p = 34 param_blob.num_axes() = 1 param_blob.shape_string() = 256 (256)
p = 35 param_blob.num_axes() = 1 param_blob.shape_string() = 256 (256)
p = 36 param_blob.num_axes() = 2 param_blob.shape_string() = 256 36864 (9437184)
p = 37 param_blob.num_axes() = 1 param_blob.shape_string() = 256 (256)
p = 38 param_blob.num_axes() = 1 param_blob.shape_string() = 256 (256)
p = 39 param_blob.num_axes() = 1 param_blob.shape_string() = 1 (1)
p = 40 param_blob.num_axes() = 1 param_blob.shape_string() = 256 (256)
p = 41 param_blob.num_axes() = 1 param_blob.shape_string() = 256 (256)
p = 42 param_blob.num_axes() = 2 param_blob.shape_string() = 256 256 (65536)
p = 43 param_blob.num_axes() = 1 param_blob.shape_string() = 256 (256)
p = 44 param_blob.num_axes() = 1 param_blob.shape_string() = 256 (256)
p = 45 param_blob.num_axes() = 1 param_blob.shape_string() = 1 (1)
p = 46 param_blob.num_axes() = 1 param_blob.shape_string() = 256 (256)
p = 47 param_blob.num_axes() = 1 param_blob.shape_string() = 256 (256)
p = 48 param_blob.num_axes() = 2 param_blob.shape_string() = 3 256 (768)

n = 0 n_layer_nodes = 96		input --> conv1
n = 1 n_layer_nodes = 96
n = 2 n_layer_nodes = 96
n = 3 n_layer_nodes = 1
n = 4 n_layer_nodes = 96
n = 5 n_layer_nodes = 96

n = 6 n_layer_nodes = 192		conv1 --> conv2
n = 7 n_layer_nodes = 192
n = 8 n_layer_nodes = 192
n = 9 n_layer_nodes = 1
n = 10 n_layer_nodes = 192
n = 11 n_layer_nodes = 192

n = 12 n_layer_nodes = 224		conv2 --> conv3a
n = 13 n_layer_nodes = 224
n = 14 n_layer_nodes = 224
n = 15 n_layer_nodes = 1
n = 16 n_layer_nodes = 224
n = 17 n_layer_nodes = 224

n = 18 n_layer_nodes = 224		conv3a --> conv3b
n = 19 n_layer_nodes = 224
n = 20 n_layer_nodes = 224
n = 21 n_layer_nodes = 1
n = 22 n_layer_nodes = 224
n = 23 n_layer_nodes = 224

n = 24 n_layer_nodes = 256		conv3b --> conv4a
n = 25 n_layer_nodes = 256
n = 26 n_layer_nodes = 256
n = 27 n_layer_nodes = 1
n = 28 n_layer_nodes = 256
n = 29 n_layer_nodes = 256

n = 30 n_layer_nodes = 256		conv4a --> conv4b
n = 31 n_layer_nodes = 256
n = 32 n_layer_nodes = 256
n = 33 n_layer_nodes = 1
n = 34 n_layer_nodes = 256
n = 35 n_layer_nodes = 256

n = 36 n_layer_nodes = 256             conv4b --> fc5
n = 37 n_layer_nodes = 256
n = 38 n_layer_nodes = 256
n = 39 n_layer_nodes = 1
n = 40 n_layer_nodes = 256
n = 41 n_layer_nodes = 256

n = 42 n_layer_nodes = 256            fc5 --> fc6
n = 43 n_layer_nodes = 256
n = 44 n_layer_nodes = 256
n = 45 n_layer_nodes = 1
n = 46 n_layer_nodes = 256
n = 47 n_layer_nodes = 256

n = 48 n_layer_nodes = 3	     fc6 --> fc7_faces

frac_correct = 0.881681 frac_unsure = 0.00960042 frac_incorrect = 0.108718

frac_nonface_correct = 0.996212
frac_nonface_incorrect = 0.00378788

frac_male_correct = 0.839855
frac_male_unsure = 0.0152174
frac_male_incorrect = 0.144928

frac_female_correct = 0.837094
frac_female_unsure = 0.0112835
frac_female_incorrect = 0.151622

Confusion matrix:

1054	3	2	0	
87	1159	113	21	
71	144	1187	16	
0	0	0	0	

male_score_threshold = 0.52 female_score_threshold = 0.52

------------------------------------------------------------------- 
*. Sun, Sep 11, Titan 1, Facenet model 2n, 6 conv layers,
conv3a+conv3b+conv4a+conv4b, 3 FC layers, no batch normalization 

Training started on titan 1 on Sep 11 around 3 pm

  /data/caffe/faces/trained_models/Sep10_2r_T1/train_iter_288000.caffemodel
  caffe validation accuracy asymptotes to 0.904; some small overfitting

frac_correct = 0.863554 frac_unsure = 0.00985733 frac_incorrect = 0.126589

frac_nonface_correct = 0.993377
frac_nonface_incorrect = 0.00662252

frac_male_correct = 0.813043
frac_male_unsure = 0.0137681
frac_male_incorrect = 0.173188

frac_female_correct = 0.815938
frac_female_unsure = 0.0133992
frac_female_incorrect = 0.170663

Confusion matrix:

1052	2	5	0	
114	1122	125	19	
91	151	1157	19	
0	0	0	0	

male_score_threshold = 0.52 female_score_threshold = 0.52

------------------------------------------------------------------- 
*. Weds, Sep 14, Titan 3, Facenet model 2s, 6 conv layers,
conv3a+conv3b+conv4a+conv4b, 3 FC layers, batch normalization, l2
reg param = 0.0005, less dropout

Started training on Titan 3 around 8:15 am 

/data/caffe/faces/trained_models/Sep14_2s_T3/train_iter_262000.caffemodel
caffe validation accuracy asymptotes to 0.921; definitely some nontrivial
overfitting!

frac_correct = 0.879668 frac_unsure = 0.00700207 frac_incorrect = 0.11333

frac_nonface_correct = 0.990548
frac_nonface_incorrect = 0.0094518

frac_male_correct = 0.836232
frac_male_unsure = 0.0108696
frac_male_incorrect = 0.152899

frac_female_correct = 0.83921
frac_female_unsure = 0.00846262
frac_female_incorrect = 0.152327

Confusion matrix:
 
1048	6	5	0	
84	1154	127	15	
65	151	1190	12	
0	0	0	0	

------------------------------------------------------------------- 
*. Thurs, Sep 15, Titan 1, Facenet model 2t, 6 conv layers,
conv3a+conv3b+conv4a+conv4b, 3 FC layers, batch normalization, l2
reg param = 0.0005, No dropout in first 2 conv layers.

Training started at noontime on titan1

/data/caffe/faces/trained_models/Sep15_2t_T1/train_iter_172000.caffemodel
caffe validation accuracy asymptotes to 0.916 after approximate 25 training
epochs; no significant overfitting

base learning rate = 0.03
stepsize = 10K
gamma = 0.5
weight_decay = 0.0005

...........................................
n = 0 n_layer_nodes = 96
n = 1 n_layer_nodes = 96
n = 2 n_layer_nodes = 96
n = 3 n_layer_nodes = 1
n = 4 n_layer_nodes = 96
n = 5 n_layer_nodes = 96
n = 6 n_layer_nodes = 192
n = 7 n_layer_nodes = 192
n = 8 n_layer_nodes = 192
n = 9 n_layer_nodes = 1
n = 10 n_layer_nodes = 192
n = 11 n_layer_nodes = 192
n = 12 n_layer_nodes = 224
n = 13 n_layer_nodes = 224
n = 14 n_layer_nodes = 224
n = 15 n_layer_nodes = 1
n = 16 n_layer_nodes = 224
n = 17 n_layer_nodes = 224
n = 18 n_layer_nodes = 224
n = 19 n_layer_nodes = 224
n = 20 n_layer_nodes = 224
n = 21 n_layer_nodes = 1
n = 22 n_layer_nodes = 224
n = 23 n_layer_nodes = 224
n = 24 n_layer_nodes = 256
n = 25 n_layer_nodes = 256
n = 26 n_layer_nodes = 256
n = 27 n_layer_nodes = 1
n = 28 n_layer_nodes = 256
n = 29 n_layer_nodes = 256
n = 30 n_layer_nodes = 256
n = 31 n_layer_nodes = 256
n = 32 n_layer_nodes = 256
n = 33 n_layer_nodes = 1
n = 34 n_layer_nodes = 256
n = 35 n_layer_nodes = 256
n = 36 n_layer_nodes = 256
n = 37 n_layer_nodes = 256
n = 38 n_layer_nodes = 256
n = 39 n_layer_nodes = 1
n = 40 n_layer_nodes = 256
n = 41 n_layer_nodes = 256
n = 42 n_layer_nodes = 256
n = 43 n_layer_nodes = 256
n = 44 n_layer_nodes = 256
n = 45 n_layer_nodes = 1
n = 46 n_layer_nodes = 256
n = 47 n_layer_nodes = 256
n = 48 n_layer_nodes = 3

Minor layer l = 0 layer_name = input
Minor layer l = 1 layer_name = conv1
Minor layer l = 2 layer_name = bn1
Minor layer l = 3 layer_name = scale1
Minor layer l = 4 layer_name = relu1
Minor layer l = 5 layer_name = drop1
Minor layer l = 6 layer_name = maxpool1
Minor layer l = 7 layer_name = conv2
Minor layer l = 8 layer_name = bn2
Minor layer l = 9 layer_name = scale2
Minor layer l = 10 layer_name = relu2
Minor layer l = 11 layer_name = drop2
Minor layer l = 12 layer_name = maxpool2
Minor layer l = 13 layer_name = conv3a
Minor layer l = 14 layer_name = bn3a
Minor layer l = 15 layer_name = scale3a
Minor layer l = 16 layer_name = relu3a
Minor layer l = 17 layer_name = drop3a
Minor layer l = 18 layer_name = conv3b
Minor layer l = 19 layer_name = bn3b
Minor layer l = 20 layer_name = scale3b
Minor layer l = 21 layer_name = relu3b
Minor layer l = 22 layer_name = drop3b
Minor layer l = 23 layer_name = maxpool3
Minor layer l = 24 layer_name = conv4a
Minor layer l = 25 layer_name = bn4a
Minor layer l = 26 layer_name = scale4a
Minor layer l = 27 layer_name = relu4a
Minor layer l = 28 layer_name = drop4a
Minor layer l = 29 layer_name = conv4b
Minor layer l = 30 layer_name = bn4b
Minor layer l = 31 layer_name = scale4b
Minor layer l = 32 layer_name = relu4b
Minor layer l = 33 layer_name = drop4b
Minor layer l = 34 layer_name = fc5
Minor layer l = 35 layer_name = bn5
Minor layer l = 36 layer_name = scale5
Minor layer l = 37 layer_name = relu5
Minor layer l = 38 layer_name = drop5
Minor layer l = 39 layer_name = fc6
Minor layer l = 40 layer_name = bn6
Minor layer l = 41 layer_name = scale6
Minor layer l = 42 layer_name = relu6
Minor layer l = 43 layer_name = drop6
Minor layer l = 44 layer_name = fc7_faces
Minor layer l = 45 layer_name = prob
Minor layer l = 46 layer_name = output

Blob b = 0 blob_name = data : 1 3 96 96 (27648)
Blob b = 1 blob_name = conv1 : 1 96 96 96 (884736)
Blob b = 2 blob_name = maxpool1 : 1 96 48 48 (221184)
Blob b = 3 blob_name = conv2 : 1 192 48 48 (442368)
Blob b = 4 blob_name = maxpool2 : 1 192 24 24 (110592)
Blob b = 5 blob_name = conv3a : 1 224 24 24 (129024)
Blob b = 6 blob_name = conv3b : 1 224 24 24 (129024)
Blob b = 7 blob_name = maxpool3 : 1 224 12 12 (32256)
Blob b = 8 blob_name = conv4a : 1 256 12 12 (36864)
Blob b = 9 blob_name = conv4b : 1 256 12 12 (36864)
Blob b = 10 blob_name = fc5 : 1 256 (256)
Blob b = 11 blob_name = fc6 : 1 256 (256)
Blob b = 12 blob_name = fc7_faces : 1 3 (3)
Blob b = 13 blob_name = prob : 1 3 (3)
Blob b = 14 blob_name = output : 1 2 1 (2)

Number of layers containing calculated parameters = 49
p = 0 param_blob.num_axes() = 4 param_blob.shape_string() = 96 3 3 3 (2592)
p = 1 param_blob.num_axes() = 1 param_blob.shape_string() = 96 (96)
p = 2 param_blob.num_axes() = 1 param_blob.shape_string() = 96 (96)
p = 3 param_blob.num_axes() = 1 param_blob.shape_string() = 1 (1)
p = 4 param_blob.num_axes() = 1 param_blob.shape_string() = 96 (96)
p = 5 param_blob.num_axes() = 1 param_blob.shape_string() = 96 (96)
p = 6 param_blob.num_axes() = 4 param_blob.shape_string() = 192 96 3 3 (165888)
p = 7 param_blob.num_axes() = 1 param_blob.shape_string() = 192 (192)
p = 8 param_blob.num_axes() = 1 param_blob.shape_string() = 192 (192)
p = 9 param_blob.num_axes() = 1 param_blob.shape_string() = 1 (1)
p = 10 param_blob.num_axes() = 1 param_blob.shape_string() = 192 (192)
p = 11 param_blob.num_axes() = 1 param_blob.shape_string() = 192 (192)
p = 12 param_blob.num_axes() = 4 param_blob.shape_string() = 224 192 3 3 (387072)
p = 13 param_blob.num_axes() = 1 param_blob.shape_string() = 224 (224)
p = 14 param_blob.num_axes() = 1 param_blob.shape_string() = 224 (224)
p = 15 param_blob.num_axes() = 1 param_blob.shape_string() = 1 (1)
p = 16 param_blob.num_axes() = 1 param_blob.shape_string() = 224 (224)
p = 17 param_blob.num_axes() = 1 param_blob.shape_string() = 224 (224)
p = 18 param_blob.num_axes() = 4 param_blob.shape_string() = 224 224 3 3 (451584)
p = 19 param_blob.num_axes() = 1 param_blob.shape_string() = 224 (224)
p = 20 param_blob.num_axes() = 1 param_blob.shape_string() = 224 (224)
p = 21 param_blob.num_axes() = 1 param_blob.shape_string() = 1 (1)
p = 22 param_blob.num_axes() = 1 param_blob.shape_string() = 224 (224)
p = 23 param_blob.num_axes() = 1 param_blob.shape_string() = 224 (224)
p = 24 param_blob.num_axes() = 4 param_blob.shape_string() = 256 224 3 3 (516096)
p = 25 param_blob.num_axes() = 1 param_blob.shape_string() = 256 (256)
p = 26 param_blob.num_axes() = 1 param_blob.shape_string() = 256 (256)
p = 27 param_blob.num_axes() = 1 param_blob.shape_string() = 1 (1)
p = 28 param_blob.num_axes() = 1 param_blob.shape_string() = 256 (256)
p = 29 param_blob.num_axes() = 1 param_blob.shape_string() = 256 (256)
p = 30 param_blob.num_axes() = 4 param_blob.shape_string() = 256 256 3 3 (589824)
p = 31 param_blob.num_axes() = 1 param_blob.shape_string() = 256 (256)
p = 32 param_blob.num_axes() = 1 param_blob.shape_string() = 256 (256)
p = 33 param_blob.num_axes() = 1 param_blob.shape_string() = 1 (1)
p = 34 param_blob.num_axes() = 1 param_blob.shape_string() = 256 (256)
p = 35 param_blob.num_axes() = 1 param_blob.shape_string() = 256 (256)
p = 36 param_blob.num_axes() = 2 param_blob.shape_string() = 256 36864 (9437184)
p = 37 param_blob.num_axes() = 1 param_blob.shape_string() = 256 (256)
p = 38 param_blob.num_axes() = 1 param_blob.shape_string() = 256 (256)
p = 39 param_blob.num_axes() = 1 param_blob.shape_string() = 1 (1)
p = 40 param_blob.num_axes() = 1 param_blob.shape_string() = 256 (256)
p = 41 param_blob.num_axes() = 1 param_blob.shape_string() = 256 (256)
p = 42 param_blob.num_axes() = 2 param_blob.shape_string() = 256 256 (65536)
p = 43 param_blob.num_axes() = 1 param_blob.shape_string() = 256 (256)
p = 44 param_blob.num_axes() = 1 param_blob.shape_string() = 256 (256)
p = 45 param_blob.num_axes() = 1 param_blob.shape_string() = 1 (1)
p = 46 param_blob.num_axes() = 1 param_blob.shape_string() = 256 (256)
p = 47 param_blob.num_axes() = 1 param_blob.shape_string() = 256 (256)
p = 48 param_blob.num_axes() = 2 param_blob.shape_string() = 3 256 (768)

--------------------------------------------
n_total_params = 11616544
--------------------------------------------
n_param_layers = 49
Parameter layer p = 0
n_param_layer_nodes = 96
  Params:  mu = -0.0019578 sigma = 0.0632684
Parameter layer p = 1
n_param_layer_nodes = 96
  Params:  mu = -107.591 sigma = 537.743
Parameter layer p = 2
n_param_layer_nodes = 96
  Params:  mu = 352440 sigma = 369837
Parameter layer p = 3
n_param_layer_nodes = 1
  Params:  mu = 999.982 sigma = 0
Parameter layer p = 4
n_param_layer_nodes = 96
  Params:  mu = 0.105581 sigma = 0.0615517
Parameter layer p = 5
n_param_layer_nodes = 96
  Params:  mu = -0.0611344 sigma = 0.111843
Parameter layer p = 6
n_param_layer_nodes = 192
  Params:  mu = -0.00077341 sigma = 0.0100557
Parameter layer p = 7
n_param_layer_nodes = 192
  Params:  mu = -69.8906 sigma = 69.1877
Parameter layer p = 8
n_param_layer_nodes = 192
  Params:  mu = 4.80583 sigma = 3.52057
Parameter layer p = 9
n_param_layer_nodes = 1
  Params:  mu = 999.982 sigma = 0
Parameter layer p = 10
n_param_layer_nodes = 192
  Params:  mu = 0.113998 sigma = 0.0366861
Parameter layer p = 11
n_param_layer_nodes = 192
  Params:  mu = -0.0516389 sigma = 0.052919
Parameter layer p = 12
n_param_layer_nodes = 224
  Params:  mu = -0.000821355 sigma = 0.0091484
Parameter layer p = 13
n_param_layer_nodes = 224
  Params:  mu = -44.5999 sigma = 27.1983
Parameter layer p = 14
n_param_layer_nodes = 224
  Params:  mu = 4.59831 sigma = 1.52792
Parameter layer p = 15
n_param_layer_nodes = 1
  Params:  mu = 999.982 sigma = 0
Parameter layer p = 16
n_param_layer_nodes = 224
  Params:  mu = 0.129481 sigma = 0.019303
Parameter layer p = 17
n_param_layer_nodes = 224
  Params:  mu = -0.0793734 sigma = 0.029569
Parameter layer p = 18
n_param_layer_nodes = 224
  Params:  mu = -0.00177042 sigma = 0.0104046
Parameter layer p = 19
n_param_layer_nodes = 224
  Params:  mu = -47.9354 sigma = 21.2745
Parameter layer p = 20
n_param_layer_nodes = 224
  Params:  mu = 1.75844 sigma = 0.481216
Parameter layer p = 21
n_param_layer_nodes = 1
  Params:  mu = 999.982 sigma = 0
Parameter layer p = 22
n_param_layer_nodes = 224
  Params:  mu = 0.127859 sigma = 0.0150917
Parameter layer p = 23
n_param_layer_nodes = 224
  Params:  mu = -0.130827 sigma = 0.02649
Parameter layer p = 24
n_param_layer_nodes = 256
  Params:  mu = -0.00117091 sigma = 0.0104112
Parameter layer p = 25
n_param_layer_nodes = 256
  Params:  mu = -34.7875 sigma = 15.107
Parameter layer p = 26
n_param_layer_nodes = 256
  Params:  mu = 1.6635 sigma = 0.319021
Parameter layer p = 27
n_param_layer_nodes = 1
  Params:  mu = 999.982 sigma = 0
Parameter layer p = 28
n_param_layer_nodes = 256
  Params:  mu = 0.1192 sigma = 0.0153824
Parameter layer p = 29
n_param_layer_nodes = 256
  Params:  mu = -0.108604 sigma = 0.0336844
Parameter layer p = 30
n_param_layer_nodes = 256
  Params:  mu = -0.00164762 sigma = 0.00827566
Parameter layer p = 31
n_param_layer_nodes = 256
  Params:  mu = -36.6976 sigma = 18.5409
Parameter layer p = 32
n_param_layer_nodes = 256
  Params:  mu = 0.798933 sigma = 0.391371
Parameter layer p = 33
n_param_layer_nodes = 1
  Params:  mu = 999.982 sigma = 0
Parameter layer p = 34
n_param_layer_nodes = 256
  Params:  mu = 0.0973607 sigma = 0.0314797
Parameter layer p = 35
n_param_layer_nodes = 256
  Params:  mu = -0.0874832 sigma = 0.0463007
Parameter layer p = 36
n_param_layer_nodes = 256
  Params:  mu = -0.000146386 sigma = 0.0032806
Parameter layer p = 37
n_param_layer_nodes = 256
  Params:  mu = -79.2548 sigma = 45.8389
Parameter layer p = 38
n_param_layer_nodes = 256
  Params:  mu = 15.2367 sigma = 5.25795
Parameter layer p = 39
n_param_layer_nodes = 1
  Params:  mu = 999.982 sigma = 0
Parameter layer p = 40
n_param_layer_nodes = 256
  Params:  mu = 0.111991 sigma = 0.016843
Parameter layer p = 41
n_param_layer_nodes = 256
  Params:  mu = -0.0624233 sigma = 0.0437613
Parameter layer p = 42
n_param_layer_nodes = 256
  Params:  mu = -0.000160943 sigma = 0.00937837
Parameter layer p = 43
n_param_layer_nodes = 256
  Params:  mu = -0.849341 sigma = 7.47459
Parameter layer p = 44
n_param_layer_nodes = 256
  Params:  mu = 1.08402 sigma = 0.367622
Parameter layer p = 45
n_param_layer_nodes = 1
  Params:  mu = 999.982 sigma = 0
Parameter layer p = 46
n_param_layer_nodes = 256
  Params:  mu = 0.228273 sigma = 0.0326414
Parameter layer p = 47
n_param_layer_nodes = 256
  Params:  mu = 0.0757586 sigma = 0.04428
Parameter layer p = 48
n_param_layer_nodes = 3
  Params:  mu = -1.35781e-05 sigma = 0.144161


frac_correct = 0.884047 frac_unsure = 0.0127108 frac_incorrect = 0.103243

frac_nonface_correct = 0.99527
frac_nonface_incorrect = 0.00473037

frac_male_correct = 0.83913
frac_male_unsure = 0.0195652
frac_male_incorrect = 0.141304

frac_female_correct = 0.844852
frac_female_unsure = 0.0155148
frac_female_incorrect = 0.139633

Confusion matrix:

1053	3	3	0	= 1059
84	1158	111	27	= 1380
62	136	1198	22	= 1418
0	0	0	0	

***************************
As of 9/16/16, 6 conv-layer model 2t with batch normalization and reduced
regularization yields the best overall gender classification performance
results!

***************************

FC6:  First 41 nodes = general face
Middle 149 nodes = female/male face
Last 66 nodes = non-face

------------------------------------------------------------------- 
*. Thurs, Sep 15, Titan 3, Facenet model 2t, 6 conv layers,
conv3a+conv3b+conv4a+conv4b, 3 FC layers, batch normalization, l2
reg param = 0.0010, No dropout in first 2 conv layers.

Training started at noontime on titan3
/data/caffe/faces/trained_models/Sep15_2t_T3/train_iter_194000.caffemodel
caffe validation accuracy asymptotes to 0.913; no overfitting

frac_correct = 0.878403 frac_unsure = 0.00881514 frac_incorrect = 0.112782

frac_nonface_correct = 0.995279
frac_nonface_incorrect = 0.00472144

frac_male_correct = 0.83913
frac_male_unsure = 0.0123188
frac_male_incorrect = 0.148551

frac_female_correct = 0.829337
frac_female_unsure = 0.0119887
frac_female_incorrect = 0.158674

Confusion matrix:

1054	3	2	0	
83	1158	122	17	
66	159	1176	17	
0	0	0	0	

------------------------------------------------------------------- 
*. Fri, Sep 16, Titan 3, Facenet model 2t, 6 conv layers,
conv3a+conv3b+conv4a+conv4b, 3 FC layers, batch normalization, No L2
regularization.  No dropout in first 2 conv layers.

Training started around 8:30 am on 9/16/16

/data/caffe/faces/trained_models/Sep16_2t_T3/train_iter_230000.caffemodel
caffe validation accuracy asymptotes to 0.880; no overfitting
frac_correct = 0.860996 frac_unsure = 0.0225622 frac_incorrect = 0.116442

frac_nonface_correct = 0.996219
frac_nonface_incorrect = 0.00378072

frac_male_correct = 0.818116
frac_male_unsure = 0.0304348
frac_male_incorrect = 0.151449

frac_female_correct = 0.801834
frac_female_unsure = 0.0317348
frac_female_incorrect = 0.166432

Confusion matrix:

1054	5	0	0	
89	1129	120	42	
74	162	1137	45	
0	0	0	0	

Results with no L2 regularization are clearly inferior to those with l2 reg
= 0.0005

------------------------------------------------------------------- 
*. Sun, Sep 19, Titan 1, Facenet model 2u, 6 conv layers,
conv3a+conv3b+conv4a+conv4b, 2 FC layers, batch normalization, L2 
reg param = 0.0005.  No dropout in first 2 conv layers.

/data/caffe/faces/trained_models/Sep18_2u_T1/train_iter_148000.caffemodel
caffe validation accuracy asymptotes to 0.916; no overfitting

n_minor_layers = 42
n_blobs = 14
network_phase = caffe::TEST
Minor layer l = 0 layer_name = input
Minor layer l = 1 layer_name = conv1
Minor layer l = 2 layer_name = bn1
Minor layer l = 3 layer_name = scale1
Minor layer l = 4 layer_name = relu1
Minor layer l = 5 layer_name = drop1
Minor layer l = 6 layer_name = maxpool1
Minor layer l = 7 layer_name = conv2
Minor layer l = 8 layer_name = bn2
Minor layer l = 9 layer_name = scale2
Minor layer l = 10 layer_name = relu2
Minor layer l = 11 layer_name = drop2
Minor layer l = 12 layer_name = maxpool2
Minor layer l = 13 layer_name = conv3a
Minor layer l = 14 layer_name = bn3a
Minor layer l = 15 layer_name = scale3a
Minor layer l = 16 layer_name = relu3a
Minor layer l = 17 layer_name = drop3a
Minor layer l = 18 layer_name = conv3b
Minor layer l = 19 layer_name = bn3b
Minor layer l = 20 layer_name = scale3b
Minor layer l = 21 layer_name = relu3b
Minor layer l = 22 layer_name = drop3b
Minor layer l = 23 layer_name = maxpool3
Minor layer l = 24 layer_name = conv4a
Minor layer l = 25 layer_name = bn4a
Minor layer l = 26 layer_name = scale4a
Minor layer l = 27 layer_name = relu4a
Minor layer l = 28 layer_name = drop4a
Minor layer l = 29 layer_name = conv4b
Minor layer l = 30 layer_name = bn4b
Minor layer l = 31 layer_name = scale4b
Minor layer l = 32 layer_name = relu4b
Minor layer l = 33 layer_name = drop4b
Minor layer l = 34 layer_name = fc5
Minor layer l = 35 layer_name = bn5
Minor layer l = 36 layer_name = scale5
Minor layer l = 37 layer_name = relu5
Minor layer l = 38 layer_name = drop5
Minor layer l = 39 layer_name = fc6_faces
Minor layer l = 40 layer_name = prob
Minor layer l = 41 layer_name = output

Blob b = 0 blob_name = data : 1 3 96 96 (27648)
Blob b = 1 blob_name = conv1 : 1 96 96 96 (884736)
Blob b = 2 blob_name = maxpool1 : 1 96 48 48 (221184)
Blob b = 3 blob_name = conv2 : 1 192 48 48 (442368)
Blob b = 4 blob_name = maxpool2 : 1 192 24 24 (110592)
Blob b = 5 blob_name = conv3a : 1 224 24 24 (129024)
Blob b = 6 blob_name = conv3b : 1 224 24 24 (129024)
Blob b = 7 blob_name = maxpool3 : 1 224 12 12 (32256)
Blob b = 8 blob_name = conv4a : 1 256 12 12 (36864)
Blob b = 9 blob_name = conv4b : 1 256 12 12 (36864)
Blob b = 10 blob_name = fc5 : 1 256 (256)
Blob b = 11 blob_name = fc6_faces : 1 3 (3)
Blob b = 12 blob_name = prob : 1 3 (3)
Blob b = 13 blob_name = output : 1 2 1 (2)

Number of layers containing calculated parameters = 43
p = 0 param_blob.num_axes() = 4 param_blob.shape_string() = 96 3 3 3 (2592)
p = 1 param_blob.num_axes() = 1 param_blob.shape_string() = 96 (96)
p = 2 param_blob.num_axes() = 1 param_blob.shape_string() = 96 (96)
p = 3 param_blob.num_axes() = 1 param_blob.shape_string() = 1 (1)
p = 4 param_blob.num_axes() = 1 param_blob.shape_string() = 96 (96)
p = 5 param_blob.num_axes() = 1 param_blob.shape_string() = 96 (96)
p = 6 param_blob.num_axes() = 4 param_blob.shape_string() = 192 96 3 3 (165888)
p = 7 param_blob.num_axes() = 1 param_blob.shape_string() = 192 (192)
p = 8 param_blob.num_axes() = 1 param_blob.shape_string() = 192 (192)
p = 9 param_blob.num_axes() = 1 param_blob.shape_string() = 1 (1)
p = 10 param_blob.num_axes() = 1 param_blob.shape_string() = 192 (192)
p = 11 param_blob.num_axes() = 1 param_blob.shape_string() = 192 (192)
p = 12 param_blob.num_axes() = 4 param_blob.shape_string() = 224 192 3 3 (387072)
p = 13 param_blob.num_axes() = 1 param_blob.shape_string() = 224 (224)
p = 14 param_blob.num_axes() = 1 param_blob.shape_string() = 224 (224)
p = 15 param_blob.num_axes() = 1 param_blob.shape_string() = 1 (1)
p = 16 param_blob.num_axes() = 1 param_blob.shape_string() = 224 (224)
p = 17 param_blob.num_axes() = 1 param_blob.shape_string() = 224 (224)
p = 18 param_blob.num_axes() = 4 param_blob.shape_string() = 224 224 3 3 (451584)
p = 19 param_blob.num_axes() = 1 param_blob.shape_string() = 224 (224)
p = 20 param_blob.num_axes() = 1 param_blob.shape_string() = 224 (224)
p = 21 param_blob.num_axes() = 1 param_blob.shape_string() = 1 (1)
p = 22 param_blob.num_axes() = 1 param_blob.shape_string() = 224 (224)
p = 23 param_blob.num_axes() = 1 param_blob.shape_string() = 224 (224)
p = 24 param_blob.num_axes() = 4 param_blob.shape_string() = 256 224 3 3 (516096)
p = 25 param_blob.num_axes() = 1 param_blob.shape_string() = 256 (256)
p = 26 param_blob.num_axes() = 1 param_blob.shape_string() = 256 (256)
p = 27 param_blob.num_axes() = 1 param_blob.shape_string() = 1 (1)
p = 28 param_blob.num_axes() = 1 param_blob.shape_string() = 256 (256)
p = 29 param_blob.num_axes() = 1 param_blob.shape_string() = 256 (256)
p = 30 param_blob.num_axes() = 4 param_blob.shape_string() = 256 256 3 3 (589824)
p = 31 param_blob.num_axes() = 1 param_blob.shape_string() = 256 (256)
p = 32 param_blob.num_axes() = 1 param_blob.shape_string() = 256 (256)
p = 33 param_blob.num_axes() = 1 param_blob.shape_string() = 1 (1)
p = 34 param_blob.num_axes() = 1 param_blob.shape_string() = 256 (256)
p = 35 param_blob.num_axes() = 1 param_blob.shape_string() = 256 (256)
p = 36 param_blob.num_axes() = 2 param_blob.shape_string() = 256 36864 (9437184)
p = 37 param_blob.num_axes() = 1 param_blob.shape_string() = 256 (256)
p = 38 param_blob.num_axes() = 1 param_blob.shape_string() = 256 (256)
p = 39 param_blob.num_axes() = 1 param_blob.shape_string() = 1 (1)
p = 40 param_blob.num_axes() = 1 param_blob.shape_string() = 256 (256)
p = 41 param_blob.num_axes() = 1 param_blob.shape_string() = 256 (256)
p = 42 param_blob.num_axes() = 2 param_blob.shape_string() = 3 256 (768)

--------------------------------------------
n_total_params = 11551008
--------------------------------------------
n_param_layers = 43
Parameter layer p = 0
n_param_layer_nodes = 96
  Params:  mu = -0.00125386 sigma = 0.0637989
Parameter layer p = 1
n_param_layer_nodes = 96
  Params:  mu = -83.072 sigma = 545.768
Parameter layer p = 2
n_param_layer_nodes = 96
  Params:  mu = 353670 sigma = 343644
Parameter layer p = 3
n_param_layer_nodes = 1
  Params:  mu = 999.982 sigma = 0
Parameter layer p = 4
n_param_layer_nodes = 96
  Params:  mu = 0.102181 sigma = 0.0634938
Parameter layer p = 5
n_param_layer_nodes = 96
  Params:  mu = -0.053286 sigma = 0.10698
Parameter layer p = 6
n_param_layer_nodes = 192
  Params:  mu = -0.000939713 sigma = 0.0102567
Parameter layer p = 7
n_param_layer_nodes = 192
  Params:  mu = -71.9374 sigma = 67.8308
Parameter layer p = 8
n_param_layer_nodes = 192
  Params:  mu = 5.35309 sigma = 3.82573
Parameter layer p = 9
n_param_layer_nodes = 1
  Params:  mu = 999.982 sigma = 0
Parameter layer p = 10
n_param_layer_nodes = 192
  Params:  mu = 0.114266 sigma = 0.0353001
Parameter layer p = 11
n_param_layer_nodes = 192
  Params:  mu = -0.0528894 sigma = 0.0467916
Parameter layer p = 12
n_param_layer_nodes = 224
  Params:  mu = -0.000805973 sigma = 0.00913701
Parameter layer p = 13
n_param_layer_nodes = 224
  Params:  mu = -41.9411 sigma = 22.5564
Parameter layer p = 14
n_param_layer_nodes = 224
  Params:  mu = 3.97002 sigma = 1.15715
Parameter layer p = 15
n_param_layer_nodes = 1
  Params:  mu = 999.982 sigma = 0
Parameter layer p = 16
n_param_layer_nodes = 224
  Params:  mu = 0.129816 sigma = 0.018363
Parameter layer p = 17
n_param_layer_nodes = 224
  Params:  mu = -0.083253 sigma = 0.0295765
Parameter layer p = 18
n_param_layer_nodes = 224
  Params:  mu = -0.00182562 sigma = 0.0104982
Parameter layer p = 19
n_param_layer_nodes = 224
  Params:  mu = -46.7 sigma = 21.0664
Parameter layer p = 20
n_param_layer_nodes = 224
  Params:  mu = 1.62823 sigma = 0.419345
Parameter layer p = 21
n_param_layer_nodes = 1
  Params:  mu = 999.982 sigma = 0
Parameter layer p = 22
n_param_layer_nodes = 224
  Params:  mu = 0.127973 sigma = 0.0154715
Parameter layer p = 23
n_param_layer_nodes = 224
  Params:  mu = -0.129398 sigma = 0.0254351
Parameter layer p = 24
n_param_layer_nodes = 256
  Params:  mu = -0.00114237 sigma = 0.0104523
Parameter layer p = 25
n_param_layer_nodes = 256
  Params:  mu = -33.3547 sigma = 14.003
Parameter layer p = 26
n_param_layer_nodes = 256
  Params:  mu = 1.57698 sigma = 0.306422
Parameter layer p = 27
n_param_layer_nodes = 1
  Params:  mu = 999.982 sigma = 0
Parameter layer p = 28
n_param_layer_nodes = 256
  Params:  mu = 0.118019 sigma = 0.0147562
Parameter layer p = 29
n_param_layer_nodes = 256
  Params:  mu = -0.108918 sigma = 0.0310227
Parameter layer p = 30
n_param_layer_nodes = 256
  Params:  mu = -0.00175428 sigma = 0.00819086
Parameter layer p = 31
n_param_layer_nodes = 256
  Params:  mu = -35.1846 sigma = 17.9316
Parameter layer p = 32
n_param_layer_nodes = 256
  Params:  mu = 0.681833 sigma = 0.298597
Parameter layer p = 33
n_param_layer_nodes = 1
  Params:  mu = 999.982 sigma = 0
Parameter layer p = 34
n_param_layer_nodes = 256
  Params:  mu = 0.0961928 sigma = 0.0275481
Parameter layer p = 35
n_param_layer_nodes = 256
  Params:  mu = -0.0873314 sigma = 0.0396653
Parameter layer p = 36
n_param_layer_nodes = 256
  Params:  mu = -3.61235e-05 sigma = 0.00286688
Parameter layer p = 37
n_param_layer_nodes = 256
  Params:  mu = -9.92208 sigma = 31.5134
Parameter layer p = 38
n_param_layer_nodes = 256
  Params:  mu = 9.0511 sigma = 2.65438
Parameter layer p = 39
n_param_layer_nodes = 1
  Params:  mu = 999.982 sigma = 0
Parameter layer p = 40
n_param_layer_nodes = 256
  Params:  mu = 0.283945 sigma = 0.0302986
Parameter layer p = 41
n_param_layer_nodes = 256
  Params:  mu = 0.0281623 sigma = 0.0805298
Parameter layer p = 42
n_param_layer_nodes = 3
  Params:  mu = 1.01037e-05 sigma = 0.175405

////////////////////////////////////////////////

n_minor_layers = 42
minor_layer_skip = 6
n_major_layers = 9
n_param_layers = 43
Major_layer_ID = 0 has 3 input RGB nodes
  Initial major weight node ID = 0
  Final major weight node ID = 2
Major_layer_ID = 1 parameter layer p = 0 n_param_layer_nodes[p] = 96
  Initial major weight node ID = 3
  Final major weight node ID = 98
Major_layer_ID = 2 parameter layer p = 6 n_param_layer_nodes[p] = 192
  Initial major weight node ID = 99
  Final major weight node ID = 290
Major_layer_ID = 3 parameter layer p = 12 n_param_layer_nodes[p] = 224
  Initial major weight node ID = 291
  Final major weight node ID = 514
Major_layer_ID = 4 parameter layer p = 18 n_param_layer_nodes[p] = 224
  Initial major weight node ID = 515
  Final major weight node ID = 738
Major_layer_ID = 5 parameter layer p = 24 n_param_layer_nodes[p] = 256
  Initial major weight node ID = 739
  Final major weight node ID = 994
Major_layer_ID = 6 parameter layer p = 30 n_param_layer_nodes[p] = 256
  Initial major weight node ID = 995
  Final major weight node ID = 1250
Major_layer_ID = 7 parameter layer p = 36 n_param_layer_nodes[p] = 256
  Initial major weight node ID = 1251
  Final major weight node ID = 1506
Major_layer_ID = 8 parameter layer p = 42 n_param_layer_nodes[p] = 3
  Initial major weight node ID = 1507
  Final major weight node ID = 1509
...........................................
n = 0 n_layer_nodes = 96
n = 1 n_layer_nodes = 96
n = 2 n_layer_nodes = 96
n = 3 n_layer_nodes = 1
n = 4 n_layer_nodes = 96
n = 5 n_layer_nodes = 96
n = 6 n_layer_nodes = 192
n = 7 n_layer_nodes = 192
n = 8 n_layer_nodes = 192
n = 9 n_layer_nodes = 1
n = 10 n_layer_nodes = 192
n = 11 n_layer_nodes = 192
n = 12 n_layer_nodes = 224
n = 13 n_layer_nodes = 224
n = 14 n_layer_nodes = 224
n = 15 n_layer_nodes = 1
n = 16 n_layer_nodes = 224
n = 17 n_layer_nodes = 224
n = 18 n_layer_nodes = 224
n = 19 n_layer_nodes = 224
n = 20 n_layer_nodes = 224
n = 21 n_layer_nodes = 1
n = 22 n_layer_nodes = 224
n = 23 n_layer_nodes = 224
n = 24 n_layer_nodes = 256
n = 25 n_layer_nodes = 256
n = 26 n_layer_nodes = 256
n = 27 n_layer_nodes = 1
n = 28 n_layer_nodes = 256
n = 29 n_layer_nodes = 256
n = 30 n_layer_nodes = 256
n = 31 n_layer_nodes = 256
n = 32 n_layer_nodes = 256
n = 33 n_layer_nodes = 1
n = 34 n_layer_nodes = 256
n = 35 n_layer_nodes = 256
n = 36 n_layer_nodes = 256
n = 37 n_layer_nodes = 256
n = 38 n_layer_nodes = 256
n = 39 n_layer_nodes = 1
n = 40 n_layer_nodes = 256
n = 41 n_layer_nodes = 256
n = 42 n_layer_nodes = 3

frac_correct = 0.882261 frac_unsure = 0.0114108 frac_incorrect = 0.106328

frac_nonface_correct = 0.994329
frac_nonface_incorrect = 0.00567108

frac_male_correct = 0.835507
frac_male_unsure = 0.0166667
frac_male_incorrect = 0.147826

frac_female_correct = 0.844147
frac_female_unsure = 0.0148096
frac_female_incorrect = 0.141044

Confusion matrix:

1052	3	4	0	
84	1153	120	23	
68	132	1197	21	
0	0	0	0	


male_score_threshold = 0.52 female_score_threshold = 0.52

*************************** 
As of 9/19/16, 6 conv-layer model 2u with batch normalization, only 2 FC
layers and reduced regularization yields gender classification performance
very close to model 2t!
***************************

------------------------------------------------------------------- 
*. Sun, Sep 19, Titan 3, Facenet model 2u, 6 conv layers,
conv3a+conv3b+conv4a+conv4b, 2 FC layers, batch normalization, L2 
reg param = 0.0010.  No dropout in first 2 conv layers.

/data/caffe/faces/trained_models/Sep18_2u_T3/train_iter_168000.caffemodel
caffe validation accuracy asymptotes to 0.918; no significant overfitting
frac_correct = 0.875714 frac_unsure = 0.017644 frac_incorrect = 0.106642

frac_nonface_correct = 0.995265
frac_nonface_incorrect = 0.00473485

frac_male_correct = 0.83913
frac_male_unsure = 0.0217391
frac_male_incorrect = 0.13913

frac_female_correct = 0.822285
frac_female_unsure = 0.0267983
frac_female_incorrect = 0.150917

Confusion matrix:

1052	2	5	0	
82	1158	110	30	
68	146	1166	38	
0	0	0	0	

These Titan3 results for model 2u are worse than those from Titan1 with
less L2 regularization.

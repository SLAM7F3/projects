===========================================================================
Notes related to Davis King's DLIB library and helpful hints
===========================================================================
Last updated on 8/2/13; 8/12/13; 10/17/13; 6/1/16
===========================================================================

*.  As of June 2016, we can download dlib from github:

	git clone https://github.com/davisking/dlib.git

Follow Davis' instructions for compiling dlib's example programs:

	The simplest way to compile the example programs is to use
	CMake. You can do this by typing the following on the command line.

	cd examples
	mkdir build
	cd build
	cmake ..
	cmake --build . --config Release


Along the way, Davis' all/source.cpp will be compiled and incorporated into
static library ./examples/build/dlib_build/libdlib.a . Copy libdlib.a into
/usr/local/lib.  We then do NOT need to

#include dlib/all/source.cpp 

in any of our .cc files.

-----------------------------------------------------------------
cat /proc/cpuinfo | grep processor | wc -l

-----------------------------------------------------------------

Davis' trick for fast file transfer from remote to local machine:

On remote machine:

tar -cf - *.jpg | nc -l 12346 -w 30


(-w 30 is a 30 second timeout - i.e. this command will terminate the tar job
if there's 30 seconds of inactivity)


On local machine:

nc 172.25.183.161 12346 -w 5 | tar -xf -



where 172.25.183.161 = IP for "remote" machine


Davis says this method for file transfer involves no checking, handshaking,
etc and so goes very fast.


=========================================================================
Notes from conversations with Davis about multi-feature fusion
=========================================================================

Davis strongly recommends using his random projection hash dlib class in
order to generate "pseudo-voronoi" cells for vector quantization.  See
scan_image_pyramid_tools.h at line 62 for an example of how to instantiate
a projection hash object:

Look at "Algorithms" heading on LHS of Dlib C++ library webpage.  
create_random_projection_hash and projection_hash are listed on RHS of this
page under "Hashing" sub-category.


        projection_hash phash = create_random_projection_hash(samps, bits);

Take a look also at line 178 in object_detector_ex.cpp in examples
subdirectory.

Davis then says that we can use phash as a function.  We need to pass a
vector (as a dlib matrix) object to phash.  It should return an integer ID
corresponding to the Voronoi cell in which the vector lands.

Davis advocates running hashed "words" as inputs to LDA in order to
intelligently cluster features into image topics.  

Davis also says that we should NOT concatenate different feature vectors
into a single feature vector before quantization.  Instead, perform LDA on
concatenated vocab words.  

*.  Look into "Large margin nearest neighbor" wikipedia


*.  Look into P. Felzenszwalb: Efficent graph-based image segmentation
http://cs.brown.edu/~pff/segment/

namespace dlib
{

// ----------------------------------------------------------------------------------------

    template <
        typename in_image_type,
        typename out_image_type
        >
    void segment_image (
        const in_image_type& in_img,
        out_image_type& out_img,
        const double k = 200,
        const unsigned long min_size = 10
    );
    /*!
        requires
            - in_image_type  == an implementation of array2d/array2d_kernel_abstract.h
            - out_image_type == an implementation of array2d/array2d_kernel_abstract.h
            - in_image_type::type  == Any pixel type with a pixel_traits specialization or a
              dlib matrix object representing a row or column vector.
            - out_image_type::type == unsigned integer type 
            - is_same_object(in_img, out_img) == false
        ensures
            - Attempts to segment in_img into regions which have some visual consistency to
              them.  In particular, this function implements the algorithm described in the
              paper: Efficient Graph-Based Image Segmentation by Felzenszwalb and Huttenlocher.
            - #out_img.nr() == in_img.nr()
            - #out_img.nc() == in_img.nc()
            - for all valid r and c:
                - #out_img[r][c] == an integer value indicating the identity of the segment
                  containing the pixel in_img[r][c].  
            - The k parameter is a measure used to influence how large the segment regions
              will be.  Larger k generally results in larger segments being produced.  For
              a deeper discussion of the k parameter you should consult the above
              referenced paper.
            - min_size is a lower bound on the size of the output segments.  That is, it is
              guaranteed that all output segments will have at least min_size pixels in
              them (unless the whole image contains fewer than min_size pixels, in this
              case the entire image will be put into a single segment).
    !*/

// ----------------------------------------------------------------------------------------
// ----------------------------------------------------------------------------------------
// ----------------------------------------------------------------------------------------

    template <
        typename in_image_type,
        typename EXP
        >
    void find_candidate_object_locations (
        const in_image_type& in_img,
        std::vector<rectangle>& rects,
        const matrix_exp<EXP>& kvals = linspace(50, 200, 3),
        const unsigned long min_size = 20,
        const unsigned long max_merging_iterations = 50
    );
    /*!
        requires
            - is_vector(kvals) == true
            - kvals.size() > 0
        ensures
            - This function takes an input image and generates a set of
candidate
              rectangles which are expected to bound any objects in the image.  It does
              this by running a version of the segment_image() routine on the image and
              then reports rectangles containing each of the segments as well as rectangles
              containing unions of adjacent segments.  The basic idea is described in the
              paper: 
                  Segmentation as Selective Search for Object Recognition by Koen E. A. van de Sande, et al.
              Note that this function deviates from what is described in the paper slightly. 
              See the code for details.
            - The basic segmentation is performed kvals.size() times, each time with the k
              parameter (see segment_image() and the Felzenszwalb paper for details on k)
              set to a different value from kvals.   
            - All the rectangles output by this function will have an area >= min_size.
            - There are max_merging_iterations rounds of neighboring blob merging.
              Therefore, this parameter has some effect on the number of output rectangles
              you get, with larger values of the parameter giving more output rectangles.
            - This function appends the output rectangles into #rects.  This means that any
              rectangles in rects before this function was called will still be in there
              after it terminates.  Note further that #rects will not contain any duplicate
              rectangles.  That is, for all valid i and j where i != j it will be true
              that:
                - #rects[i] != rects[j]
    !*/

-----------------------------------------------------------------

On 10/7/13, Davis recommended that we check out his
assignment_learning_ex.cpp in order to perform feature-aided
tracking of human faces within video clips.

Davis further suggested that we employ the "roc_c1_trainer" in order to 
force a desired level of performance for positive or negative labeling.
For example in model_selection_ex.cpp, Davis started manipulating the
following line in order to demonstrate how to work with a 
roc_c1_trainer object:


// Finally, do 10 fold cross validation and then check if the results
// are the best we have seen so far.

matrix<double> result = cross_validate_trainer(roc_c1_trainer(trainer,0.95), samples, labels, 10);
cout << "gamma: " << setw(11) << gamma << "  nu: " << setw(11) << nu <<  "  cross validation accuracy: " << result;




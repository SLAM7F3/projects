==========================================================================
Deep Q learning notes for Breakout
==========================================================================
Last updated on 12/11/16; 12/12/16; 12/13/16; 12/14/16
==========================================================================

Working with breakout ROM obtained from
http://www.atarimania.com/rom_collection_archive_atari_2600_roms.html which
returns n_lives information.

Random play results:
-------------------

reward = ?
Median reward +/- quatile width = ?

n_frames / episode = ?
Median nframes/episode +/- quartile width = ?

-------------------------------------------------------------
TERMINATED EXPERIMENTS
-------------------------------------------------------------

1.  TM:  H1 = 128, H2 = 32

Sun Dec 11 17:14:16 2016
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 5000
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 4
   n_weights = 644224 (FC)
base_learning_rate = 0.003; batch_size = 1; n_max_episodes = 3000
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 15000
Number random samples drawn from replay memory Nd = 16
Discount factor gamma = 0.99
Epsilon time constant = 200; minimum epsilon = 0.1
Learning rate decrease period = 1000 episodes
Old weights period = 32
Discard zero reward frac = 0.95
Use big states flag = 1
Frame skip = 3
1 big state = n_screen_states = 2
nn_update_frame_period = 500

2.  TM:  Repeat expt 1

3.  TM:  Repeat expt 1

Expts 1 - 3 ran for approx 12 hours on Sunday night for approx 1200
episodes.  In all 3 cases, nframes/episode slowly but steadily rose from
1000 to roughly 1500.  Epsilon asymptoted to 0.1 around episode 1000.  All
weight distributions move fairly slowly.  So we should probably experiment
with even larger base learning rate than 0.003.  

4.  Titan 1:  H1 = 256, H2 = 0

Sun Dec 11 17:01:22 2016
Neural net params:
   n_layers = 3
   layer = 0 n_nodes = 5000
   layer = 1 n_nodes = 256
   layer = 2 n_nodes = 4
   n_weights = 1281024 (FC)
base_learning_rate = 0.003; batch_size = 1; n_max_episodes = 3000
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 15000
Number random samples drawn from replay memory Nd = 16
Discount factor gamma = 0.99
Epsilon time constant = 200; minimum epsilon = 0.1
Learning rate decrease period = 1000 episodes
Old weights period = 32
Discard zero reward frac = 0.95
Use big states flag = 1
Frame skip = 3
1 big state = n_screen_states = 2
nn_update_frame_period = 500

5.  Titan 1: Repeat expt 4
 
6.  Titan 1: Repeat expt 4

Expts 4 - 6 ran for approx 12 hours on Sunday night for approx 700 episodes
before they almost certainly ran out of memory due to some memory leak.  In
all 3 cases, nframes/episode very slowly rose from 1000 to roughly 1200.
Epsilon asymptoted to 0.1 around episode 1000.  All weight distributions
move slowly.  Conclusion: Working with 2 hidden layers is probably
better than 1

7.  Titan3:  H1 = 128, H2 = 64

Sun Dec 11 17:12:25 2016
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 5000
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 64
   layer = 3 n_nodes = 4
   n_weights = 648448 (FC)
base_learning_rate = 0.003; batch_size = 1; n_max_episodes = 3000
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 15000
Number random samples drawn from replay memory Nd = 16
Discount factor gamma = 0.99
Epsilon time constant = 200; minimum epsilon = 0.1
Learning rate decrease period = 1000 episodes
Old weights period = 32
Discard zero reward frac = 0.95
Use big states flag = 1
Frame skip = 3
1 big state = n_screen_states = 2
nn_update_frame_period = 500

8.  Titan3:  Repeat expt 7

9.  Titan3:  Repeat expt 7

Expts 4 - 6 ran for approx 7 hours on Sunday night for approx 700
episodes.  In all 3 cases, nframes/episode very slowly rose from 1000 to
roughly 1200.  Epsilon asymptoted to 0.1 around episode 1000.  All weight
distributions move very slowly.  Conclusion: Working with 2 hidden layers
is probably better than 1

10.  m6700:  H1 = 64, H2 = 16

Sat Dec 10 23:19:46 2016
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 5000
   layer = 1 n_nodes = 64
   layer = 2 n_nodes = 16
   layer = 3 n_nodes = 4
   n_weights = 321088 (FC)
base_learning_rate = 0.0003; batch_size = 1; n_max_episodes = 6000
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 15000
Number random samples drawn from replay memory Nd = 16
Discount factor gamma = 0.99
Epsilon time constant = 400; minimum epsilon = 0.1
Learning rate decrease period = 1000 episodes
Old weights period = 32
Discard zero reward frac = 0.95
Use big states flag = 1
Frame skip = 3
1 big state = n_screen_states = 2
nn_update_frame_period = 100

TERMINATED

11.  m6700:  repeat expt 10

12.  m6700

Sun Dec 11 14:37:08 2016
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 5000
   layer = 1 n_nodes = 64
   layer = 2 n_nodes = 16
   layer = 3 n_nodes = 4
   n_weights = 321088 (FC)
base_learning_rate = 0.0003; batch_size = 1; n_max_episodes = 3000
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 15000
Number random samples drawn from replay memory Nd = 16
Discount factor gamma = 0.99
Epsilon time constant = 200; minimum epsilon = 0.1
Learning rate decrease period = 1000 episodes
Old weights period = 32
Discard zero reward frac = 0.95
Use big states flag = 1
Frame skip = 3
1 big state = n_screen_states = 2
nn_update_frame_period = 500

13. m6700:  repeat of expt 12

14.  Titan1:

Mon Dec 12 07:46:57 2016
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 5000
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 4
   n_weights = 644224 (FC)
base_learning_rate = 0.003; batch_size = 1; n_max_episodes = 3000
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 15000
Number random samples drawn from replay memory Nd = 16
Discount factor gamma = 0.99
Epsilon time constant = 200; minimum epsilon = 0.1
Learning rate decrease period = 1000 episodes
Old weights period = 32
Discard zero reward frac = 0.85
Use big states flag = 1
Frame skip = 3
1 big state = n_screen_states = 2
nn_update_frame_period = 500

15.  Titan1: n_screen_states = 1; tau = 300; 

Mon Dec 12 08:16:46 2016
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 2500
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 4
   n_weights = 324224 (FC)
base_learning_rate = 0.003; batch_size = 1; n_max_episodes = 3000
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 15000
Number random samples drawn from replay memory Nd = 16
Discount factor gamma = 0.99
Epsilon time constant = 300; minimum epsilon = 0.1
Learning rate decrease period = 1000 episodes
Old weights period = 32
Discard zero reward frac = 0.85
Use big states flag = 0
Frame skip = 3
1 big state = n_screen_states = 1
nn_update_frame_period = 500

16.  Titan1:  Repeat expt 15

17.  Titan 3:

Mon Dec 12 08:20:19 2016
Neural net params:
   n_layers = 5
   layer = 0 n_nodes = 2500
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 8
   layer = 4 n_nodes = 4
   n_weights = 324384 (FC)
base_learning_rate = 0.003; batch_size = 1; n_max_episodes = 3000
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 15000
Number random samples drawn from replay memory Nd = 16
Discount factor gamma = 0.99
Epsilon time constant = 300; minimum epsilon = 0.1
Learning rate decrease period = 1000 episodes
Old weights period = 32
Discard zero reward frac = 0.85
Use big states flag = 0
Frame skip = 3
1 big state = n_screen_states = 1
nn_update_frame_period = 500

18.  Titan 3: Repeat of expt 17
	Terminated because metafiles stopped being written after 9:30 am

19.  Titan 3: Repeat of expt 17
	Terminated because metafiles stopped being written after 9:30 am

20.  Titan 3
Mon Dec 12 12:00:16 2016
Neural net params:
   n_layers = 5
   layer = 0 n_nodes = 2500
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 8
   layer = 4 n_nodes = 4
   n_weights = 324384 (FC)
base_learning_rate = 0.003; batch_size = 1; n_max_episodes = 3000
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 15000
Number random samples drawn from replay memory Nd = 16
Discount factor gamma = 0.99
Epsilon time constant = 300; minimum epsilon = 0.1
Learning rate decrease period = 1000 episodes
Old weights period = 32
Discard zero reward frac = 0.85
Use big states flag = 0
Frame skip = 3
1 big state = n_screen_states = 1
nn_update_frame_period = 500

21.  Titan 3: repeat expt 20

22.  Titan 1:  Try to write out quasi-random weight values; 3 hidden layers

Mon Dec 12 12:35:01 2016
Neural net params:
   n_layers = 5
   layer = 0 n_nodes = 2500
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 16
   layer = 4 n_nodes = 4
   n_weights = 324672 (FC)
base_learning_rate = 0.003; batch_size = 1; n_max_episodes = 3000
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 15000
Number random samples drawn from replay memory Nd = 16
Discount factor gamma = 0.99
Epsilon time constant = 400; minimum epsilon = 0.1
Learning rate decrease period = 1000 episodes
Old weights period = 32
Discard zero reward frac = 0.85
Use big states flag = 0
Frame skip = 3
1 big state = n_screen_states = 1
nn_update_frame_period = 500

23.  Titan 1:  Repeat of expt 22 

24.  Titan 1:  Repeat of expt 22 

25.  Titan 3:  

Mon Dec 12 15:19:25 2016
Neural net params:
   n_layers = 5
   layer = 0 n_nodes = 2500
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 16
   layer = 4 n_nodes = 4
   n_weights = 324672 (FC)
base_learning_rate = 0.003; batch_size = 1; n_max_episodes = 3000
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 15000
Number random samples drawn from replay memory Nd = 16
Discount factor gamma = 0.99
Epsilon time constant = 400; minimum epsilon = 0.1
Learning rate decrease period = 1000 episodes
Old weights period = 32
Discard zero reward frac = 0.85
Use big states flag = 0
Frame skip = 3
1 big state = n_screen_states = 1
nn_update_frame_period = 250

After O(1K) episodes, reward shows no sign of increasing.  Frames/episode
perhaps increases slightly from 1K --> 1.2K.  H3 weights contnue to evolve
while other layers have stopped evolving.  Very small losses.

26.  Titan 3:  Repeat expt 25

After O(1K) episodes, reward has decreased and nframes/episode shows no
signs of increasing.

27.  Titan 3:  Repeat expt 25

After O(1K) episodes, reward has increased slightly from 1 to 1.8.
nframes/episodes increased from 1K to 1.2K.  But weight coeffs have all
frozen.

28.  Titan 1:

Mon Dec 12 15:21:29 2016
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 2500
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 4
   n_weights = 324224 (FC)
base_learning_rate = 0.003; batch_size = 1; n_max_episodes = 3000
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 15000
Number random samples drawn from replay memory Nd = 16
Discount factor gamma = 0.99
Epsilon time constant = 400; minimum epsilon = 0.1
Learning rate decrease period = 1000 episodes
Old weights period = 32
Discard zero reward frac = 0.85
Use big states flag = 0
Frame skip = 3
1 big state = n_screen_states = 1
nn_update_frame_period = 250

After O(1K) episodes, reward has decreased and nepisodes/frame has remained
around 1K.  

29.  Titan 1:  Repeat expt 28

After O(1K) episdes, reward has remained basically around 1.
nepisodes/frame has remained around 1K.

30.  Titan 1:  Repeat expt 28

After O(1K) episdes, reward has remained basically around 1.
nepisodes/frame has remained around 1K.


31.  TM: H1 = 128, H2 = 64, H3 = 32

Mon Dec 12 15:38:04 2016
Neural net params:
   n_layers = 5
   layer = 0 n_nodes = 2500
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 64
   layer = 3 n_nodes = 32
   layer = 4 n_nodes = 4
   n_weights = 330368 (FC)
base_learning_rate = 0.003; batch_size = 1; n_max_episodes = 3000
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 15000
Number random samples drawn from replay memory Nd = 16
Discount factor gamma = 0.99
Epsilon time constant = 400; minimum epsilon = 0.1
Learning rate decrease period = 1000 episodes
Old weights period = 32
Discard zero reward frac = 0.85
Use big states flag = 0
Frame skip = 3
1 big state = n_screen_states = 1
nn_update_frame_period = 250

After O(700) episodes, reward has decreased from 1 to 0.5 and
nframes/episode has stayed fixed at 1K.  H3 layer coeffs still somewhat
evolving, but other hidden layer coeffs have frozen.

32.  TM:  repeat of expt 31

After 700 episodes, no significant change in reward or nframes/episode.

33.  TM:  repeat of expt 31

After 700 episodes, reward and nframes/episode have decreased.

34.  m6700:  H1 = 64, H2 = 128

35.  m6700 repeat of expt 34

36.  m6700 repeat of expt 34

As of 4 am on Tues Dec 30, O(400) episodes have been played for expts 34 -
36 on m6700.  Reward is no better than for random play.  But eps is roughly
0.5.  Weights continue to evolve.

37:  Titan 3:  

Tue Dec 13 08:39:19 2016
Neural net params:
   n_layers = 3
   layer = 0 n_nodes = 2500
   layer = 1 n_nodes = 256
   layer = 2 n_nodes = 4
   n_weights = 641024 (FC)
base_learning_rate = 0.01; batch_size = 1
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 15000
Number random samples drawn from replay memory Nd = 16
Discount factor gamma = 0.99
Epsilon time constant = 800; minimum epsilon = 0.1
Learning rate decrease period = 1000 episodes
Old weights period = 32
Discard zero reward frac = 0.95
Use big states flag = 0
Frame skip = 3
1 big state = n_screen_states = 1
nn_update_frame_period = 100
n_max_episodes = 30000

38:  Titan 3:  Repeat expt 34

39:  Titan 3:  Repeat expt 34

40:  Titan 1:  

Tue Dec 13 08:43:18 2016
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 2500
   layer = 1 n_nodes = 256
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 4
   n_weights = 648320 (FC)
base_learning_rate = 0.01; batch_size = 1
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 15000
Number random samples drawn from replay memory Nd = 16
Discount factor gamma = 0.99
Epsilon time constant = 800; minimum epsilon = 0.1
Learning rate decrease period = 1000 episodes
Old weights period = 32
Discard zero reward frac = 0.95
Use big states flag = 0
Frame skip = 3
1 big state = n_screen_states = 1
nn_update_frame_period = 100
n_max_episodes = 30000

41:  Titan 1:  Repeat expt 40

42:  Titan 1:  Repeat expt 40

43:  TM:  

Tue Dec 13 15:30:46 2016
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 2500
   layer = 1 n_nodes = 256
   layer = 2 n_nodes = 64
   layer = 3 n_nodes = 4
   n_weights = 656640 (FC)
base_learning_rate = 0.01; batch_size = 1
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 15000
Number random samples drawn from replay memory Nd = 16
Discount factor gamma = 0.99
Epsilon time constant = 800; minimum epsilon = 0.1
Learning rate decrease period = 1000 episodes
Old weights period = 32
Discard zero reward frac = 0.95
Use big states flag = 0
Frame skip = 3
1 big state = n_screen_states = 1
nn_update_frame_period = 100
n_max_episodes = 30000

44:  TM:  Repeat expt 43

45:  TM:  Repeat expt 43


-------------------------------------------------------------
RUNNING EXPERIMENTS
-------------------------------------------------------------

46.


TODO:

A.  Implement weight normalization 

B.  Implement data-dependent weight initialization

C.  Experiment with trying to extract ball and paddle posn and velocity and
forming much smaller input layer for a much smaller neural net


DONE:

A.  Fix memory leak!

B.  Revert to working with difference state rather than 1 big state
containing 2 screen states

C.  Fix exporting of each individual screen rather than every 3rd or 4th
one

G.  Trace individual weights' evolutions

F.  Perhaps experiment with 3 hidden layer neural network but only after
implementing batch normalization 


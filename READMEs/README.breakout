==========================================================================
Deep Q learning notes for Breakout
==========================================================================
Last updated on 12/14/16; 12/15/16; 12/16/16; 12/17/16
==========================================================================

Working with breakout ROM obtained from
http://www.atarimania.com/rom_collection_archive_atari_2600_roms.html which
returns n_lives information.

Random play results:
-------------------

Log10(loss) averages around -4
After 20K episodes, reward stays around 0.8
nframes/episode stays around 800

Best play results to date:
-------------------------

86.  Titan3:  H1 = 64, H2 = 128

Fri Dec 16 16:02:04 2016
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 2500
   layer = 1 n_nodes = 64
   layer = 2 n_nodes = 128
   layer = 3 n_nodes = 2
   n_weights = 168448 (FC)
base_learning_rate = 0.01; batch_size = 1
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.001
Replay memory capacity = 50000
Number random samples drawn from replay memory Nd = 16
Discount factor gamma = 0.99
Epsilon time constant = 1500; minimum epsilon = 0.1
n_actions = 2
Leaky ReLU small slope = 0.01
Learning rate decrease period = 1000 episodes
Old weights period = 32
Discard zero reward frac = 0.85
Use big states flag = 0
Frame skip = 1
1 big state = n_screen_states = 1
nn_update_frame_period = 50
nframes / epoch = 50000
n_max_epochs = 3000
n_max_episodes = 150000

After 30K episodes, reward --> 1.3 and nframes/episode --> 980.
Log10(loss) --> -1.2.  Layer 1 weights in range [-5,20] and weakly diverging.
Weak evidence of ball tracks in 0th layer weights.

87.  Titan3:  Repeat expt 86. 
After 35K episodes, reward --> 2.0 and nframes/episode --> 1050.
Log10(loss) --> -3.  Layer 1 weight dist lies in range [-3,2].  Some 0th
layer weights exhibit strong signs of ball tracks while others do not.  


-------------------------------------------------------------
TERMINATED EXPERIMENTS
-------------------------------------------------------------

1.  TM:  H1 = 128, H2 = 32

Sun Dec 11 17:14:16 2016
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 5000
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 4
   n_weights = 644224 (FC)
base_learning_rate = 0.003; batch_size = 1; n_max_episodes = 3000
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 15000
Number random samples drawn from replay memory Nd = 16
Discount factor gamma = 0.99
Epsilon time constant = 200; minimum epsilon = 0.1
Learning rate decrease period = 1000 episodes
Old weights period = 32
Discard zero reward frac = 0.95
Use big states flag = 1
Frame skip = 3
1 big state = n_screen_states = 2
nn_update_frame_period = 500

2.  TM:  Repeat expt 1

3.  TM:  Repeat expt 1

Expts 1 - 3 ran for approx 12 hours on Sunday night for approx 1200
episodes.  In all 3 cases, nframes/episode slowly but steadily rose from
1000 to roughly 1500.  Epsilon asymptoted to 0.1 around episode 1000.  All
weight distributions move fairly slowly.  So we should probably experiment
with even larger base learning rate than 0.003.  

4.  Titan 1:  H1 = 256, H2 = 0

Sun Dec 11 17:01:22 2016
Neural net params:
   n_layers = 3
   layer = 0 n_nodes = 5000
   layer = 1 n_nodes = 256
   layer = 2 n_nodes = 4
   n_weights = 1281024 (FC)
base_learning_rate = 0.003; batch_size = 1; n_max_episodes = 3000
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 15000
Number random samples drawn from replay memory Nd = 16
Discount factor gamma = 0.99
Epsilon time constant = 200; minimum epsilon = 0.1
Learning rate decrease period = 1000 episodes
Old weights period = 32
Discard zero reward frac = 0.95
Use big states flag = 1
Frame skip = 3
1 big state = n_screen_states = 2
nn_update_frame_period = 500

5.  Titan 1: Repeat expt 4
 
6.  Titan 1: Repeat expt 4

Expts 4 - 6 ran for approx 12 hours on Sunday night for approx 700 episodes
before they almost certainly ran out of memory due to some memory leak.  In
all 3 cases, nframes/episode very slowly rose from 1000 to roughly 1200.
Epsilon asymptoted to 0.1 around episode 1000.  All weight distributions
move slowly.  Conclusion: Working with 2 hidden layers is probably
better than 1

7.  Titan3:  H1 = 128, H2 = 64

Sun Dec 11 17:12:25 2016
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 5000
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 64
   layer = 3 n_nodes = 4
   n_weights = 648448 (FC)
base_learning_rate = 0.003; batch_size = 1; n_max_episodes = 3000
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 15000
Number random samples drawn from replay memory Nd = 16
Discount factor gamma = 0.99
Epsilon time constant = 200; minimum epsilon = 0.1
Learning rate decrease period = 1000 episodes
Old weights period = 32
Discard zero reward frac = 0.95
Use big states flag = 1
Frame skip = 3
1 big state = n_screen_states = 2
nn_update_frame_period = 500

8.  Titan3:  Repeat expt 7

9.  Titan3:  Repeat expt 7

Expts 4 - 6 ran for approx 7 hours on Sunday night for approx 700
episodes.  In all 3 cases, nframes/episode very slowly rose from 1000 to
roughly 1200.  Epsilon asymptoted to 0.1 around episode 1000.  All weight
distributions move very slowly.  Conclusion: Working with 2 hidden layers
is probably better than 1

10.  m6700:  H1 = 64, H2 = 16

Sat Dec 10 23:19:46 2016
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 5000
   layer = 1 n_nodes = 64
   layer = 2 n_nodes = 16
   layer = 3 n_nodes = 4
   n_weights = 321088 (FC)
base_learning_rate = 0.0003; batch_size = 1; n_max_episodes = 6000
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 15000
Number random samples drawn from replay memory Nd = 16
Discount factor gamma = 0.99
Epsilon time constant = 400; minimum epsilon = 0.1
Learning rate decrease period = 1000 episodes
Old weights period = 32
Discard zero reward frac = 0.95
Use big states flag = 1
Frame skip = 3
1 big state = n_screen_states = 2
nn_update_frame_period = 100

TERMINATED

11.  m6700:  repeat expt 10

12.  m6700

Sun Dec 11 14:37:08 2016
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 5000
   layer = 1 n_nodes = 64
   layer = 2 n_nodes = 16
   layer = 3 n_nodes = 4
   n_weights = 321088 (FC)
base_learning_rate = 0.0003; batch_size = 1; n_max_episodes = 3000
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 15000
Number random samples drawn from replay memory Nd = 16
Discount factor gamma = 0.99
Epsilon time constant = 200; minimum epsilon = 0.1
Learning rate decrease period = 1000 episodes
Old weights period = 32
Discard zero reward frac = 0.95
Use big states flag = 1
Frame skip = 3
1 big state = n_screen_states = 2
nn_update_frame_period = 500

13. m6700:  repeat of expt 12

14.  Titan1:

Mon Dec 12 07:46:57 2016
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 5000
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 4
   n_weights = 644224 (FC)
base_learning_rate = 0.003; batch_size = 1; n_max_episodes = 3000
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 15000
Number random samples drawn from replay memory Nd = 16
Discount factor gamma = 0.99
Epsilon time constant = 200; minimum epsilon = 0.1
Learning rate decrease period = 1000 episodes
Old weights period = 32
Discard zero reward frac = 0.85
Use big states flag = 1
Frame skip = 3
1 big state = n_screen_states = 2
nn_update_frame_period = 500

15.  Titan1: n_screen_states = 1; tau = 300; 

Mon Dec 12 08:16:46 2016
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 2500
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 4
   n_weights = 324224 (FC)
base_learning_rate = 0.003; batch_size = 1; n_max_episodes = 3000
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 15000
Number random samples drawn from replay memory Nd = 16
Discount factor gamma = 0.99
Epsilon time constant = 300; minimum epsilon = 0.1
Learning rate decrease period = 1000 episodes
Old weights period = 32
Discard zero reward frac = 0.85
Use big states flag = 0
Frame skip = 3
1 big state = n_screen_states = 1
nn_update_frame_period = 500

16.  Titan1:  Repeat expt 15

17.  Titan 3:

Mon Dec 12 08:20:19 2016
Neural net params:
   n_layers = 5
   layer = 0 n_nodes = 2500
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 8
   layer = 4 n_nodes = 4
   n_weights = 324384 (FC)
base_learning_rate = 0.003; batch_size = 1; n_max_episodes = 3000
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 15000
Number random samples drawn from replay memory Nd = 16
Discount factor gamma = 0.99
Epsilon time constant = 300; minimum epsilon = 0.1
Learning rate decrease period = 1000 episodes
Old weights period = 32
Discard zero reward frac = 0.85
Use big states flag = 0
Frame skip = 3
1 big state = n_screen_states = 1
nn_update_frame_period = 500

18.  Titan 3: Repeat of expt 17
	Terminated because metafiles stopped being written after 9:30 am

19.  Titan 3: Repeat of expt 17
	Terminated because metafiles stopped being written after 9:30 am

20.  Titan 3
Mon Dec 12 12:00:16 2016
Neural net params:
   n_layers = 5
   layer = 0 n_nodes = 2500
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 8
   layer = 4 n_nodes = 4
   n_weights = 324384 (FC)
base_learning_rate = 0.003; batch_size = 1; n_max_episodes = 3000
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 15000
Number random samples drawn from replay memory Nd = 16
Discount factor gamma = 0.99
Epsilon time constant = 300; minimum epsilon = 0.1
Learning rate decrease period = 1000 episodes
Old weights period = 32
Discard zero reward frac = 0.85
Use big states flag = 0
Frame skip = 3
1 big state = n_screen_states = 1
nn_update_frame_period = 500

21.  Titan 3: repeat expt 20

22.  Titan 1:  Try to write out quasi-random weight values; 3 hidden layers

Mon Dec 12 12:35:01 2016
Neural net params:
   n_layers = 5
   layer = 0 n_nodes = 2500
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 16
   layer = 4 n_nodes = 4
   n_weights = 324672 (FC)
base_learning_rate = 0.003; batch_size = 1; n_max_episodes = 3000
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 15000
Number random samples drawn from replay memory Nd = 16
Discount factor gamma = 0.99
Epsilon time constant = 400; minimum epsilon = 0.1
Learning rate decrease period = 1000 episodes
Old weights period = 32
Discard zero reward frac = 0.85
Use big states flag = 0
Frame skip = 3
1 big state = n_screen_states = 1
nn_update_frame_period = 500

23.  Titan 1:  Repeat of expt 22 

24.  Titan 1:  Repeat of expt 22 

25.  Titan 3:  

Mon Dec 12 15:19:25 2016
Neural net params:
   n_layers = 5
   layer = 0 n_nodes = 2500
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 16
   layer = 4 n_nodes = 4
   n_weights = 324672 (FC)
base_learning_rate = 0.003; batch_size = 1; n_max_episodes = 3000
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 15000
Number random samples drawn from replay memory Nd = 16
Discount factor gamma = 0.99
Epsilon time constant = 400; minimum epsilon = 0.1
Learning rate decrease period = 1000 episodes
Old weights period = 32
Discard zero reward frac = 0.85
Use big states flag = 0
Frame skip = 3
1 big state = n_screen_states = 1
nn_update_frame_period = 250

After O(1K) episodes, reward shows no sign of increasing.  Frames/episode
perhaps increases slightly from 1K --> 1.2K.  H3 weights contnue to evolve
while other layers have stopped evolving.  Very small losses.

26.  Titan 3:  Repeat expt 25

After O(1K) episodes, reward has decreased and nframes/episode shows no
signs of increasing.

27.  Titan 3:  Repeat expt 25

After O(1K) episodes, reward has increased slightly from 1 to 1.8.
nframes/episodes increased from 1K to 1.2K.  But weight coeffs have all
frozen.

28.  Titan 1:

Mon Dec 12 15:21:29 2016
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 2500
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 4
   n_weights = 324224 (FC)
base_learning_rate = 0.003; batch_size = 1; n_max_episodes = 3000
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 15000
Number random samples drawn from replay memory Nd = 16
Discount factor gamma = 0.99
Epsilon time constant = 400; minimum epsilon = 0.1
Learning rate decrease period = 1000 episodes
Old weights period = 32
Discard zero reward frac = 0.85
Use big states flag = 0
Frame skip = 3
1 big state = n_screen_states = 1
nn_update_frame_period = 250

After O(1K) episodes, reward has decreased and nepisodes/frame has remained
around 1K.  

29.  Titan 1:  Repeat expt 28

After O(1K) episdes, reward has remained basically around 1.
nepisodes/frame has remained around 1K.

30.  Titan 1:  Repeat expt 28

After O(1K) episdes, reward has remained basically around 1.
nepisodes/frame has remained around 1K.


31.  TM: H1 = 128, H2 = 64, H3 = 32

Mon Dec 12 15:38:04 2016
Neural net params:
   n_layers = 5
   layer = 0 n_nodes = 2500
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 64
   layer = 3 n_nodes = 32
   layer = 4 n_nodes = 4
   n_weights = 330368 (FC)
base_learning_rate = 0.003; batch_size = 1; n_max_episodes = 3000
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 15000
Number random samples drawn from replay memory Nd = 16
Discount factor gamma = 0.99
Epsilon time constant = 400; minimum epsilon = 0.1
Learning rate decrease period = 1000 episodes
Old weights period = 32
Discard zero reward frac = 0.85
Use big states flag = 0
Frame skip = 3
1 big state = n_screen_states = 1
nn_update_frame_period = 250

After O(700) episodes, reward has decreased from 1 to 0.5 and
nframes/episode has stayed fixed at 1K.  H3 layer coeffs still somewhat
evolving, but other hidden layer coeffs have frozen.

32.  TM:  repeat of expt 31

After 700 episodes, no significant change in reward or nframes/episode.

33.  TM:  repeat of expt 31

After 700 episodes, reward and nframes/episode have decreased.

34.  m6700:  H1 = 64, H2 = 128

35.  m6700 repeat of expt 34

36.  m6700 repeat of expt 34

As of 4 am on Tues Dec 30, O(400) episodes have been played for expts 34 -
36 on m6700.  Reward is no better than for random play.  But eps is roughly
0.5.  Weights continue to evolve.

37:  Titan 3:  

Tue Dec 13 08:39:19 2016
Neural net params:
   n_layers = 3
   layer = 0 n_nodes = 2500
   layer = 1 n_nodes = 256
   layer = 2 n_nodes = 4
   n_weights = 641024 (FC)
base_learning_rate = 0.01; batch_size = 1
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 15000
Number random samples drawn from replay memory Nd = 16
Discount factor gamma = 0.99
Epsilon time constant = 800; minimum epsilon = 0.1
Learning rate decrease period = 1000 episodes
Old weights period = 32
Discard zero reward frac = 0.95
Use big states flag = 0
Frame skip = 3
1 big state = n_screen_states = 1
nn_update_frame_period = 100
n_max_episodes = 30000

38:  Titan 3:  Repeat expt 34

39:  Titan 3:  Repeat expt 34

40:  Titan 1:  

Tue Dec 13 08:43:18 2016
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 2500
   layer = 1 n_nodes = 256
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 4
   n_weights = 648320 (FC)
base_learning_rate = 0.01; batch_size = 1
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 15000
Number random samples drawn from replay memory Nd = 16
Discount factor gamma = 0.99
Epsilon time constant = 800; minimum epsilon = 0.1
Learning rate decrease period = 1000 episodes
Old weights period = 32
Discard zero reward frac = 0.95
Use big states flag = 0
Frame skip = 3
1 big state = n_screen_states = 1
nn_update_frame_period = 100
n_max_episodes = 30000

41:  Titan 1:  Repeat expt 40

42:  Titan 1:  Repeat expt 40

43:  TM:  

Tue Dec 13 15:30:46 2016
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 2500
   layer = 1 n_nodes = 256
   layer = 2 n_nodes = 64
   layer = 3 n_nodes = 4
   n_weights = 656640 (FC)
base_learning_rate = 0.01; batch_size = 1
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 15000
Number random samples drawn from replay memory Nd = 16
Discount factor gamma = 0.99
Epsilon time constant = 800; minimum epsilon = 0.1
Learning rate decrease period = 1000 episodes
Old weights period = 32
Discard zero reward frac = 0.95
Use big states flag = 0
Frame skip = 3
1 big state = n_screen_states = 1
nn_update_frame_period = 100
n_max_episodes = 30000

44:  TM:  Repeat expt 43

45:  TM:  Repeat expt 43


46.  Titan 3: Fixed serious logic bugs in qbreak.cc: H1 = 128; H2 = 64

Wed Dec 14 07:00:54 2016
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 2500
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 64
   layer = 3 n_nodes = 3
   n_weights = 328384 (FC)
base_learning_rate = 0.01; batch_size = 1
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 15000
Number random samples drawn from replay memory Nd = 16
Discount factor gamma = 0.99
Epsilon time constant = 1000; minimum epsilon = 0.1
Learning rate decrease period = 1000 episodes
Old weights period = 32
Discard zero reward frac = 0.75
Use big states flag = 0
Frame skip = 3
1 big state = n_screen_states = 1
nn_update_frame_period = 50
n_max_episodes = 50000

47.  Titan 3: Repeat expt 46

48.  Titan 3: Repeat expt 46

49.  Titan 1: Fixed serious logic bugs in qbreak.cc: H1 = 256; H2 = 64

Wed Dec 14 07:11:41 2016
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 2500
   layer = 1 n_nodes = 256
   layer = 2 n_nodes = 64
   layer = 3 n_nodes = 3
   n_weights = 656576 (FC)
base_learning_rate = 0.01; batch_size = 1
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 15000
Number random samples drawn from replay memory Nd = 16
Discount factor gamma = 0.99
Epsilon time constant = 1000; minimum epsilon = 0.1
Learning rate decrease period = 1000 episodes
Old weights period = 32
Discard zero reward frac = 0.75
Use big states flag = 0
Frame skip = 3
1 big state = n_screen_states = 1
nn_update_frame_period = 50
n_max_episodes = 50000

50.  Titan 1: Repeat expt 49

51.  Titan 1: Repeat expt 49

52.  Titan 3:  0.01 L2 regularization + leaky ReLU + bug fixes

Thu Dec 15 07:29:51 2016
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 2500
   layer = 1 n_nodes = 256
   layer = 2 n_nodes = 64
   layer = 3 n_nodes = 3
   n_weights = 656576 (FC)
base_learning_rate = 0.01; batch_size = 1
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 50000
Number random samples drawn from replay memory Nd = 16
Discount factor gamma = 0.99
Epsilon time constant = 1000; minimum epsilon = 0.1
Learning rate decrease period = 1000 episodes
Old weights period = 32
Discard zero reward frac = 0.75
Use big states flag = 0
Frame skip = 3
1 big state = n_screen_states = 1
nn_update_frame_period = 50
nframes / epoch = 50000
n_max_epochs = 1000
n_max_episodes = 50000

53.  Titan 3:  Expt 52 with lambda = 1E-3

54.  Titan 3:  Expt 52 with lambda = 1E-4

59.  Titan 3:  Expt 52 with lambda = 1E-1

61.  Titan 3:  Expt 52 with lambda = 1


55.  Titan 1:  0.01 L2 regularization + leaky ReLU + bug fixes

Thu Dec 15 07:34:56 2016
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 2500
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 64
   layer = 3 n_nodes = 3
   n_weights = 328384 (FC)
base_learning_rate = 0.01; batch_size = 1
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 50000
Number random samples drawn from replay memory Nd = 16
Discount factor gamma = 0.99
Epsilon time constant = 1000; minimum epsilon = 0.1
Learning rate decrease period = 1000 episodes
Old weights period = 32
Discard zero reward frac = 0.75
Use big states flag = 0
Frame skip = 3
1 big state = n_screen_states = 1
nn_update_frame_period = 50
nframes / epoch = 50000
n_max_epochs = 1000
n_max_episodes = 50000

56.  Titan 1:  Expt 55 with lambda = 1E-3

57.  Titan 1:  Expt 55 with lambda = 1E-4

58.  Titan 1:  Expt 55 with lambda = 1E-1

60.  Titan 1:  Expt 55 with lambda = 1

65.  Titan 3:  H1 = 128, H2 = 128, lambda = 1E-3, leaky slope = 0.01

n_actions = 2
Thu Dec 15 12:36:15 2016
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 2500
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 128
   layer = 3 n_nodes = 2
   n_weights = 336640 (FC)
base_learning_rate = 0.01; batch_size = 1
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.001
Replay memory capacity = 50000
Number random samples drawn from replay memory Nd = 16
Discount factor gamma = 0.99
Epsilon time constant = 1000; minimum epsilon = 0.1
n_actions = 2
Leaky ReLU small slope = 0.01
Learning rate decrease period = 1000 episodes
Old weights period = 32
Discard zero reward frac = 0.75
Use big states flag = 0
Frame skip = 3
1 big state = n_screen_states = 1
nn_update_frame_period = 50
nframes / epoch = 50000
n_max_epochs = 1000
n_max_episodes = 50000

66.  Titan 1:  Repeat expt 65

67.  Titan 1:  Repeat expt 65

62.  Titan 1:  H1 = 128, H2 = 64, lambda = 1E-3, leaky slope = 0.01
n_actions = 2

Thu Dec 15 12:33:27 2016
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 2500
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 64
   layer = 3 n_nodes = 2
   n_weights = 328320 (FC)
base_learning_rate = 0.01; batch_size = 1
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.001
Replay memory capacity = 50000
Number random samples drawn from replay memory Nd = 16
Discount factor gamma = 0.99
Epsilon time constant = 1000; minimum epsilon = 0.1
n_actions = 2
Leaky ReLU small slope = 0.01
Learning rate decrease period = 1000 episodes
Old weights period = 32
Discard zero reward frac = 0.75
Use big states flag = 0
Frame skip = 3
1 big state = n_screen_states = 1
nn_update_frame_period = 50
nframes / epoch = 50000
n_max_epochs = 1000
n_max_episodes = 50000

Processed O(34K) episodes by 5 am on Fri Dec 16.  No increase in reward or
nframes/episode.   Trained weights remain bounded within interval [-1,1].
log10(loss) remains around -2.  Ball tracks clearly visible within several
of 0th-layer trained weights.

63.  Titan 1:  Repeat expt 62

64.  Titan 1:  Repeat expt 62

68.  Titan 3: H1 = 200, H2 = 0; n_actions = 2; discard 0 reward frac = 0.85

Thu Dec 15 15:56:05 2016
Neural net params:
   n_layers = 3
   layer = 0 n_nodes = 2500
   layer = 1 n_nodes = 200
   layer = 2 n_nodes = 2
   n_weights = 500400 (FC)
base_learning_rate = 0.01; batch_size = 1
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 50000
Number random samples drawn from replay memory Nd = 16
Discount factor gamma = 0.99
Epsilon time constant = 1000; minimum epsilon = 0.1
n_actions = 2
Leaky ReLU small slope = 0.01
Learning rate decrease period = 1000 episodes
Old weights period = 32
Discard zero reward frac = 0.85
Use big states flag = 0
Frame skip = 3
1 big state = n_screen_states = 1
nn_update_frame_period = 50
nframes / epoch = 50000
n_max_epochs = 1000
n_max_episodes = 50000

Processed O(12K) episodes by 5 am on Fri, Dec 16.  No increase in reward or
nframes/episode. Log10(loss) rises to 1.5 and then gradually declines
towards 0.  Weight coeffs increase up to +5.

69.  Titan 3: Repeat 68

70.  Titan 3: Repeat 68

Conclusion:  Working with just one large hidden layer as Kaparthy did for
pong doesn't look promising.

71.  TM: H1 = H2 = H3 = 64; n_actions = 2; discard 0 reward frac = 0.85

Neural net params:
   n_layers = 5
   layer = 0 n_nodes = 2500
   layer = 1 n_nodes = 64
   layer = 2 n_nodes = 64
   layer = 3 n_nodes = 64
   layer = 4 n_nodes = 2
   n_weights = 168320 (FC)
base_learning_rate = 0.01; batch_size = 1
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.001
Replay memory capacity = 50000
Number random samples drawn from replay memory Nd = 16
Discount factor gamma = 0.99
Epsilon time constant = 1000; minimum epsilon = 0.1
n_actions = 2
Leaky ReLU small slope = 0.01
Learning rate decrease period = 1000 episodes
Old weights period = 32
Discard zero reward frac = 0.85
Use big states flag = 0
Frame skip = 3
1 big state = n_screen_states = 1
nn_update_frame_period = 50
nframes / epoch = 50000
n_max_epochs = 1000
n_max_episodes = 50000

After 18K episodes: |Weight values| roughly stay bounded inside [-2,2].
Log10(loss) = -1.5.  0th layer weights exhibit faint tracks for ball.  But
reward and nframes/episode do not grow at all.

72.  TM:  Repeat of expt 71

73.  TM:  Repeat of expt 71

74.  m6700:  H1 = H2 = 32
Fri Dec 16 04:11:18 2016
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 2500
   layer = 1 n_nodes = 32
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 2
   n_weights = 81088 (FC)
base_learning_rate = 0.01; batch_size = 1
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.001
Replay memory capacity = 50000
Number random samples drawn from replay memory Nd = 16
Discount factor gamma = 0.99
Epsilon time constant = 1500; minimum epsilon = 0.1
n_actions = 2
Leaky ReLU small slope = 0.01
Learning rate decrease period = 1000 episodes
Old weights period = 32
Discard zero reward frac = 0.85
Use big states flag = 0
Frame skip = 3
1 big state = n_screen_states = 1
nn_update_frame_period = 50
nframes / epoch = 50000
n_max_epochs = 1000
n_max_episodes = 50000

75.  m6700: Repeat of expt 74

82.  TM:  H1 = 64, H2 = 32

Fri Dec 16 15:49:23 2016
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 2500
   layer = 1 n_nodes = 64
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 2
   n_weights = 162112 (FC)
base_learning_rate = 0.01; batch_size = 1
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.001
Replay memory capacity = 50000
Number random samples drawn from replay memory Nd = 16
Discount factor gamma = 0.99
Epsilon time constant = 1500; minimum epsilon = 0.1
n_actions = 2
Leaky ReLU small slope = 0.01
Learning rate decrease period = 1000 episodes
Old weights period = 32
Discard zero reward frac = 0.85
Use big states flag = 0
Frame skip = 1
1 big state = n_screen_states = 1
nn_update_frame_period = 50
nframes / epoch = 50000
n_max_epochs = 3000
n_max_episodes = 150000

After 35K episodes, reward --> 1.0 and nframes/episode --> 900.
0th layer weights exhibit stronger signs of ball tracks than H1 = 128, H2 =
34 expt results did.  Log10(loss) is around 0 and RISING!  Weight dist for
layer 1 lies in range [-60, 80] and diverging!!!

83.  TM:  Repeat expt 82

After 36K episodes, Log10(loss) --> -0.5 and increasing!  Reward --> 1.5
and nframes/episode --> 1000.  Weight dist for layer 1 lies in range
[-60,80] and diverging !!!  0th layer weights exhibit strong signs of ball
tracks.

Paddle gets pinned to right side of game board.


88.  TM:  H1 = 128, H2 = 32

Fri Dec 16 16:05:40 2016
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 2500
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 2
   n_weights = 324160 (FC)
base_learning_rate = 0.01; batch_size = 1
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.001
Replay memory capacity = 50000
Number random samples drawn from replay memory Nd = 16
Discount factor gamma = 0.99
Epsilon time constant = 1500; minimum epsilon = 0.1
n_actions = 2
Leaky ReLU small slope = 0.01
Learning rate decrease period = 1000 episodes
Old weights period = 32
Discard zero reward frac = 0.85
Use big states flag = 0
Frame skip = 1
1 big state = n_screen_states = 1
nn_update_frame_period = 50
nframes / epoch = 50000
n_max_epochs = 3000
n_max_episodes = 150000

After 20K episodes, reward --> 1.35 and nframes/episode --> 950
log10(loss) --> -2.3
0th-layer weights exhibit weak evidence of ball tracks
Paddle still stays pinned to right wall a lot.

89.  TM:  Repeat expt 88
After 20K episodes, reward --> 1 and nframes/episode --> 900
log10(loss) --> -1.8.  Layer 1 weight dist diverging past [-10,10] !!!

Conclusions: Ball is still being pinned way too often on right or left
walls.  

90.  m6700:  Random play
After 20K episodes, reward stays around 0.8
nframes/episode stays around 800
Log10(loss) averages around -4

92.  m6700 Random play
Similar results as for expt 90

76.  Titan 1: H1 = H2 = 32; Removed ALE randomness; Input state vector
mostly contains 0.

Fri Dec 16 12:21:25 2016
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 2500
   layer = 1 n_nodes = 32
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 2
   n_weights = 81088 (FC)
base_learning_rate = 0.01; batch_size = 1
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.001
Replay memory capacity = 50000
Number random samples drawn from replay memory Nd = 16
Discount factor gamma = 0.99
Epsilon time constant = 1500; minimum epsilon = 0.1
n_actions = 2
Leaky ReLU small slope = 0.01
Learning rate decrease period = 1000 episodes
Old weights period = 32
Discard zero reward frac = 0.85
Use big states flag = 0
Frame skip = 1
1 big state = n_screen_states = 1
nn_update_frame_period = 50
nframes / epoch = 50000
n_max_epochs = 1000
n_max_episodes = 50000

After 50K episodes, reward gyrates between 0.9 and 2.  nframes/episode
gyrates between 800 and 1100.  Layer 1 weights in range [-1,3].
log10(loss) around -3.  Fairly strong signs of ball tracks in 0th layer
weights.  Paddle gets pinned to right side wall.

77.  Titan 1:  Repeat expt 76

Qualitatively similar results to expt 76.  Except 0th layer weights exhibit
more randomness than those for expt 76.  

78.  Titan 1:  Repeat expt 76

Qualitatively similar results to expt 76.  Except 0th layer weights exhibit
weak signs of ball tracks.  

79.  Titan 3:  H1 = H2 = H3 = 32; No ALE randomness; sparse input state vec

Fri Dec 16 12:50:51 2016
Neural net params:
   n_layers = 5
   layer = 0 n_nodes = 2500
   layer = 1 n_nodes = 32
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 32
   layer = 4 n_nodes = 2
   n_weights = 82112 (FC)
base_learning_rate = 0.01; batch_size = 1
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.001
Replay memory capacity = 50000
Number random samples drawn from replay memory Nd = 16
Discount factor gamma = 0.99
Epsilon time constant = 1500; minimum epsilon = 0.1
n_actions = 2
Leaky ReLU small slope = 0.01
Learning rate decrease period = 1000 episodes
Old weights period = 32
Discard zero reward frac = 0.85
Use big states flag = 0
Frame skip = 1
1 big state = n_screen_states = 1
nn_update_frame_period = 50
nframes / epoch = 50000
n_max_epochs = 1000
n_max_episodes = 50000

After 50K episodes, reward --> 3.4 and nframes/episode --> 1350.  But both
have large gaps where they equaled 0.  Weight coeff dists are reasonable
for all 3 hidden layers.  Ball tracks in 0th layer weights are much less
distinct.  Log10(loss) --> -3 after large gyrations between [-2,-5].  

80.  Titan 3:  Repeat expt 79

Qualitatively similar results to expt 79.

81.  Titan 3:  Repeat expt 79

Qualitatively similar results to expt 79.

84.  Titan1:  H1 = 64, H2 = 64

Fri Dec 16 15:56:29 2016
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 2500
   layer = 1 n_nodes = 64
   layer = 2 n_nodes = 64
   layer = 3 n_nodes = 2
   n_weights = 164224 (FC)
base_learning_rate = 0.01; batch_size = 1
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.001
Replay memory capacity = 50000
Number random samples drawn from replay memory Nd = 16
Discount factor gamma = 0.99
Epsilon time constant = 1500; minimum epsilon = 0.1
n_actions = 2
Leaky ReLU small slope = 0.01
Learning rate decrease period = 1000 episodes
Old weights period = 32
Discard zero reward frac = 0.85
Use big states flag = 0
Frame skip = 1
1 big state = n_screen_states = 1
nn_update_frame_period = 50
nframes / epoch = 50000
n_max_epochs = 3000
n_max_episodes = 150000


After 50K episodes, reward --> 1.4  and nframes/episode --> 1000.
Some 0th layer weights show strong signs of ball traces while others do
not. log10(loss) --> 0.8 and rising!  Weight dist for layer 1 outside range
[-70,70] and diverging!  Definitely need more L2 regularization!  

Paddle gets pinned to right wall.

85.  Titan1:  Repeat expt 84

Qualitatively very similar results to expt 84.

86.  Titan3:  H1 = 64, H2 = 128

Fri Dec 16 16:02:04 2016
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 2500
   layer = 1 n_nodes = 64
   layer = 2 n_nodes = 128
   layer = 3 n_nodes = 2
   n_weights = 168448 (FC)
base_learning_rate = 0.01; batch_size = 1
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.001
Replay memory capacity = 50000
Number random samples drawn from replay memory Nd = 16
Discount factor gamma = 0.99
Epsilon time constant = 1500; minimum epsilon = 0.1
n_actions = 2
Leaky ReLU small slope = 0.01
Learning rate decrease period = 1000 episodes
Old weights period = 32
Discard zero reward frac = 0.85
Use big states flag = 0
Frame skip = 1
1 big state = n_screen_states = 1
nn_update_frame_period = 50
nframes / epoch = 50000
n_max_epochs = 3000
n_max_episodes = 150000

After 30K episodes, reward --> 1.3 and nframes/episode --> 980.
Log10(loss) --> -1.2.  Layer 1 weights in range [-5,20] and weakly diverging.
Weak evidence of ball tracks in 0th layer weights.

87.  Titan3:  Repeat expt 86. 
After 35K episodes, reward --> 2.0 and nframes/episode --> 1050.
Log10(loss) --> -3.  Layer 1 weight dist lies in range [-3,2].  Some 0th
layer weights exhibit strong signs of ball tracks while others do not.  



-------------------------------------------------------------
RUNNING EXPERIMENTS
-------------------------------------------------------------


===========================================================
TODO:

A.  Implement weight normalization 

B.  Implement data-dependent weight initialization

C.  Experiment with trying to extract ball and paddle posn and velocity and
forming much smaller input layer for a much smaller neural net


DONE:

A.  Fix memory leak!

B.  Revert to working with difference state rather than 1 big state
containing 2 screen states

C.  Fix exporting of each individual screen rather than every 3rd or 4th
one

G.  Trace individual weights' evolutions

F.  Perhaps experiment with 3 hidden layer neural network but only after
implementing batch normalization 


==========================================================================
Deep Q learning notes for Breakout
==========================================================================
Last updated on 12/10/16; 12/11/16; 12/12/16
==========================================================================

Working with breakout ROM obtained from
http://www.atarimania.com/rom_collection_archive_atari_2600_roms.html which
returns n_lives information.

Random play results:
-------------------

reward = ?
Median reward +/- quatile width = ?

n_frames / episode = ?
Median nframes/episode +/- quartile width = ?

----------------------
TERMINATED EXPERIMENTS
----------------------


1.  TM:  H1 = 128, H2 = 32

Sun Dec 11 17:14:16 2016
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 5000
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 4
   n_weights = 644224 (FC)
base_learning_rate = 0.003; batch_size = 1; n_max_episodes = 3000
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 15000
Number random samples drawn from replay memory Nd = 16
Discount factor gamma = 0.99
Epsilon time constant = 200; minimum epsilon = 0.1
Learning rate decrease period = 1000 episodes
Old weights period = 32
Discard zero reward frac = 0.95
Use big states flag = 1
Frame skip = 3
1 big state = n_screen_states = 2
nn_update_frame_period = 500

2.  TM:  Repeat expt 1

3.  TM:  Repeat expt 1

Expts 1 - 3 ran for approx 12 hours on Sunday night for approx 1200
episodes.  In all 3 cases, nframes/episode slowly but steadily rose from
1000 to roughly 1500.  Epsilon asymptoted to 0.1 around episode 1000.  All
weight distributions move fairly slowly.  So we should probably experiment
with even larger base learning rate than 0.003.  

4.  Titan 1:  H1 = 256, H2 = 0

Sun Dec 11 17:01:22 2016
Neural net params:
   n_layers = 3
   layer = 0 n_nodes = 5000
   layer = 1 n_nodes = 256
   layer = 2 n_nodes = 4
   n_weights = 1281024 (FC)
base_learning_rate = 0.003; batch_size = 1; n_max_episodes = 3000
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 15000
Number random samples drawn from replay memory Nd = 16
Discount factor gamma = 0.99
Epsilon time constant = 200; minimum epsilon = 0.1
Learning rate decrease period = 1000 episodes
Old weights period = 32
Discard zero reward frac = 0.95
Use big states flag = 1
Frame skip = 3
1 big state = n_screen_states = 2
nn_update_frame_period = 500

5.  Titan 1: Repeat expt 4
 
6.  Titan 1: Repeat expt 4


Expts 4 - 6 ran for approx 12 hours on Sunday night for approx 700 episodes
before they almost certainly ran out of memory due to some memory leak.  In
all 3 cases, nframes/episode very slowly rose from 1000 to roughly 1200.
Epsilon asymptoted to 0.1 around episode 1000.  All weight distributions
move slowly.  Conclusion: Working with 2 hidden layers is probably
better than 1

7.  Titan3:  H1 = 128, H2 = 64

Sun Dec 11 17:12:25 2016
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 5000
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 64
   layer = 3 n_nodes = 4
   n_weights = 648448 (FC)
base_learning_rate = 0.003; batch_size = 1; n_max_episodes = 3000
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 15000
Number random samples drawn from replay memory Nd = 16
Discount factor gamma = 0.99
Epsilon time constant = 200; minimum epsilon = 0.1
Learning rate decrease period = 1000 episodes
Old weights period = 32
Discard zero reward frac = 0.95
Use big states flag = 1
Frame skip = 3
1 big state = n_screen_states = 2
nn_update_frame_period = 500

8.  Titan3:  Repeat expt 7

9.  Titan3:  Repeat expt 7

Expts 4 - 6 ran for approx 7 hours on Sunday night for approx 700
episodes.  In all 3 cases, nframes/episode very slowly rose from 1000 to
roughly 1200.  Epsilon asymptoted to 0.1 around episode 1000.  All weight
distributions move very slowly.  Conclusion: Working with 2 hidden layers
is probably better than 1

10.  m6700:  H1 = 64, H2 = 16

Sat Dec 10 23:19:46 2016
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 5000
   layer = 1 n_nodes = 64
   layer = 2 n_nodes = 16
   layer = 3 n_nodes = 4
   n_weights = 321088 (FC)
base_learning_rate = 0.0003; batch_size = 1; n_max_episodes = 6000
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 15000
Number random samples drawn from replay memory Nd = 16
Discount factor gamma = 0.99
Epsilon time constant = 400; minimum epsilon = 0.1
Learning rate decrease period = 1000 episodes
Old weights period = 32
Discard zero reward frac = 0.95
Use big states flag = 1
Frame skip = 3
1 big state = n_screen_states = 2
nn_update_frame_period = 100

TERMINATED

11.  m6700:  repeat expt 10

18.  Titan 3: Repeat of expt 17
	Terminated because metafiles stopped being written after 9:30 am

19.  Titan 3: Repeat of expt 17
	Terminated because metafiles stopped being written after 9:30 am


--------------------
RUNNING EXPERIMENTS
--------------------

12.  m6700

Sun Dec 11 14:37:08 2016
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 5000
   layer = 1 n_nodes = 64
   layer = 2 n_nodes = 16
   layer = 3 n_nodes = 4
   n_weights = 321088 (FC)
base_learning_rate = 0.0003; batch_size = 1; n_max_episodes = 3000
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 15000
Number random samples drawn from replay memory Nd = 16
Discount factor gamma = 0.99
Epsilon time constant = 200; minimum epsilon = 0.1
Learning rate decrease period = 1000 episodes
Old weights period = 32
Discard zero reward frac = 0.95
Use big states flag = 1
Frame skip = 3
1 big state = n_screen_states = 2
nn_update_frame_period = 500

13. m6700:  repeat of expt 12

14.  Titan1:

Mon Dec 12 07:46:57 2016
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 5000
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 4
   n_weights = 644224 (FC)
base_learning_rate = 0.003; batch_size = 1; n_max_episodes = 3000
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 15000
Number random samples drawn from replay memory Nd = 16
Discount factor gamma = 0.99
Epsilon time constant = 200; minimum epsilon = 0.1
Learning rate decrease period = 1000 episodes
Old weights period = 32
Discard zero reward frac = 0.85
Use big states flag = 1
Frame skip = 3
1 big state = n_screen_states = 2
nn_update_frame_period = 500

15.  Titan1: n_screen_states = 1; tau = 300; 

Mon Dec 12 08:16:46 2016
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 2500
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 4
   n_weights = 324224 (FC)
base_learning_rate = 0.003; batch_size = 1; n_max_episodes = 3000
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 15000
Number random samples drawn from replay memory Nd = 16
Discount factor gamma = 0.99
Epsilon time constant = 300; minimum epsilon = 0.1
Learning rate decrease period = 1000 episodes
Old weights period = 32
Discard zero reward frac = 0.85
Use big states flag = 0
Frame skip = 3
1 big state = n_screen_states = 1
nn_update_frame_period = 500

16.  Titan1:  Repeat expt 15

17.  Titan 3:

Mon Dec 12 08:20:19 2016
Neural net params:
   n_layers = 5
   layer = 0 n_nodes = 2500
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 8
   layer = 4 n_nodes = 4
   n_weights = 324384 (FC)
base_learning_rate = 0.003; batch_size = 1; n_max_episodes = 3000
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 15000
Number random samples drawn from replay memory Nd = 16
Discount factor gamma = 0.99
Epsilon time constant = 300; minimum epsilon = 0.1
Learning rate decrease period = 1000 episodes
Old weights period = 32
Discard zero reward frac = 0.85
Use big states flag = 0
Frame skip = 3
1 big state = n_screen_states = 1
nn_update_frame_period = 500


20.  Titan 3
Mon Dec 12 12:00:16 2016
Neural net params:
   n_layers = 5
   layer = 0 n_nodes = 2500
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 8
   layer = 4 n_nodes = 4
   n_weights = 324384 (FC)
base_learning_rate = 0.003; batch_size = 1; n_max_episodes = 3000
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 15000
Number random samples drawn from replay memory Nd = 16
Discount factor gamma = 0.99
Epsilon time constant = 300; minimum epsilon = 0.1
Learning rate decrease period = 1000 episodes
Old weights period = 32
Discard zero reward frac = 0.85
Use big states flag = 0
Frame skip = 3
1 big state = n_screen_states = 1
nn_update_frame_period = 500

21.  Titan 3: repeat expt 20

TODO:

D.  Implement batch normalization

E.  Experiment with trying to extract ball and paddle posn and velocity and
forming much smaller input layer for a much smaller neural net

F.  Perhaps experiment with 3 hidden layer neural network but only after
implementing batch normalization 

G.  Trace individual weights' evolutions

DONE:

A.  Fix memory leak!

B.  Revert to working with difference state rather than 1 big state
containing 2 screen states

C.  Fix exporting of each individual screen rather than every 3rd or 4th
one

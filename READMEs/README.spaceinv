==========================================================================
Deep Q learning notes for Space Invaders
==========================================================================
Last updated on 12/4/16; 12/5/16; 12/6/16; 12/7/16
==========================================================================

Random play results:
-------------------

reward = 159.6 +/- 16.6
Median reward +/- quartile width = 158.3 +/- 10.7

n_frames / episode = 2132.3 +/- 670.7
Median nframes/episode +/- quartile width = 2057 +/- 375


BEST AGENT RESULTS SO FAR
-------------------------

46.  Tue Dec  6 15:38:41 2016 (H1 = 16, H2 = 32)
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 4717
   layer = 1 n_nodes = 16
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 6
   n_weights = 76176 (FC)
base_learning_rate = 0.0003; batch_size = 10; n_max_episodes = 10000
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 10
Discount factor gamma = 0.99 frame_skip = 1
Epsilon decay factor = 0.99; minimum epsilon = 0.05
n_anneal_steps = 6
Discard 95% of 0-reward entries from memory store    
Old weights period = 32

After O(5K) episodes:

Loss = -0.5 +/- 0.5
Reward --> 245
Nframes / episode = 3.2K !!
Gentle evolution of all weights


38.  Tue Dec  6 07:23:18 2016  (increase gamma)
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 4717
   layer = 1 n_nodes = 16
   layer = 2 n_nodes = 8
   layer = 3 n_nodes = 6
   n_weights = 75648 (FC)
base_learning_rate = 0.0003; batch_size = 10; n_max_episodes = 10000
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 10
Discount factor gamma = 0.99 frame_skip = 1
Epsilon decay factor = 0.99; minimum epsilon = 0.05
n_anneal_steps = 5
Discard 95% of 0-reward entries from memory store    
Old weights period = 32

After O(10K) frames:
   log10_loss = -0.5 +/- 1.25
   reward --> 250
   n_frames / episode --> (2.6 +/- 0.2)K
   gentle evolution of weights over O(4K) episodes

27.  H1 = 16; H2 = 8; 
    base learning rate = 3E-4; rms_decay = 0.9; batch size = 10
    solver = RMSPROP; frame_skip = 1
    old weights period = 10; min_epsilon = 0.025
    epsilon decay factor = 0.95
    replay memory capacity = 5 * 2000
    lambda = 1E-3
    Nd = 10
    Discard 95% of 0-reward entries from memory store   
    Resize ROI to capture mother ships
    Started on TM at 6:13 pm on Mon Dec 5

   log10_loss = -0.3 +/- 0.3
   reward --> 230 +/- 10
   n_frames / episode --> (2.7 +/- 0.2)K
   gentle evolution of weights over O(700) episodes

26.  H1 = 16; H2 = 8; 
    base learning rate = 3E-4; rms_decay = 0.9; batch size = 10
    solver = RMSPROP; frame_skip = 1
    old weights period = 10; min_epsilon = 0.025
    epsilon decay factor = 0.95
    replay memory capacity = 5 * 2000
    Nd = 10
    lambda = 1E-3
    Discard 90% of 0-reward entries from memory store   
    Resize ROI to capture mother ships
    Started on TM at 6:11 pm on Mon Dec 5

   log10_loss = 0 +/- 0.3
   reward --> 230 +/- 10
   n_frames / episode --> (2.4 +/- 0.2)K
   gentle evolution of weights over 800+ episodes


17.  H1 = 16; H2 = 8; 
    base learning rate = 3E-4; rms_decay = 0.9; batch size = 5
    solver = RMSPROP; frame_skip = 1
    old weights period = 10; min_epsilon = 0.025
    epsilon decay factor = 0.95
    replay memory capacity = 5 * 2000
    Nd = 10
    Discard 90% of 0-reward entries from memory store   
    Started on TM at 7:50 am on Mon Dec 5
    Mild evolution of weights over O(300) episodes

   After 700 episodes:

   log10_loss = -0.25 +/- 0.5
   reward --> 256
   n_frames / episode --> 3K
   gentle evolution of weights over 700 episodes

----------------------
TERMINATED EXPERIMENTS
----------------------

1.  H1 = 16; H2 = 8; 
    base learning rate = 1E-2; rms_decay = 0.9; batch size = 5
    solver = RMS_prop; frame_skip = 1
    old weights period = 10; min_epsilon = 0.025
    epsilon decay factor = 0.95
    Learning rate too high

2.  H1 = 16; H2 = 8; 
    base learning rate = 1E-3; rms_decay = 0.9; batch size = 5
    solver = RMS_prop; frame_skip = 1
    old weights period = 10; min_epsilon = 0.025
    epsilon decay factor = 0.95
    Long term reward comparable to chance

3.  H1 = 16; H2 = 8; 
    base learning rate = 1E-4; rms_decay = 0.9; batch size = 5
    solver = RMS_prop; frame_skip = 1
    old weights period = 10; min_epsilon = 0.025
    epsilon decay factor = 0.95
    Long term reward around 195 whereas chance is around 170.
    nframes/episode around 2400

4.  Random play
    Started on TM at 2:07 on Sun Dec 4
    Long term reward around 160; Frames per episodes around 2100

    Random reward = 159.6 +/- 16.6
    Median reward +/- quartile width = 158.3 +/- 10.7

    n_frames / episode = 2132.3 +/- 670.7
    Median nframes/episode +/- quartile width = 2057 +/- 375

5.  H1 = 16; H2 = 8; 
    base learning rate = 3E-3; rms_decay = 0.9; batch size = 5
    solver = RMS_prop; frame_skip = 1
    old weights period = 10; min_epsilon = 0.025
    epsilon decay factor = 0.95
    Started on TM at 2:27 pm on Sun Dec 4
    Long term reward decreases down to around 170

6.  H1 = 16; H2 = 8; 
    base learning rate = 3E-4; rms_decay = 0.9; batch size = 5
    solver = RMS_prop; frame_skip = 1
    old weights period = 10; min_epsilon = 0.025
    epsilon decay factor = 0.95
    Started on TM at 2:27 pm on Sun Dec 4
    Long term reward around 162

7.  H1 = 16; H2 = 8; 
    base learning rate = 3E-2; rms_decay = 0.9; batch size = 5
    solver = RMS_prop; frame_skip = 1
    old weights period = 10; min_epsilon = 0.025
    epsilon decay factor = 0.95
    Started on TM at 2:27 pm on Sun Dec 4
    Long term reward around 176; nframes/episode around 2285

8.  Random play
    Started on m6700 at 3:38 on Sun Dec 4
    Terminated

9.  Random play
    Started on m6700 at 8:16 on Sun Dec 4
    Terminated

10.  H1 = 16; H2 = 8; 
    base learning rate = 1E-2; rms_decay = 0.9; batch size = 5
    solver = SGD; frame_skip = 1
    old weights period = 10; min_epsilon = 0.025
    epsilon decay factor = 0.95
    replay memory capacity = 5 * 2000
    Nd = 5
    Discard 90% of 0-reward entries from memory store   
    Started on TM at 6:55 am on Mon Dec 5
    Minor weight evolution in first 80 episodes

11.  H1 = 16; H2 = 8; 
    base learning rate = 1E-2; rms_decay = 0.9; batch size = 5
    solver = RMSPROP; frame_skip = 1
    old weights period = 10; min_epsilon = 0.025
    epsilon decay factor = 0.95
    replay memory capacity = 5 * 2000
    Nd = 5
    Discard 90% of 0-reward entries from memory store   
    Started on TM at 7:05 am on Mon Dec 5
    Major weight evolution in first 80 episodes
    Divergent loss function

12.  H1 = 16; H2 = 8; 
    base learning rate = 1E-1; rms_decay = 0.9; batch size = 5
    solver = SGD; frame_skip = 1
    old weights period = 10; min_epsilon = 0.025
    epsilon decay factor = 0.95
    replay memory capacity = 5 * 2000
    Nd = 5
    Discard 90% of 0-reward entries from memory store   
    Started on TM at 6:55 am on Mon Dec 5
    Significant weight evolution in first 60 episodes


13.  H1 = 16; H2 = 8; 
    base learning rate = 1E-1; rms_decay = 0.9; batch size = 5
    solver = RMSPROP; frame_skip = 1
    old weights period = 10; min_epsilon = 0.025
    epsilon decay factor = 0.95
    replay memory capacity = 5 * 2000
    Nd = 5
    Discard 90% of 0-reward entries from memory store   
    Started on TM at 7:05 am on Mon Dec 5
    Major weight evolution in first 50 episodes

14.  H1 = 16; H2 = 8; 
    base learning rate = 1E-3; rms_decay = 0.9; batch size = 5
    solver = RMSPROP; frame_skip = 1
    old weights period = 10; min_epsilon = 0.025
    epsilon decay factor = 0.95
    replay memory capacity = 5 * 2000
    Nd = 5
    Discard 90% of 0-reward entries from memory store   
    Started on TM at 7:18 am on Mon Dec 5
    Gentle weight evolution in first 120 episodes
    Reward went up for 150 episodes.  Then it fell precipitously for next
    250 episodes!  Terminated!

16.  H1 = 16; H2 = 8; 
    base learning rate = 3E-3; rms_decay = 0.9; batch size = 5
    solver = RMSPROP; frame_skip = 1
    old weights period = 10; min_epsilon = 0.025
    epsilon decay factor = 0.95
    replay memory capacity = 5 * 2000
    Nd = 10
    Discard 90% of 0-reward entries from memory store   
    Divergent loss function
    Significant movement among weights over O(300) episodes
    Reward starts trending downwards after O(200) episodes.
    This base learning rate is very likely too high.  TERMINATED


17.  H1 = 16; H2 = 8; 
    base learning rate = 3E-4; rms_decay = 0.9; batch size = 5
    solver = RMSPROP; frame_skip = 1
    old weights period = 10; min_epsilon = 0.025
    epsilon decay factor = 0.95
    replay memory capacity = 5 * 2000
    Nd = 10
    Discard 90% of 0-reward entries from memory store   
    Started on TM at 7:50 am on Mon Dec 5
    Mild evolution of weights over O(300) episodes

   After 700 episodes:


   log10_loss = -0.25 +/- 0.5
   reward --> 256
   n_frames / episode --> 3K
   gentle evolution of weights over 700 episodes

18.  H1 = 16; H2 = 8; 
    base learning rate = 1E-4; rms_decay = 0.9; batch size = 5
    solver = RMSPROP; frame_skip = 1
    old weights period = 10; min_epsilon = 0.025
    epsilon decay factor = 0.95
    replay memory capacity = 5 * 2000
    Nd = 10
    Discard 90% of 0-reward entries from memory store   
    Started on TM at 8:01 am on Mon Dec 5
    Very little evolution of weights over 250 episodes.
    Reward increased for O(100) episodes to roughly random play value.
    But then it decreases for next 150 episodes!  TERMINATED.

19.  H1 = 16; H2 = 8; 
    base learning rate = 3E-3; rms_decay = 0.9; batch size = 5
    solver = RMSPROP; frame_skip = 1
    old weights period = 10; min_epsilon = 0.025
    epsilon decay factor = 0.95
    replay memory capacity = 5 * 2000
    Nd = 16
    Discard 85% of 0-reward entries from memory store   
    Started on TM at 11:30 am on Mon Dec 5
    Diverging loss function; dropping reward; TERMINATED

20.  H1 = 16; H2 = 8; 
    base learning rate = 1E-3; rms_decay = 0.9; batch size = 5
    solver = RMSPROP; frame_skip = 1
    old weights period = 10; min_epsilon = 0.025
    epsilon decay factor = 0.95
    replay memory capacity = 5 * 2000
    Nd = 16
    Discard 85% of 0-reward entries from memory store   
    Started on TM at 11:30 am on Mon Dec 5
    Reward after O(200) episodes is not nearly so good as that for expt 15
     and not much better than chance. TERMINATED.

21.  H1 = 16; H2 = 8; 
    base learning rate = 3E-4; rms_decay = 0.9; batch size = 5
    solver = RMSPROP; frame_skip = 1
    old weights period = 10; min_epsilon = 0.025
    epsilon decay factor = 0.95
    replay memory capacity = 5 * 2000
    Nd = 16
    Discard 85% of 0-reward entries from memory store   
    Started on TM at 11:30 am on Mon Dec 5
    Reward after O(200) episodes is not nearly so good as that for expt 15.
    TERMINATED.



15.  H1 = 16; H2 = 8; 
    base learning rate = 1E-3; rms_decay = 0.9; batch size = 5
    solver = RMSPROP; frame_skip = 1
    old weights period = 10; min_epsilon = 0.025
    epsilon decay factor = 0.95
    replay memory capacity = 5 * 2000
    Nd = 10
    Discard 90% of 0-reward entries from memory store   
    Started on TM at 7:47 am on Mon Dec 5
    Relatively gentle evolution of weights over O(300) episodes
    Results for expt 17 are better than those for expt 15.

17.  H1 = 16; H2 = 8; 
    base learning rate = 3E-4; rms_decay = 0.9; batch size = 5
    solver = RMSPROP; frame_skip = 1
    old weights period = 10; min_epsilon = 0.025
    epsilon decay factor = 0.95
    replay memory capacity = 5 * 2000
    Nd = 10
    Discard 90% of 0-reward entries from memory store   
    Started on TM at 7:50 am on Mon Dec 5
    Mile evolution of weights over O(300) episodes

22.  H1 = 16; H2 = 8; 
    base learning rate = 1E-3; rms_decay = 0.9; batch size = 5
    solver = RMSPROP; frame_skip = 1
    old weights period = 10; min_epsilon = 0.025
    epsilon decay factor = 0.95
    replay memory capacity = 5 * 2000
    Nd = 16
    Discard 90% of 0-reward entries from memory store   
    Started on TM at 3:22 pm on Mon Dec 5


23.  H1 = 16; H2 = 8; 
    base learning rate = 3E-4; rms_decay = 0.9; batch size = 5
    solver = RMSPROP; frame_skip = 1
    old weights period = 10; min_epsilon = 0.025
    epsilon decay factor = 0.95
    replay memory capacity = 5 * 2000
    Nd = 16
    Discard 90% of 0-reward entries from memory store   
    Started on TM at 3:23 pm on Mon Dec 5


24.  H1 = 24; H2 = 8; 
    base learning rate = 1E-3; rms_decay = 0.9; batch size = 5
    solver = RMSPROP; frame_skip = 1
    old weights period = 10; min_epsilon = 0.025
    epsilon decay factor = 0.95
    replay memory capacity = 5 * 2000
    Nd = 10
    Discard 90% of 0-reward entries from memory store   

25.  H1 = 24; H2 = 8; 
    base learning rate = 3E-4; rms_decay = 0.9; batch size = 5
    solver = RMSPROP; frame_skip = 1
    old weights period = 10; min_epsilon = 0.025
    epsilon decay factor = 0.95
    replay memory capacity = 5 * 2000
    Nd = 10
    Discard 90% of 0-reward entries from memory store   

27.  H1 = 16; H2 = 8; 
    base learning rate = 3E-4; rms_decay = 0.9; batch size = 10
    solver = RMSPROP; frame_skip = 1
    old weights period = 10; min_epsilon = 0.025
    epsilon decay factor = 0.95
    replay memory capacity = 5 * 2000
    Nd = 10
    Discard 95% of 0-reward entries from memory store   
    Resize ROI to capture mother ships
    Started on TM at 6:13 pm on Mon Dec 5

28.  H1 = 16; H2 = 8; 
    base learning rate = 3E-4; rms_decay = 0.9; batch size = 10
    solver = RMSPROP; frame_skip = 1
    old weights period = 10; min_epsilon = 0.025
    epsilon decay factor = 0.95
    replay memory capacity = 10 * 2000
    Nd = 10
    Discard 90% of 0-reward entries from memory store   
    Resize ROI to capture mother ships
    Started on TM at 6:18 pm on Mon Dec 5

29.  H1 = 16; H2 = 8; 
    base learning rate = 3E-4; rms_decay = 0.95; batch size = 10
    solver = RMSPROP; frame_skip = 1
    old weights period = 10; min_epsilon = 0.025
    epsilon decay factor = 0.95
    replay memory capacity = 5 * 2000
    Nd = 10
    Discard 90% of 0-reward entries from memory store   
    Resize ROI to capture mother ships
    Started on TM at 6:11 pm on Mon Dec 5

30.  Random play
    Started on m6700 around 6:30 pm on Mon, Dec 5


26.  H1 = 16; H2 = 8; 
    base learning rate = 3E-4; rms_decay = 0.9; batch size = 10
    solver = RMSPROP; eps const added to denom in RMSprop = 1E-5;
    frame_skip = 1
    old weights period = 10; min_epsilon = 0.025
    epsilon decay factor = 0.95
    replay memory capacity = 5 * 2000
    Nd = 10
    Discard 90% of 0-reward entries from memory store   
    Resize ROI to capture mother ships
    Started on TM at 6:11 pm on Mon Dec 5

31.  H1 = 16; H2 = 8; 
    base learning rate = 3E-4; rms_decay = 0.9; batch size = 10
    solver = RMSPROP; eps const added to denom in RMSprop = 1E-5;
    frame_skip = 1
    old weights period = 10; min_epsilon = 0.05
    epsilon decay factor = 0.9
    replay memory capacity = 5 * 2000
    Nd = 10
    Discard 90% of 0-reward entries from memory store   
    Resize ROI to capture mother ships
    Started on TM at 4 am on Tues Dec 6

    Conclusion:  Need bigger epsilon decay factor (not smaller!)

32.  H1 = 16; H2 = 8; 
    base learning rate = 3E-4; rms_decay = 0.9; batch size = 20
    solver = RMSPROP; eps const added to denom in RMSprop = 1E-3;
    frame_skip = 1
    old weights period = 10; min_epsilon = 0.025
    epsilon decay factor = 0.95
    replay memory capacity = 5 * 2000
    Nd = 10
    Discard 90% of 0-reward entries from memory store   
    Resize ROI to capture mother ships
    Started on TM at 4:05 am on Tues Dec 6

    Conclusion:  eps const added to denom = 1E-3 is probably too big!

33.  H1 = 16; H2 = 8; 
    base learning rate = 2.5E-4; rms_decay = 0.9; batch size = 10
    solver = RMSPROP; eps const added to denom in RMSprop = 1E-3;
    frame_skip = 1
    old weights period = 10; min_epsilon = 0.025
    epsilon decay factor = 0.95
    replay memory capacity = 5 * 2000
    Nd = 10
    Discard 90% of 0-reward entries from memory store   
    Resize ROI to capture mother ships
    Started on TM at 4:08 am on Tues Dec 6

34.  H1 = 16; H2 = 8; 
    base learning rate = 3E-4; rms_decay = 0.9; batch size = 10
    solver = RMSPROP; eps const added to denom in RMSprop = 1E-3;
    frame_skip = 1
    old weights period = 32; min_epsilon = 0.025
    epsilon decay factor = 0.95
    replay memory capacity = 5 * 2000
    Nd = 10
    Discard 90% of 0-reward entries from memory store   
    Resize ROI to capture mother ships
    Started on TM at 4:11 am on Tues Dec 6

35.  Tue Dec  6 07:17:20 2016  (baseline)
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 4717
   layer = 1 n_nodes = 16
   layer = 2 n_nodes = 8
   layer = 3 n_nodes = 6
   n_weights = 75648 (FC)
base_learning_rate = 0.0003; batch_size = 10; n_max_episodes = 10000
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 10
Discount factor gamma = 0.95 frame_skip = 1
Epsilon decay factor = 0.95; minimum epsilon = 0.025
Discard 90% of 0-reward entries from memory store    
Old weights period = 32

Disappointing results


36.  Tue Dec  6 07:19:10 2016 (discard 95% of zero reward entries)
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 4717
   layer = 1 n_nodes = 16
   layer = 2 n_nodes = 8
   layer = 3 n_nodes = 6
   n_weights = 75648 (FC)
base_learning_rate = 0.0003; batch_size = 10; n_max_episodes = 10000
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 10
Discount factor gamma = 0.95 frame_skip = 1
Epsilon decay factor = 0.95; minimum epsilon = 0.025

Discard 95% of 0-reward entries from memory store    
Old weights period = 32
 
  Conclusion:  Discarding 95% of 0-reward entries from memory store seems
               to be better than discarding 90%.

37.  Tue Dec  6 07:21:13 2016  (increase eps decay factor and min eps)
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 4717
   layer = 1 n_nodes = 16
   layer = 2 n_nodes = 8
   layer = 3 n_nodes = 6
   n_weights = 75648 (FC)
base_learning_rate = 0.0003; batch_size = 10; n_max_episodes = 10000
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 10
Discount factor gamma = 0.95 frame_skip = 1
Epsilon decay factor = 0.99; minimum epsilon = 0.05

Discard 95% of 0-reward entries from memory store    
Old weights period = 32

  Conclusion: Definitely good idea to increase min_eps and eps decay factor

39.  Tue Dec  6 07:23:18 2016  (increase old weights period)
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 4717
   layer = 1 n_nodes = 16
   layer = 2 n_nodes = 8
   layer = 3 n_nodes = 6
   n_weights = 75648 (FC)
base_learning_rate = 0.0003; batch_size = 10; n_max_episodes = 10000
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 10
Discount factor gamma = 0.99 frame_skip = 1
Epsilon decay factor = 0.99; minimum epsilon = 0.05

Discard 95% of 0-reward entries from memory store    
Old weights period = 320
n_anneal_steps = 5

Conclusion: Increasing old weights period by order of magnitude hurt
performance

40.  Tue Dec  6 07:59:25 2016  (Try to find reasonable baseline)
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 4717
   layer = 1 n_nodes = 16
   layer = 2 n_nodes = 8
   layer = 3 n_nodes = 6
   n_weights = 75648 (FC)
base_learning_rate = 0.0003; batch_size = 1; n_max_episodes = 10000
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 10
Discount factor gamma = 0.95 frame_skip = 1
Epsilon decay factor = 0.99; minimum epsilon = 0.05

Discard 90% of 0-reward entries from memory store    
Old weights period = 10
n_anneal_steps = 10

Conclusion:  batch_size = 1 seems to run noticeably slower (by roughly
factor of 2) compared to batch_size = 10


41.  Tue Dec  6 12:01:52 2016  (increase batch size --> 20)
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 4717
   layer = 1 n_nodes = 16
   layer = 2 n_nodes = 8
   layer = 3 n_nodes = 6
   n_weights = 75648 (FC)
base_learning_rate = 0.0003; batch_size = 20; n_max_episodes = 10000
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 10
Discount factor gamma = 0.99 frame_skip = 1
Epsilon decay factor = 0.99; minimum epsilon = 0.05
n_anneal_steps = 7
Discard 95% of 0-reward entries from memory store    
Old weights period = 32
Reward rises for 1K episodes and then falls below random play for next 1K
episodes !


43.  Tue Dec  6 12:06:41 2016  (H1 --> 24)
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 4717
   layer = 1 n_nodes = 24
   layer = 2 n_nodes = 8
   layer = 3 n_nodes = 6
   n_weights = 113448 (FC)
base_learning_rate = 0.0003; batch_size = 10; n_max_episodes = 10000
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 10
Discount factor gamma = 0.99 frame_skip = 1
Epsilon decay factor = 0.99; minimum epsilon = 0.05
n_anneal_steps = 7
Discard 95% of 0-reward entries from memory store    
Old weights period = 32
Reward rises for 1K episodes and then falls for next 1K!

47.  Tue Dec  6 17:47:02 2016  (m6700) : living reward
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 4717
   layer = 1 n_nodes = 16
   layer = 2 n_nodes = 8
   layer = 3 n_nodes = 6
   n_weights = 75648 (FC)
base_learning_rate = 0.0003; batch_size = 10; n_max_episodes = 10000
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 10
Discount factor gamma = 0.99 frame_skip = 1
Epsilon decay factor = 0.99; minimum epsilon = 0.05
n_anneal_steps = 7
Discard 95% of 0-reward entries from memory store    
Old weights period = 32
live_timestep_reward = 50.0 / 2000.0 = 1 / 40.0 = 0.025
death penalty = 0;

Over O(2K) episodes, reward falls!  Nframes per episode also shows no
significant increase.

conclusion:  Living reward doesn't help at least with these hyperparams.

48.  Tue Dec  6 18:55:53 2016  (m6700) : dying penalty
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 4717
   layer = 1 n_nodes = 16
   layer = 2 n_nodes = 8
   layer = 3 n_nodes = 6
   n_weights = 75648 (FC)
base_learning_rate = 0.0003; batch_size = 10; n_max_episodes = 10000
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 10
Discount factor gamma = 0.99 frame_skip = 1
Epsilon decay factor = 0.99; minimum epsilon = 0.05
n_anneal_steps = 7
Discard 95% of 0-reward entries from memory store    
Old weights period = 32
live_timestep_reward = 50.0 / 2000.0 = 1 / 40.0 = 0.025
death penalty = -15;

Over O(2K) episodes, reward falls!.  And Nframes/episode also doesn't
increase.

Conclusion:  Death penalty doesn't help (at least with these hyperparams)

38.  Tue Dec  6 07:23:18 2016  (increase gamma)
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 4717
   layer = 1 n_nodes = 16
   layer = 2 n_nodes = 8
   layer = 3 n_nodes = 6
   n_weights = 75648 (FC)
base_learning_rate = 0.0003; batch_size = 10; n_max_episodes = 10000
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 10
Discount factor gamma = 0.99 frame_skip = 1
Epsilon decay factor = 0.99; minimum epsilon = 0.05
n_anneal_steps = 5

Discard 95% of 0-reward entries from memory store    
Old weights period = 32

  Conclusion :  Increasing gamma --> 0.99 might be a good idea


42.  Tue Dec  6 12:03:50 2016  (new "baseline")
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 4717
   layer = 1 n_nodes = 16
   layer = 2 n_nodes = 8
   layer = 3 n_nodes = 6
   n_weights = 75648 (FC)
base_learning_rate = 0.0003; batch_size = 10; n_max_episodes = 10000
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 10
Discount factor gamma = 0.99 frame_skip = 1
Epsilon decay factor = 0.99; minimum epsilon = 0.05
n_anneal_steps = 7
Discard 95% of 0-reward entries from memory store    
Old weights period = 32
Loss function looks dangerously large 
Fairly slow rise in reward over nearly 2K episodes

Results are definitely not as good as those from experiment 38.

After O(8K) episodes:

Loss = +0.25 +/- 0.5
Reward --> 240
nframes/episode = (2.4 +/- 0.5)K
Gentle evolution of weights


44. Tue Dec  6 12:08:05 2016  (H3 = 8)
Neural net params:
   n_layers = 5
   layer = 0 n_nodes = 4717
   layer = 1 n_nodes = 16
   layer = 2 n_nodes = 8
   layer = 3 n_nodes = 8
   layer = 4 n_nodes = 6
   n_weights = 75712 (FC)
base_learning_rate = 0.0003; batch_size = 10; n_max_episodes = 10000
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 10
Discount factor gamma = 0.99 frame_skip = 1
Epsilon decay factor = 0.99; minimum epsilon = 0.05
n_anneal_steps = 7
Discard 95% of 0-reward entries from memory store    
Old weights period = 32

Extremely slow rise in reward over 1K episodes

After O(8K) episodes:

Loss = -0.25 +/- 0.5
Reward stays around 180 which is not much better than chance!
Nframes/episode = 2.4K

conclusion: 3rd FC layer is bad (at least with these hyperparams)

45.  Tue Dec  6 15:37:24 2016  (H1 = H2 = 16)
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 4717
   layer = 1 n_nodes = 16
   layer = 2 n_nodes = 16
   layer = 3 n_nodes = 6
   n_weights = 75824 (FC)
base_learning_rate = 0.0003; batch_size = 10; n_max_episodes = 10000
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 10
Discount factor gamma = 0.99 frame_skip = 1
Epsilon decay factor = 0.99; minimum epsilon = 0.05
n_anneal_steps = 6
Discard 95% of 0-reward entries from memory store    
Old weights period = 32

After O(7K) episodes:

loss = 0.125 +/- 0.5
Reward --> 230
nframes/episode = 2.5K 

Conclusion : H2 = 16 naively yields poorer results than H2 = 8.  But we
still strongly suspect that adding more network capacity is wise


--------------------
RUNNING EXPERIMENTS
--------------------


46.  Tue Dec  6 15:38:41 2016 (H1 = 16, H2 = 32)
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 4717
   layer = 1 n_nodes = 16
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 6
   n_weights = 76176 (FC)
base_learning_rate = 0.0003; batch_size = 10; n_max_episodes = 10000
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 10
Discount factor gamma = 0.99 frame_skip = 1
Epsilon decay factor = 0.99; minimum epsilon = 0.05
n_anneal_steps = 6
Discard 95% of 0-reward entries from memory store    
Old weights period = 32

After O(5K) episodes:

Loss = -0.5 +/- 0.5
Reward --> 245
Nframes / episode = 3.2K !!
Gentle evolution of all weights

49.  Tue Dec  6 19:56:13 2016 (m6700): H1 = 16, H2 = 64
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 4717
   layer = 1 n_nodes = 16
   layer = 2 n_nodes = 64
   layer = 3 n_nodes = 6
   n_weights = 76880 (FC)
base_learning_rate = 0.0003; batch_size = 10; n_max_episodes = 10000
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 10
Discount factor gamma = 0.99 frame_skip = 1
Epsilon decay factor = 0.99; minimum epsilon = 0.05
n_anneal_steps = 6
Discard 95% of 0-reward entries from memory store    
Old weights period = 32

After O(1.5K) episodes, reward very slowly increases (but not much beyond
random play).  Nframes/episode also slowly increases.

Conclusion:  H2 = 64 probably doesn't hurt. 

50.  Expt 46 but with min_eps --> 0.1  (m6700)
Wed Dec  7 04:10:11 2016
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 4717
   layer = 1 n_nodes = 16
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 6
   n_weights = 76176 (FC)
base_learning_rate = 0.0003; batch_size = 10; n_max_episodes = 10000
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 10
Discount factor gamma = 0.99 frame_skip = 1
Epsilon decay factor = 0.99; minimum epsilon = 0.1
n_anneal_steps = 6
Old weights period = 32
Discard zero reward frac = 0.95

51.  Expt 46 with batch size --> 16 (m6700)
Wed Dec  7 04:13:01 2016
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 4717
   layer = 1 n_nodes = 16
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 6
   n_weights = 76176 (FC)
base_learning_rate = 0.0003; batch_size = 16; n_max_episodes = 10000
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 10
Discount factor gamma = 0.99 frame_skip = 1
Epsilon decay factor = 0.99; minimum epsilon = 0.05
n_anneal_steps = 6
Old weights period = 32
Discard zero reward frac = 0.95

52.  Expt 46 with Nd --> 16 (m6700)
Wed Dec  7 04:14:30 2016
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 4717
   layer = 1 n_nodes = 16
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 6
   n_weights = 76176 (FC)
base_learning_rate = 0.0003; batch_size = 10; n_max_episodes = 10000
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0
Replay memory capacity = 10000
Number random samples drawn from replay memory Nd = 16
Discount factor gamma = 0.99 frame_skip = 1
Epsilon decay factor = 0.99; minimum epsilon = 0.05
n_anneal_steps = 6
Old weights period = 32
Discard zero reward frac = 0.95

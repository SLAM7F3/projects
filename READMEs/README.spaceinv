==========================================================================
Deep Q learning notes for Space Invaders
==========================================================================
Last updated on 12/4/16; 12/5/16
==========================================================================

Random play results:

reward = 159.6 +/- 16.6
Median reward +/- quartile width = 158.3 +/- 10.7

n_frames / episode = 2132.3 +/- 670.7
Median nframes/episode +/- quartile width = 2057 +/- 375


BEST RESULTS SO FAR
-------------------

3.  H1 = 16; H2 = 8; 
    base learning rate = 1E-4; rms_decay = 0.9; batch size = 5
    solver = RMS_prop; frame_skip = 1
    old weights period = 10; min_epsilon = 0.025
    epsilon decay factor = 0.95
    Long term reward around 195 whereas chance is around 170.
    nframes/episode around 2400 whereas chance is around 2200

Terminated experiments
----------------------

1.  H1 = 16; H2 = 8; 
    base learning rate = 1E-2; rms_decay = 0.9; batch size = 5
    solver = RMS_prop; frame_skip = 1
    old weights period = 10; min_epsilon = 0.025
    epsilon decay factor = 0.95
    Learning rate too high

2.  H1 = 16; H2 = 8; 
    base learning rate = 1E-3; rms_decay = 0.9; batch size = 5
    solver = RMS_prop; frame_skip = 1
    old weights period = 10; min_epsilon = 0.025
    epsilon decay factor = 0.95
    Long term reward comparable to chance

3.  H1 = 16; H2 = 8; 
    base learning rate = 1E-4; rms_decay = 0.9; batch size = 5
    solver = RMS_prop; frame_skip = 1
    old weights period = 10; min_epsilon = 0.025
    epsilon decay factor = 0.95
    Long term reward around 195 whereas chance is around 170.
    nframes/episode around 2400

4.  Random play
    Started on TM at 2:07 on Sun Dec 4
    Long term reward around 160; Frames per episodes around 2100

    Random reward = 159.6 +/- 16.6
    Median reward +/- quartile width = 158.3 +/- 10.7

    n_frames / episode = 2132.3 +/- 670.7
    Median nframes/episode +/- quartile width = 2057 +/- 375

5.  H1 = 16; H2 = 8; 
    base learning rate = 3E-3; rms_decay = 0.9; batch size = 5
    solver = RMS_prop; frame_skip = 1
    old weights period = 10; min_epsilon = 0.025
    epsilon decay factor = 0.95
    Started on TM at 2:27 pm on Sun Dec 4
    Long term reward decreases down to around 170

6.  H1 = 16; H2 = 8; 
    base learning rate = 3E-4; rms_decay = 0.9; batch size = 5
    solver = RMS_prop; frame_skip = 1
    old weights period = 10; min_epsilon = 0.025
    epsilon decay factor = 0.95
    Started on TM at 2:27 pm on Sun Dec 4
    Long term reward around 162

7.  H1 = 16; H2 = 8; 
    base learning rate = 3E-2; rms_decay = 0.9; batch size = 5
    solver = RMS_prop; frame_skip = 1
    old weights period = 10; min_epsilon = 0.025
    epsilon decay factor = 0.95
    Started on TM at 2:27 pm on Sun Dec 4
    Long term reward around 176; nframes/episode around 2285

8.  Random play
    Started on m6700 at 3:38 on Sun Dec 4
    Terminated

9.  Random play
    Started on m6700 at 8:16 on Sun Dec 4
    Terminated

10.  H1 = 16; H2 = 8; 
    base learning rate = 1E-2; rms_decay = 0.9; batch size = 5
    solver = SGD; frame_skip = 1
    old weights period = 10; min_epsilon = 0.025
    epsilon decay factor = 0.95
    replay memory capacity = 5 * 2000
    Nd = 5
    Discard 90% of 0-reward entries from memory store   
    Started on TM at 6:55 am on Mon Dec 5
    Minor weight evolution in first 80 episodes

11.  H1 = 16; H2 = 8; 
    base learning rate = 1E-2; rms_decay = 0.9; batch size = 5
    solver = RMSPROP; frame_skip = 1
    old weights period = 10; min_epsilon = 0.025
    epsilon decay factor = 0.95
    replay memory capacity = 5 * 2000
    Nd = 5
    Discard 90% of 0-reward entries from memory store   
    Started on TM at 7:05 am on Mon Dec 5
    Major weight evolution in first 80 episodes
    Divergent loss function

12.  H1 = 16; H2 = 8; 
    base learning rate = 1E-1; rms_decay = 0.9; batch size = 5
    solver = SGD; frame_skip = 1
    old weights period = 10; min_epsilon = 0.025
    epsilon decay factor = 0.95
    replay memory capacity = 5 * 2000
    Nd = 5
    Discard 90% of 0-reward entries from memory store   
    Started on TM at 6:55 am on Mon Dec 5
    Significant weight evolution in first 60 episodes


13.  H1 = 16; H2 = 8; 
    base learning rate = 1E-1; rms_decay = 0.9; batch size = 5
    solver = RMSPROP; frame_skip = 1
    old weights period = 10; min_epsilon = 0.025
    epsilon decay factor = 0.95
    replay memory capacity = 5 * 2000
    Nd = 5
    Discard 90% of 0-reward entries from memory store   
    Started on TM at 7:05 am on Mon Dec 5
    Major weight evolution in first 50 episodes

14.  H1 = 16; H2 = 8; 
    base learning rate = 1E-3; rms_decay = 0.9; batch size = 5
    solver = RMSPROP; frame_skip = 1
    old weights period = 10; min_epsilon = 0.025
    epsilon decay factor = 0.95
    replay memory capacity = 5 * 2000
    Nd = 5
    Discard 90% of 0-reward entries from memory store   
    Started on TM at 7:18 am on Mon Dec 5
    Gentle weight evolution in first 120 episodes
    Reward went up for 150 episodes.  Then it fell precipitously for next
    250 episodes!  Terminated!

16.  H1 = 16; H2 = 8; 
    base learning rate = 3E-3; rms_decay = 0.9; batch size = 5
    solver = RMSPROP; frame_skip = 1
    old weights period = 10; min_epsilon = 0.025
    epsilon decay factor = 0.95
    replay memory capacity = 5 * 2000
    Nd = 10
    Discard 90% of 0-reward entries from memory store   
    Divergent loss function
    Significant movement among weights over O(300) episodes
    Reward starts trending downwards after O(200) episodes.
    This base learning rate is very likely too high.  TERMINATED

18.  H1 = 16; H2 = 8; 
    base learning rate = 1E-4; rms_decay = 0.9; batch size = 5
    solver = RMSPROP; frame_skip = 1
    old weights period = 10; min_epsilon = 0.025
    epsilon decay factor = 0.95
    replay memory capacity = 5 * 2000
    Nd = 10
    Discard 90% of 0-reward entries from memory store   
    Started on TM at 8:01 am on Mon Dec 5
    Very little evolution of weights over 250 episodes.
    Reward increased for O(100) episodes to roughly random play value.
    But then it decreases for next 150 episodes!  TERMINATED.

19.  H1 = 16; H2 = 8; 
    base learning rate = 3E-3; rms_decay = 0.9; batch size = 5
    solver = RMSPROP; frame_skip = 1
    old weights period = 10; min_epsilon = 0.025
    epsilon decay factor = 0.95
    replay memory capacity = 5 * 2000
    Nd = 16
    Discard 85% of 0-reward entries from memory store   
    Started on TM at 11:30 am on Mon Dec 5
    Diverging loss function; dropping reward; TERMINATED

20.  H1 = 16; H2 = 8; 
    base learning rate = 1E-3; rms_decay = 0.9; batch size = 5
    solver = RMSPROP; frame_skip = 1
    old weights period = 10; min_epsilon = 0.025
    epsilon decay factor = 0.95
    replay memory capacity = 5 * 2000
    Nd = 16
    Discard 85% of 0-reward entries from memory store   
    Started on TM at 11:30 am on Mon Dec 5
    Reward after O(200) episodes is not nearly so good as that for expt 15
     and not much better than chance. TERMINATED.

21.  H1 = 16; H2 = 8; 
    base learning rate = 3E-4; rms_decay = 0.9; batch size = 5
    solver = RMSPROP; frame_skip = 1
    old weights period = 10; min_epsilon = 0.025
    epsilon decay factor = 0.95
    replay memory capacity = 5 * 2000
    Nd = 16
    Discard 85% of 0-reward entries from memory store   
    Started on TM at 11:30 am on Mon Dec 5
    Reward after O(200) episodes is not nearly so good as that for expt 15.
    TERMINATED.


Running experiments
--------------------

15.  H1 = 16; H2 = 8; 
    base learning rate = 1E-3; rms_decay = 0.9; batch size = 5
    solver = RMSPROP; frame_skip = 1
    old weights period = 10; min_epsilon = 0.025
    epsilon decay factor = 0.95
    replay memory capacity = 5 * 2000
    Nd = 10
    Discard 90% of 0-reward entries from memory store   
    Started on TM at 7:47 am on Mon Dec 5
    Relatively gentle evolution of weights over O(300) episodes

17.  H1 = 16; H2 = 8; 
    base learning rate = 3E-4; rms_decay = 0.9; batch size = 5
    solver = RMSPROP; frame_skip = 1
    old weights period = 10; min_epsilon = 0.025
    epsilon decay factor = 0.95
    replay memory capacity = 5 * 2000
    Nd = 10
    Discard 90% of 0-reward entries from memory store   
    Started on TM at 7:50 am on Mon Dec 5
    Mile evolution of weights over O(300) episodes


22.  H1 = 16; H2 = 8; 
    base learning rate = 1E-3; rms_decay = 0.9; batch size = 5
    solver = RMSPROP; frame_skip = 1
    old weights period = 10; min_epsilon = 0.025
    epsilon decay factor = 0.95
    replay memory capacity = 5 * 2000
    Nd = 16
    Discard 90% of 0-reward entries from memory store   
    Started on TM at 3:22 pm on Mon Dec 5


23.  H1 = 16; H2 = 8; 
    base learning rate = 3E-4; rms_decay = 0.9; batch size = 5
    solver = RMSPROP; frame_skip = 1
    old weights period = 10; min_epsilon = 0.025
    epsilon decay factor = 0.95
    replay memory capacity = 5 * 2000
    Nd = 16
    Discard 90% of 0-reward entries from memory store   
    Started on TM at 3:23 pm on Mon Dec 5


24.  H1 = 24; H2 = 8; 
    base learning rate = 1E-3; rms_decay = 0.9; batch size = 5
    solver = RMSPROP; frame_skip = 1
    old weights period = 10; min_epsilon = 0.025
    epsilon decay factor = 0.95
    replay memory capacity = 5 * 2000
    Nd = 10
    Discard 90% of 0-reward entries from memory store   


25.  H1 = 24; H2 = 8; 
    base learning rate = 3E-4; rms_decay = 0.9; batch size = 5
    solver = RMSPROP; frame_skip = 1
    old weights period = 10; min_epsilon = 0.025
    epsilon decay factor = 0.95
    replay memory capacity = 5 * 2000
    Nd = 10
    Discard 90% of 0-reward entries from memory store   

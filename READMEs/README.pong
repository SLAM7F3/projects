==========================================================================
RL notes for Pong
==========================================================================
Last updated on 12/30/16
==========================================================================


Random play results for O(250 episodes) = O(20) epochs:
------------------------------------------------------

Paddle X looks like uniform distribution except for 2 spikes around X = 8
and X = 24

nframes/episode = 3700
reward = -20.6


Best play results to date:
-------------------------

-------------------------------------------------------------
TERMINATED EXPERIMENTS
-------------------------------------------------------------

4.  m6700:

Fri Dec 30 09:48:33 2016
Neural net params:
   n_layers = 3
   layer = 0 n_nodes = 1024
   layer = 1 n_nodes = 64
   layer = 2 n_nodes = 2
   n_weights = 65664 (FC)
base_learning_rate = 0.0003; batch_size = -1
P learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 20000
Number random samples drawn from replay memory Nd = 0
Discount factor gamma = 0.99
minimum epsilon = 0
n_actions = 2
Leaky ReLU small slope = 0.01
Learning rate decrease period = 10000 episodes
nframes / epoch = 50000
n_max_epochs = 3000
Random seed = -9310

No learning after 30 epochs

5.  Repeat of expt 4

No learning after 30 epochs

-------------------------------------------------------------
RUNNING EXPERIMENTS
-------------------------------------------------------------

m6700
-----

6.  Fri Dec 30 11:25:08 2016
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 1024
   layer = 1 n_nodes = 64
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 2
   n_weights = 67648 (FC)
base_learning_rate = 0.001; batch_size = -1
P learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 20000
Number random samples drawn from replay memory Nd = 0
Discount factor gamma = 0.99
minimum epsilon = 0
n_actions = 2
Leaky ReLU small slope = 0.01
Learning rate decrease period = 10000 episodes
nframes / epoch = 50000
n_max_epochs = 3000
Random seed = -15104

7.  m6700:  repeat expt 6

Thinkmate
---------

1008.  TM:  H1 = 64; H2 = 0; replay memory = 20K; ppong

Fri Dec 30 15:10:35 2016
Neural net params:
   n_layers = 3
   layer = 0 n_nodes = 1024
   layer = 1 n_nodes = 64
   layer = 2 n_nodes = 2
   n_weights = 65664 (FC)
base_learning_rate = 0.001; batch_size = -1
P learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 20000
Number random samples drawn from replay memory Nd = -999
Discount factor gamma = 0.99
minimum epsilon = -nan
n_actions = 2
Leaky ReLU small slope = 0.01
Learning rate decrease period = 10000 episodes
nframes / epoch = 50000
n_max_epochs = 3000
Random seed = -39423

1009.  TM:  repeat expt 1008

10.  TM:  H1 = 128; H2 = 0; replay memory = 20K; ppong

Fri Dec 30 15:20:17 2016
Neural net params:
   n_layers = 3
   layer = 0 n_nodes = 1024
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 2
   n_weights = 131328 (FC)
base_learning_rate = 0.001; batch_size = -1
P learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 20000
Number random samples drawn from replay memory Nd = -999
Discount factor gamma = 0.99
minimum epsilon = -nan
n_actions = 2
Leaky ReLU small slope = 0.01
Learning rate decrease period = 10000 episodes
nframes / epoch = 50000
n_max_epochs = 3000
Random seed = -40016

11. TM:  repeat expt 10

24.  TM:  H1 = 64; H2 = 32; replay memory 20K; qpong

Fri Dec 30 16:23:29 2016
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 1024
   layer = 1 n_nodes = 64
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 2
   n_weights = 67648 (FC)
base_learning_rate = 0.001; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 200000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.99
minimum epsilon = 0.1
n_actions = 2
Leaky ReLU small slope = 0.01
Learning rate decrease period = 10000 episodes
Old weights period = 2 epochs
Discard zero reward frac = 0
Use big states flag = 0
Frame skip = 1
1 big state = n_screen_states = 1
  Compute difference flag = 1
  Compute max flag = 0
Starting epoch for linear epsilon decay = 4
Stopping epoch for linear epsilon decay = 300
nn_update_frame_period = 1
nframes / epoch = 50000
n_max_epochs = 2000
Random seed = -43806

25.  TM:  repeat expt 24


Titan 1:
-------

16.  Titan 1:  H1 = 64; H2 = 64; replay memory = 20K; ppong

Fri Dec 30 15:35:26 2016
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 1024
   layer = 1 n_nodes = 64
   layer = 2 n_nodes = 64
   layer = 3 n_nodes = 2
   n_weights = 69760 (FC)
base_learning_rate = 0.001; batch_size = -1
P learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 20000
Number random samples drawn from replay memory Nd = -999
Discount factor gamma = 0.99
minimum epsilon = -nan
n_actions = 2
Leaky ReLU small slope = 0.01
Learning rate decrease period = 10000 episodes
nframes / epoch = 50000
n_max_epochs = 3000
Random seed = -40924

17.  Titan 1:  repeat expt 16

18.  Titan 1:  H1 = 64; H2 = 128; replay memory = 20K; ppong
Fri Dec 30 15:38:09 2016
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 1024
   layer = 1 n_nodes = 64
   layer = 2 n_nodes = 128
   layer = 3 n_nodes = 2
   n_weights = 73984 (FC)
base_learning_rate = 0.001; batch_size = -1
P learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 20000
Number random samples drawn from replay memory Nd = -999
Discount factor gamma = 0.99
minimum epsilon = -nan
n_actions = 2
Leaky ReLU small slope = 0.01
Learning rate decrease period = 10000 episodes
nframes / epoch = 50000
n_max_epochs = 3000
Random seed = -41088

19.  Titan 1:  repeat expt 18

22.  Titan 1:  H1 = 128; H2 = 0; replay memory = 20K; qpong
Fri Dec 30 16:22:21 2016
Neural net params:
   n_layers = 3
   layer = 0 n_nodes = 1024
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 2
   n_weights = 131328 (FC)
base_learning_rate = 0.001; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 200000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.99
minimum epsilon = 0.1
n_actions = 2
Leaky ReLU small slope = 0.01
Learning rate decrease period = 10000 episodes
Old weights period = 2 epochs
Discard zero reward frac = 0
Use big states flag = 0
Frame skip = 1
1 big state = n_screen_states = 1
  Compute difference flag = 1
  Compute max flag = 0
Starting epoch for linear epsilon decay = 4
Stopping epoch for linear epsilon decay = 300
nn_update_frame_period = 1
nframes / epoch = 50000
n_max_epochs = 2000
Random seed = -43739

23.  Titan 1:  H1 = 128; H2 = 0; replay memory = 20K; qpong
repeat expt 22



Titan 3:
-------

12.  Titan 3:  H1 = 64; H2 = 32; replay memory = 20K; ppong

Fri Dec 30 15:24:54 2016
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 1024
   layer = 1 n_nodes = 64
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 2
   n_weights = 67648 (FC)
base_learning_rate = 0.001; batch_size = -1
P learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 20000
Number random samples drawn from replay memory Nd = -999
Discount factor gamma = 0.99
minimum epsilon = -nan
n_actions = 2
Leaky ReLU small slope = 0.01
Learning rate decrease period = 10000 episodes
nframes / epoch = 50000
n_max_epochs = 3000
Random seed = -40292

13.  Titan 3:  repeat expt 12

14.  Titan 3: H1 = 128; H2 = 32; replay memory = 20K; ppong

Fri Dec 30 15:26:43 2016
Neural net params:
   n_layers = 4
   layer = 0 n_nodes = 1024
   layer = 1 n_nodes = 128
   layer = 2 n_nodes = 32
   layer = 3 n_nodes = 2
   n_weights = 135232 (FC)
base_learning_rate = 0.001; batch_size = -1
P learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 20000
Number random samples drawn from replay memory Nd = -999
Discount factor gamma = 0.99
minimum epsilon = -nan
n_actions = 2
Leaky ReLU small slope = 0.01
Learning rate decrease period = 10000 episodes
nframes / epoch = 50000
n_max_epochs = 3000
Random seed = -40402

15.  Titan 3:  repeat expt 14

20.  Titan 3:  H1 = 64; H2 = 0; replay memory = 20K; qpong

Fri Dec 30 16:19:09 2016
Neural net params:
   n_layers = 3
   layer = 0 n_nodes = 1024
   layer = 1 n_nodes = 64
   layer = 2 n_nodes = 2
   n_weights = 65664 (FC)
base_learning_rate = 0.001; batch_size = 1
Q learning
solver type = RMSPROP
   rmsprop_decay_rate = 0.9
   rmsprop_denom_const = 1e-05
L2 regularization lambda coeff = 0.01
Replay memory capacity = 200000
Number random samples drawn from replay memory Nd = 32
Discount factor gamma = 0.99
minimum epsilon = 0.1
n_actions = 2
Leaky ReLU small slope = 0.01
Learning rate decrease period = 10000 episodes
Old weights period = 2 epochs
Discard zero reward frac = 0
Use big states flag = 0
Frame skip = 1
1 big state = n_screen_states = 1
  Compute difference flag = 1
  Compute max flag = 0
Starting epoch for linear epsilon decay = 4
Stopping epoch for linear epsilon decay = 300
nn_update_frame_period = 1
nframes / epoch = 50000
n_max_epochs = 2000

21.  Titan 3:  repeat expt 20

===========================================================
TODO:

A.  Implement weight normalization 

B.  Implement data-dependent weight initialization

C.  Experiment with trying to extract ball and paddle posn and velocity and
forming much smaller input layer for a much smaller neural net


DONE:

A.  Fix memory leak!

B.  Revert to working with difference state rather than 1 big state
containing 2 screen states

C.  Fix exporting of each individual screen rather than every 3rd or 4th
one

G.  Trace individual weights' evolutions

F.  Perhaps experiment with 3 hidden layer neural network but only after
implementing batch normalization 


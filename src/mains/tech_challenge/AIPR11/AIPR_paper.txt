=============================================================================
AIPR 2011 paper
=============================================================================
Last updated on 10/4/11
=============================================================================

3D Exploitation of 2D Ground-Level & Aerial Imagery

Peter Cho (MIT Lincoln Laboratory) & Noah Snavely (Cornell University)

1.  Introduction

The invention of the digital camera is attributed to Eastman Kodak engineer
Steven Sasson in 1975 [ref].  Sasson's device recorded 0.01 megapixel black
and white pictures to cassette tapes.  The first digital photograph
required 23 seconds to generate and needed a television set to be displayed
[ref].  Today, digital cameras recording 10+ megapixel color photos to
Secure Digital (SD) cards with storage capacities of 1 Megabyte to 2
Terabytes are commonplace.  Though the total number of digital images in
existence is not known, billions of photos and video clips can now be
readily accessed on the internet [ref needed].

Current hardware ability for collecting and storing digital images vastly
exceeds current software capability for mining electronic pictures.
Existing archives of digital photos and videos are basically unstructured.
As anyone who has ever performed a Google image query knows, searching
imagery websites is often a frustrating experience.  Text-based queries
generally cannot retrieve information of interest such as camera
geolocation, scene identity or target characteristics.  So some basic
organizing principle is needed to enable efficient navigating and mining of
vast digital imagery repositories.

Fortunately, three-dimensional geometry provides such an organizing
principle for imagery collected at different times, places & perspectives.
For example, suppose we have a set of digital ground photos of some ground
target.  Those pictures represent 2D projections of 3D world-space onto a
variety of image planes.  If the target's geometry is captured in a 3D map,
it can be used to mathematically relate different ground photos of the
target to each other.  Moreover, the 3D map connects together data
collected by completely different sensors at different times. So we can
relate a photo of the target shot by a ground camera with a corresponding
aerial picture or ladar image provided all these data products are
georegistered with the 3D map.  And the map itself acts as a repository for
high-level knowledge distilled from multiple sensors.  Ultimately,
situational awareness comes much more directly from the knowledge stored
within the map than from the millions of low-level pixels and voxels on
which it is based.

In this paper, 

Our paper is organized as follows.  Working with photos shot around MIT, we
briefly review in section 2 remarkable recent advances in computer vision
which reconstructs three-dimensional layouts from large collections of 2D
photos.  After we georegister a 3D reconstruction, a human can conduct a
virtual tour through a field of images to get an intuitive sense for what a
complex urban scene looks like.  In section 3, we next exploit imagery
gathered from a UAV operating over a rural environment.  As we'll see,
several tactically useful problems become mathematically tractable once a
3D framework for analyzing 2D images is adopted.  Finally, we close in
section 4 with a few words about our ongoing research as well as future
applications of this work.

2.  Urban ground imagery exploitation

Before we could exploit a large, cooperatively-gathered urban photo data
set, we had to collect it ourselves.  So 2 summers ago, a few LL volunteers
conducted field trips to MIT campus and ran around shooting digital photos.
We chose MIT as a surrogate small city where we knew the natives would not
be perturbed by our somewhat unusual data gathering techniques.  For about
the first 5 minutes, we selected our shots with care & precision.  But as
time went by, our choosiness went down but our photo collection rate went
way up.  So this is why this slide is labeled as a SEMI-cooperative urban
photo collection.  Over the course of 5 field trips, more than 30,000
photos were shot.  Most were taken outdoors.  But we also consciously shot
a few thousand pictures inside some MIT buildings with the hope of
connecting together exterior & interior views.  

Here are 30 representative examples of the 30,000 photos that we shot
around MIT.  The variation in this urban data set makes it quite
challenging. All of the MIT photos were shot inside urban canyons where the
scene significantly changes with every few steps.  So more than 2 years
after collecting these images, we are still generalizing our algorithms and
codes to handle this very rich data set.  

Building upon many years of computer vision research, Snavely and
collaborators discovered in 2006 how to take such unstructured sets of
photos as input and extract organized structure as output.  Their approach
(which Microsoft picked up and named Photosynth) takes advantage of
redundancy among multiple images to solve a complex inverse problem.  It
yields the relative 3D locations of their cameras as well as the 3D
structure of the target.  In the next few slides, we?ll briefly review the
essential steps which take the quasi-random set of input photos shown on
the left and turns them into the geometrically organized output shown on
the right.
  
The first step in the reconstruction procedure is to extract features from
each input image.  For example, here we see two representative photos from
the 30K MIT data set which have partially overlapping views.  We apply
Lowe?s Scale Invariant Feature Transform (or SIFT for short) and find over
30,000 features in each image.  Like other feature extractors, SIFT
searches for gradient content in imagery.  But it generates 128-dimensional
descriptor vectors which significantly discriminate between different
features.  Because each feature has an associated 128-dimensional vector of
integer labels, SIFT in combination with other image processing algorithms
such as RANSAC can reasonably robustly identify tiepoint pair matches
between the same feature appearing in multiple photos.  When we zoom in, we
can verify that most (though not all) SIFT matches are valid.  We also
observe that SIFT detects features in images quite differently than humans.
SIFT matching imposes topological ordering upon the input set of
unstructured 30K MIT photos.  

Working with Michael Yee, we have developed a prototype image search engine
which works with SIFT matching results plus hierarchical clustering to help
a user navigate through the large imagery set.  The graph display shown on
the right displays the collection?s global structure, while the web
interface shown on the left enables drill-down into individual photos.  Our
image search engine is netcentric and supports multi-user interactions.


Once SIFT features have been detected and matched between multiple views,
Snavely et al have demonstrated how a machine can automatically recover
geometry information by solving a complex inverse problem.  Starting from a
set of images of a static scene with 2D feature tiepoints (shown here as
color-coded corner projections), the computer finds the corresponding set
of 3D points (shown here as colored cube corners) and the rotation,
position and focal length for each camera.  When we project a 3D point into
any of the image planes, the discrepancy between the projected and observed
2D features should be small.  We have applied these structure recovery
algorithms to 2.3K of our 30K MIT photos.  

The 3D reconstruction results are shown in this slide.  Instead of having
to look through thumbnail after thumbnail, a user can instead intuitively
fly through this large photo collection.  The reconstructed photos are
displayed as frusta embedded within the reconstructed sparse point cloud
for the eastern part of MIT?s campus.  After selecting a photo shot in
Kendall Square, we see one particular view of the Marriott hotel nearby
MIT.  We can then command the virtual camera to assume the position &
orientation for several other photos of the hotel in order to get a better
sense for what this urban area looks like.  Finally, as the virtual camera
zooms outwards, we see dozens, hundreds & eventually thousands of
reconstructed photos.  

It?s important to recall that cameras capture angle-angle projections of 3D
worldspace onto 2D image planes.  In the absence of metadata, photos
contain no length scales.  So 3D photo reconstruction by itself cannot
provide absolute position or distance measurements.  In order to
georegister reconstructed output, we must incorporate other sensor data
besides just pixel intensities.  

Fortunately, we have access to laser radar data for all of Boston.  Active
illumination laser radar (ladar) systems can measure detailed 3D geometry
for entire cities at ground sampling distances of 1 meter or less.  

Up to this point, our computer vision algorithms have all been fully
automated.  But we have not yet made attempted to automate matching of
ground photo and ladar data.  Instead, we select 10 photos from the 2.3K
reconstructed set which have wide angular diversity and reconstruction
uncertainties are low.  We next manually establish tiepoint pairings
between 2D pixels within the 10 selected photos and corresponding voxels in
the 3D map.  Using these tiepoint pairings, we set up and solve a system of
equations for the global translation, rotation & scaling needed to register
reconstructed photos with the 3D Boston map.  

Here we display the reconstructed photos after they have been geoaligned
with the ladar map.  For clarity?s sake, we show only O(10**2) of the
O(10**3) georegistered frusta.  Aerial ladar systems are typically
nadir-oriented onboard aircraft and consequently capture mostly rooftop
content within urban scenes.  In contrast, passive ground-level photos
typically see buildings? wall content.  As our virtual camera flies in
towards the MIT campus, we observe that the aerial ladar building rooftop
points match onto the reconstructed ground-level building sides points.
When the virtual camera zooms into a particular georegistered ground
picture, we can fade away its image plane and visually observe its 3D
correspondence with ladar points off in the distance.  

The reconstructed photos form a complicated mess, and trying to pick out
individual cameras within the 3D viewer is difficult.  So we have combined
the 3D viewer with a 2D web browser interface in order to simplify human
interaction with the rich data set.  Applying ideas recently utilized for
the Line-of-Sight Tool (LOST) development, Jennifer Drexler rapidly created
a Google Map interface to the 3D viewer.  On the left, we see a Google Map
of the MIT campus with our 2317 reconstructed photos represented as colored
dots.  When the user clicks on a particular dot, a message is sent from the
web browser to the 3D viewer which commands the virtual camera to assume
the same geoposition and pointing as the real camera that shot the selected
photo.  Here we see examples of interior health center shots which have
been successfully reconstructed.  MIT?s health center has large glass walls
at its entrance that allow photos on the inside to connect onto photos on
the outside.  So here we see at least one example where geocoordinates can
be derived for photos shot in potentially GPS denied environments.
  
The geopositions & pointing angles of all 2300 reconstructed photos are
fairly dense around the eastern part of the MIT campus.  So it?s possible
to construct paths through this field of photos which represent virtual
tours within an urban environment.  Using the Google Map interface, we can
easily enter a desired route along which we would like to see what the city
looks like.  After we press the initialize tour button, the computer
searches through all reconstructed photos for those which lie close to the
desired path and generally point in the forward direction.  When we press
the play button in the web browser, the virtual camera begins its tour.  As
the 3D viewer transitions from one photo to the next, it cross-fades the
pictures so that the user can see the virtual camera move inside the city.
The camera?s current geolocation is also displayed as a colored dot within
the Google Map.  The value of this prototype virtual tour capability for
mission planning is clear.  It would allow soldiers to see around corners
and look for potential sniper locations, hiding places, communication
obstructions etc before they actually run around corners...

3.  Rural UAV imagery exploitation

In this second section, we turn from ground photos to aerial video.  We
also work with imagery gathered over a rural setting rather than from an
urban environment.  Nevertheless, the same computer vision ideas can be
fruitfully applied to this qualitatively different set of input imagery.

Two summers ago, Lincoln Lab held a Technology Challenge called Project
Scout which involved remotely characterizing a one-square kilometer,
semi-rural area.  To solve this problem, our Tech Challenge team developed
and fielded a surveillance system based upon sensing at multiple
levels-of-detail, redundancy in data coverage and varied degrees of
technological risk.  Our system involved high-flying air vehicles for
synoptic imaging, lower-flying hover platforms for soda-straw
reconnaissance, and slower ground movers for monitoring areas under foliage
which we knew would not be visible from the air.  

One of the platforms which our Tech Challenge team fielded was a
hand-launched sailplane glider.  It carried a lightweight Canon powershot
camera mounted on its underside as well as a small Garmin GPS unit.  The
total cost for this hardware setup came to less than $1K.

Representative examples of the raw video imagery gathered by the aerial
glider are shown in this slide.  Over the course of a typical 15 minute
flight, the Canon camera shot collected a few thousand frames at roughly 3
Hz.  When the glider returned from a sortie, its aerial pictures were
offloaded from the camera?s SD chip and later processed on the ground.

Just as for the MIT ground photos, the processing pipeline for the aerial
video frames began with SIFT feature matching and incremental bundle
adjustment.  But in this aerial example, we also applied multi-view stereo
algorithms developed by Furukawa and Ponce in order to generate a dense
reconstruction for the ground scene.  

The resulting 3D point cloud representation of the rural area over which
the glider flew is much more dense than the sparse cloud generated by
incremental bundle adjustment alone.  We also need to georegister the
reconstructed cameras plus point cloud.  Unlike the previous MIT example,
we did not have ladar data for the rural area over which the glider flew.
So we used the measurements from the glider?s onboard GPS unit to fit the
camera?s reconstructed flight path and derive the global translation,
rotation and scaling which converts to absolute world geocoordinates.  Of
the 3K aerial video frames which were passed as input into the video
processing pipeline, approximately 1500 were successfully reconstructed.
In this chart, we show just 74 of the reconstructed cameras as 3D frusta.
For comparison, we also display the glider?s GPS track as the continuous
curve colored according to height.  

We can now demonstrate several examples of geometry-based exploitation of
aerial video which are difficult to perform via conventional image
processing.  Firstly, our imaging hardware and reconstruction plus
georegistration software yield detailed 3D terrain maps with absolute
altitudes above sea-level for ground, water and trees.  The approximate 1
meter ground sampling of the measurements displayed in this color-coded
height map begin to rival those from ladar systems.  But the cost of this
passive approach to terrain mapping is orders of magnitude less than that
for active ladar systems.

As a second example of 3D exploitation of 2D video, we consider
stabilization and orthorectification.  The aerial glider experienced
significant jostling during its flight above the rural scene, and its raw
video footage looks jumpy.  But once the aerial camera?s 3D location and
orientation are known, it is straightforward to orthorectify its video
frames.  We simply project reconstructed camera?s views onto a common
Z-plane.  The resulting 3D-based video stabilization results look much
better than those from conventional 2D ?rubber sheet? warping techniques.
  
Our 3rd and final example of video exploitation involves propagating
intelligence from one frame to another using 3D geometry.  Once the aerial
camera?s position and orientation are known, any pixel within its image
plane corresponds to a calculable ray in 3D world-space.  So after a user
chooses some pixel in a video frame, we can backproject the pixel?s
location into the dense point cloud and find its corresponding voxel in the
reconstructed point cloud.  The voxel has a longitude, latitude, altitude
and range which the selected pixel inherits.  In this slide, we see 3
pixels which have been annotated with their associated voxels? altitudes
and ranges.  

We next may ask which, if any, of the pixels that were previously selected
within video frame 44 appear in some other video frame such as the one
shown on the right in this slide.  It is not easy for a human eye to figure
out the answer to this question just by inspection.  But the question can
be simply answered via 3D geometry.  On the left hand side, we see the
geoposition and pose for the aerial camera in video frame #67.  Of the 3
voxels associated with the 3 pixels selected in video frame #44, only one
of them lands inside of the viewing frustum for video frame #67.  When that
voxel?s location is projected into the video frame?s image plane, we see
its new position within the new video frame.  This simple example
demonstrates how geometry could be used to automatically annotate aerial
video frame footage in the future.

4.  Summary and future work

We?ve nearly come to the end of this talk.  But it?s hopefully clear that
we?re just beginning to explore many interesting and useful applications of
3D exploitation of 2D imagery.  In this presentation, we have focused upon
augmenting UAV video with ancillary intelligence as one key application of
3D exploitation of large imagery archives.  But other important
applications include urban mission planning (e.g. a soldier could see
around a city street corner before actually running into harm?s way), robot
operation (e.g. indoor mapping of office buildings via a Google Streets
type of display) and georegistering dynamic targets (e.g. determine a
person?s geocoordinates after georegistering an uncooperatively collected
photo with a large archived set).


5.  References

*.  For a history of cameras, see
http://en.wikipedia.org/wiki/History_of_the_camera#Digital_cameras.  



==================================================

Junk

Megapixel digital cameras were first marketed for consumers in 1997 [Wiki
ref].  In the decade since their introduction, billions of photos and video
clips have been shot by digital cameras on the ground and in the air.  

First recorded attempt at building a digital camera was in 1975 by Steven
Sasson, an engeinner at Eastman Kodak.  Camera weighed 8 pounds, recorded
black and white images to cassette taqpe, resolution =0.01 megapixels, took
23 seconds to capture its firs timage.  


Today: weight = few ounces, price = $100 - $200, size = few inches,
resolution = 12 Megapixels.

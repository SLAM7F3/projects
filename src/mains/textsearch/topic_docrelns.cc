// ========================================================================
// Program TOPIC_DOCRELNS imports the "Bag of words" generated by
// TEXT2WORDS into an STL map.  Looping over components corresponding
// to corresponding coarse and fine topics, it also reads in text
// files from a series of specified subdirectories.  TOPIC_DOCRELNS
// first outputs a file containing document ID vs document filename.
// Looping over all strings within all text documents, TOPIC_DOCRELNS
// next performs the same cleaning operations on each string as
// TEXT2WORDS did to generate the bag of words.  It then computes a
// normalized term frequency for each cleaned string as well as an
// inverse document frequency.  TOPIC_DOCRELNS exports the file
// "word_importance.dat" which contains cleaned words sorted according
// to their inverse document frequencies.

// TOPIC_DOCRELNS performs an SVD on the word-document matrix and
// reduces its dimensionality to k_dims=300 x n_text_files.  Working
// with the reduced word-document matrix, it subsequently computes
// tfidf genvectors for each document which have unit magnitude.
// Genmatrix *docs_overlap_ptr holds the dotproducts between every
// document tfidf genvector with every other. Finally, TOPIC_DOCRELNS
// exports an edge list where nodes correspond to document IDs and
// edge weights equal scaled versions of the document-document inner
// products.

// ========================================================================
// Last updated on 3/3/13; 5/27/13; 5/29/13
// ========================================================================

#include <algorithm>
#include <iostream>
#include <map>
#include <string>
#include <vector>
#include "gmm/gmm.h"
#include "gmm/gmm_matrix.h"

#include "general/filefuncs.h"
#include "math/genvector.h"
#include "templates/mytemplates.h"
#include "general/outputfuncs.h"
#include "passes/PassesGroup.h"
#include "math/prob_distribution.h"
#include "general/stringfuncs.h"
#include "general/sysfuncs.h"
#include "time/timefuncs.h"

// ==========================================================================
int main( int argc, char** argv )
{
   using std::cin;
   using std::cout;
   using std::endl;
   using std::flush;
   using std::map;
   using std::ofstream;
   using std::string;
   using std::vector;
   std::set_new_handler(sysfunc::out_of_memory);

   timefunc::initialize_timeofday_clock();

   const int ascii_a=97;
   const int ascii_z=122;
   const int ascii_A=65;
   const int ascii_Z=90;
//   const int ascii_hyphen=45;

// Use an ArgumentParser object to manage the program arguments:

   osg::ArgumentParser arguments(&argc,argv);
   PassesGroup passes_group(&arguments);

   string bundle_filename=passes_group.get_bundle_filename();
//   cout << " bundle_filename = " << bundle_filename << endl;
   string bundler_IO_subdir=filefunc::getdirname(bundle_filename);
//   cout << "bundler_IO_subdir = " << bundler_IO_subdir << endl;
   string image_list_filename=bundler_IO_subdir+"image_list.dat";
   
// Instantiate and populate STL map with relationships between
// text filenames and bundler image/document IDs:

   typedef map<std::string, int > FILENAME_DOCID_MAP;
// independent var = text filename prefix
// dependent var = bundler image/document ID

   FILENAME_DOCID_MAP filename_docid_map;
   FILENAME_DOCID_MAP::iterator filename_docid_iter;

   filefunc::ReadInfile(image_list_filename);
   for (unsigned int i=0; i<filefunc::text_line.size(); i++)
   {
      string curr_image_filename=filefunc::text_line[i];
      string basename=filefunc::getbasename(curr_image_filename);
      string prefix=stringfunc::prefix(basename);
      filename_docid_map[prefix]=i;
//      cout << "bundler ID = " << i << " prefix = " << prefix << endl;
   }

// Reuters 50K:

   string doc_corpus_prefix="reuters_50K_";
//   int n_coarse_topics=40;
//   int n_fine_topics=300;

// Reuters 43K:

//   string doc_corpus_prefix="reuters_43K_";
//   int n_coarse_topics=35;
//   int n_fine_topics=250;

// Import bag of words:

   bool reuters_flag=true;
   string reuters_subdir=
      "/data_third_disk/text_docs/reuters/export/";
   string text_subdir=reuters_subdir+"text/50K_docs/";
   string topic_docs_subdir=text_subdir+"topic_docs/";

//   string arXiv_subdir="/media/66368D22368CF3F9/visualization/arXiv/";
//   string reuters_subdir=arXiv_subdir+"reuters/export/";
//   string text_subdir=reuters_subdir+"text/";
//   string mallet_subdir=text_subdir+"mallet/";
//   string topic_docs_subdir=mallet_subdir+"topic_docs/";

   typedef map<string,int> WORD_MAP;
   WORD_MAP::iterator word_iter,stop_word_iter;
   WORD_MAP multi_doc_word_map,stop_word_map;

// Import word list from bag of words generated by TEXT2WORDS:

   string bag_filename=text_subdir+"all_docs.wordbag";
   filefunc::ReadInfile(bag_filename);

   int min_corpus_word_freq=10;
//   int min_corpus_word_freq=50;

   int word_ID=0;
   vector<string> word_list,sorted_word_list;
   for (unsigned int i=0; i<filefunc::text_line.size(); i++)
   {
      string curr_line=filefunc::text_line[i];
      vector<string> substrings=
         stringfunc::decompose_string_into_substrings(curr_line);
      string curr_word=substrings[1];

      int corpus_freq=stringfunc::string_to_number(substrings[2]);
      if (corpus_freq < min_corpus_word_freq) continue;

      word_iter=multi_doc_word_map.find(curr_word);
      if (word_iter != multi_doc_word_map.end())
      {
         cout << "curr_word = " << curr_word << " already exists!" 
              << endl;
         outputfunc::enter_continue_char();
      }
      
      multi_doc_word_map[curr_word]=word_ID;
      word_list.push_back(curr_word);
      sorted_word_list.push_back(curr_word);
    
      word_ID++;
   } // loop over index i labeling lines in all_docs.wordbag

   int n_words=multi_doc_word_map.size();
   cout << "multi_doc_word_map.size() = " << n_words << endl;
   cout << "word_list.size() = " << word_list.size() << endl;

// Import stop word list downloaded from web and augmented by us:

   string stoplist_filename="./stop_list.txt";
   filefunc::ReadInfile(stoplist_filename);

   for (unsigned int i=0; i<filefunc::text_line.size(); i++)
   {
      string stop_word=filefunc::text_line[i];
      stop_word=stringfunc::remove_leading_whitespace(stop_word);
      stop_word=stringfunc::remove_trailing_whitespace(stop_word);
      stop_word_map[stop_word]=-1;
//      cout << i << "   stop_word = " << stop_word 
//           << " word size = " << stop_word.size() << endl;
   }
   cout << "stop_word_map.size() = " << stop_word_map.size() << endl;

// Import coarse & fine topic ID correspondences from
// coarse_fine_topic_docs.dat:

   vector<int> coarse_topic_IDs,fine_topic_IDs;
   string topics_docs_filename=bundler_IO_subdir+"coarse_fine_topic_docs.dat";
   filefunc::ReadInfile(topics_docs_filename);

   for (unsigned int i=0; i<filefunc::text_line.size(); i++)
   {
      string curr_line=filefunc::text_line[i];
//      cout << "curr_line = " << curr_line << endl;
      vector<string> substrings=
         stringfunc::decompose_string_into_substrings(curr_line);
      coarse_topic_IDs.push_back(stringfunc::string_to_number(substrings[0]));
      fine_topic_IDs.push_back(stringfunc::string_to_number(substrings[1]));
   }
   int n_fine_topics=fine_topic_IDs.size();
   cout << "n_fine_topics = " << n_fine_topics << endl;

// Import document IDs and filenames from reuters_43K_doc_topics.txt.
// Store document filename vs document ID in STL map:

   string doc_topics_filename=bundler_IO_subdir+doc_corpus_prefix+"doc_"
      +stringfunc::number_to_string(n_fine_topics)+"topics.txt";
   filefunc::ReadInfile(doc_topics_filename);

   typedef map<string,int> DOC_NAME_VS_ID_MAP;
   DOC_NAME_VS_ID_MAP doc_name_vs_id_map;
   DOC_NAME_VS_ID_MAP::iterator doc_name_vs_id_iter;
// independent var = document basename
// dependent var = document ID (within full 43K corpus)

   for (unsigned int i=0; i<filefunc::text_line.size(); i++)
   {
      string curr_line=filefunc::text_line[i];
      vector<string> substrings=stringfunc::decompose_string_into_substrings(
         curr_line);
//      int mallet_doc_ID=stringfunc::string_to_number(substrings[0]);
      string doc_filename=filefunc::getbasename(substrings[1]);
      string prefix=stringfunc::prefix(doc_filename);
      
      filename_docid_iter=filename_docid_map.find(prefix);
      if (filename_docid_iter==filename_docid_map.end())
      {
         cout << "Error!  Couldn't find prefix in filename_docid_map!"
              << endl;
         outputfunc::enter_continue_char();
         continue;
      }
      int doc_ID=filename_docid_iter->second;
      doc_name_vs_id_map[doc_filename]=doc_ID;
//      cout << "doc_filename = " << doc_filename << endl;
   }
   cout << "doc_name_vs_id_map.size() = " 
        << doc_name_vs_id_map.size() << endl;

// -------------------------------------------------------------------------
// Loop over components labeled by corresponding coarse & fine topic
// IDs starts here
// -------------------------------------------------------------------------

   int component_start=0;
   cout << "Enter starting component:" << endl;
   cin >> component_start;
   for (unsigned int component=component_start; component<coarse_topic_IDs.size(); 
        component++)
   {
      cout << "Component = " << component
           << " coarse topic ID = " << coarse_topic_IDs[component]
           << " fine topic ID = " << fine_topic_IDs[component]
           << endl;

      string curr_topic_docs_subdir=topic_docs_subdir+"topic_"+
         stringfunc::integer_to_string(coarse_topic_IDs[component],3)+"_"+
         stringfunc::integer_to_string(fine_topic_IDs[component],4)+"/";

// Import document text files:

      vector<string> allowed_suffixes;
      allowed_suffixes.push_back("txt");
      vector<string> text_filenames=
         filefunc::files_in_subdir_matching_specified_suffixes(
            allowed_suffixes,curr_topic_docs_subdir);
      int n_text_files=text_filenames.size();
      if (n_text_files==0) continue;

      int n_start=0;
      int n_stop=n_text_files;

// Export file containing document name vs document ID:

      vector<int> document_IDs;
      string docs_filename=curr_topic_docs_subdir+"doc_IDs_vs_filenames.dat";
      ofstream docstream;
      filefunc::openfile(docs_filename,docstream);
      docstream << "# Doc ID    Doc filename" << endl << endl;
      for (int n=n_start; n<n_stop; n++)
      {
         string text_filename=text_filenames[n];
         string basename=filefunc::getbasename(text_filename);

         doc_name_vs_id_iter=doc_name_vs_id_map.find(basename);
         if (doc_name_vs_id_iter==doc_name_vs_id_map.end())
         {
            cout << "text_filename not found in doc_name_vs_id_map!" << endl;
            exit(-1);
         }
         else
         {
            document_IDs.push_back(doc_name_vs_id_iter->second);
         }
         docstream << document_IDs.back() << "   " << text_filename << endl;
      }
      filefunc::closefile(docs_filename,docstream);
      string banner="Exported "+docs_filename;
      outputfunc::write_banner(banner);

// Loop over all strings within all text documents starts here:

      genvector* ndocs_containing_term_ptr=new genvector(n_words);
      ndocs_containing_term_ptr->clear_values();
      cout << "n_text_files = " << n_text_files
           << " n_words = " << n_words << endl;

      genvector* term_frequency_ptr=new genvector(n_words);
      gmm::row_matrix< gmm::wsvector<float> >* term_freq_sparse_matrix_ptr=
         new gmm::row_matrix< gmm::wsvector<float> >(n_text_files,n_words);
      gmm::clear(*term_freq_sparse_matrix_ptr);

      for (int n=n_start; n<n_stop; n++)
      {
         string text_filename=text_filenames[n];
         if (n%100==0)
            cout << "n = " << n << " Computing word histogram for "+
               filefunc::getbasename(text_filename) << endl;
         filefunc::ReadInfile(text_filename);
         term_frequency_ptr->clear_values();

         bool introduction_found_flag=false;      
         bool reuters_end_found_flag=false;
         int n_document_strings=0;
         for (unsigned int i=0; i<filefunc::text_line.size() && 
                 !introduction_found_flag; i++)
         {
            string curr_line=filefunc::text_line[i];
            vector<string> substrings=
               stringfunc::decompose_string_into_substrings(curr_line);
            for (unsigned int s=0; s<substrings.size(); s++)
            {
               string curr_substring=substrings[s];

// Ignore numbers:

               if (stringfunc::is_number(curr_substring)) continue;

// Search for keywords near end of Reuters' articles:

               if (reuters_flag)
               {
                  if (curr_substring=="(Reporting" ||
                  curr_substring=="(reporting" || curr_substring=="(Reported" 
                  || curr_substring=="(reported")
                  {
                     reuters_end_found_flag=true;
                  }
                  else if (curr_substring=="(Editing" ||
                  curr_substring=="(editing" || curr_substring=="(Edited" ||
                  curr_substring=="(edited")
                  {
                     reuters_end_found_flag=true;
                  }
                  else if (curr_substring=="(Additional" ||
                  curr_substring=="(additional" || curr_substring=="(Added" ||
                  curr_substring=="(added")
                  {
                     reuters_end_found_flag=true;
                  }
                  else if (curr_substring=="(Writing" ||
                  curr_substring=="(writing" || curr_substring=="(Written" ||
                  curr_substring=="(written")
                  {
                     reuters_end_found_flag=true;
                  }
                  else if (curr_substring=="(Created" ||
                  curr_substring=="(created")
                  {
                     reuters_end_found_flag=true;
                  }
                  else if (curr_substring=="(Compiled" ||
                  curr_substring=="(compiled")
                  {
                     reuters_end_found_flag=true;
                  }
                  else if (curr_substring=="(Via" ||
                  curr_substring=="(via")
                  {
                     reuters_end_found_flag=true;
                  }
                  else if (curr_substring=="(Sources:" ||
                  curr_substring=="(sources" || curr_substring=="(SOURCES:" ||
                  curr_substring=="(SOURCE:"|| curr_substring=="(source:" ||
                  curr_substring=="Sources:" || curr_substring=="sources:" ||
                  curr_substring=="SOURCES:" || curr_substring=="SOURCE:")
                  {
                     reuters_end_found_flag=true;
                  }
               } // reuters_flag conditional
               if (reuters_end_found_flag) break;

// Remove non-letters from strings:

               vector<int> ascii_string=
                  stringfunc::decompose_string_to_ascii_rep(curr_substring);

               string cleaned_substring;
               for (unsigned int s=0; s<ascii_string.size(); s++)
               {
                  int curr_ascii=ascii_string[s];
                  if (curr_ascii >= ascii_a && curr_ascii <= ascii_z)
                  {
                     cleaned_substring.push_back(curr_substring[s]);
                  }

// Convert uppercase letters to lowercase:

                  if (curr_ascii >= ascii_A && curr_ascii <= ascii_Z)
                  {
                     curr_ascii += ascii_a-ascii_A;
                     char new_char=stringfunc::ascii_integer_to_char(
                        curr_ascii);
                     cleaned_substring.push_back(new_char);
                  }
               }
               cleaned_substring=
                  stringfunc::remove_leading_whitespace(cleaned_substring);
               cleaned_substring=
                  stringfunc::remove_trailing_whitespace(cleaned_substring);

// Ignore short strings:

               if (cleaned_substring.size() < 3) continue;

// Ignore "words" containing "garbage":

               vector<string> garbage_strings;
               garbage_strings.push_back("qqq");
               garbage_strings.push_back("xxx");

               bool garbage_continue_flag=false;
               for (unsigned int g=0; g<garbage_strings.size(); g++)
               {
                  int garbage_location=stringfunc::first_substring_location(
                     cleaned_substring,garbage_strings[g]);
                  if (garbage_location > 0) 
                  {
                     cout << "garbage_string = " << garbage_strings[g]
                          << " cleaned_substring = " << cleaned_substring
                          << endl;
                     garbage_continue_flag=true;
//                  outputfunc::enter_continue_char();
                  }
               }
               if (garbage_continue_flag) continue;

// Stem cleaned substring:

// FAKE FAKE:  Sun Dec 23, 2012 at 2 pm
// Experiment with NOT stemming:

               string stemmed_substring=cleaned_substring;

/*
  string stemmed_substring=stringfunc::stem_word(cleaned_substring);
  stemmed_substring=
  stringfunc::remove_leading_whitespace(stemmed_substring);
  stemmed_substring=
  stringfunc::remove_trailing_whitespace(stemmed_substring);
*/

// Ignore stemmed strings which match "stop" words:

               stop_word_iter=stop_word_map.find(stemmed_substring);
               if (stop_word_iter != stop_word_map.end()) continue;

               word_iter=multi_doc_word_map.find(stemmed_substring);
               if (word_iter != multi_doc_word_map.end())
               {
                  int word_index=word_iter->second;
                  int word_freq=term_frequency_ptr->get(word_index);
                  term_frequency_ptr->put(word_index,word_freq+1);
                  n_document_strings++;
               }
            } // loop over index s labeling substring in current line
         } // loop over index i labeling line within current text file

//      cout << "     n_document_strings = " 
//           << n_document_strings << endl;
      
// Compute maximum term frequency for current document:

         int max_term_freq=0;
         for (int t=0; t<n_words; t++)
         {
            int curr_term_freq=term_frequency_ptr->get(t);
            max_term_freq=basic_math::max(max_term_freq,curr_term_freq);
         }
//      cout << "max_term_freq = " << max_term_freq << endl;

// Divide raw term frequency by max term frequency in order to
// minimize variation due to document length.  Then store renormalized
// term frequencies within rows of *term_freq_sparse_matrix_ptr:

         for (int t=0; t<n_words; t++)
         {
            int curr_term_freq=term_frequency_ptr->get(t);
            double renorm_term_freq=double(curr_term_freq)/max_term_freq;
            term_frequency_ptr->put(t,renorm_term_freq);
            (*term_freq_sparse_matrix_ptr)(n,t)=renorm_term_freq;
         }

// Increment *ndocs_containing_term_ptr for current document:

         for (int j=0; j<n_words; j++)
         {
            if (term_frequency_ptr->get(j) > 0)
               ndocs_containing_term_ptr->put(
                  j,ndocs_containing_term_ptr->get(j)+1);
         }

      } // loop over index n labeling input text files
      delete term_frequency_ptr;

// Compute inverse document frequency as indicator of individual word
// importance:

      vector<int> ndocs;
      vector<double> word_importance;
      genvector* inverse_doc_frequency_ptr=new genvector(n_words);   
      for (int j=0; j<n_words; j++)
      {
         ndocs.push_back(basic_math::round(ndocs_containing_term_ptr->get(j)));

// Note added on 12/29/12 at 10:40 am : This next line is WRONG!

//      double idf=log(double(n_text_files))/
//         (1+ndocs_containing_term_ptr->get(j));

         double quotient=double(n_text_files)/
            (1+ndocs_containing_term_ptr->get(j));
         double idf=log(quotient);

         if (ndocs.back() <= 1) idf=0;

         inverse_doc_frequency_ptr->put(j,idf);
         word_importance.push_back(idf);
//      cout << "j = " << j 
//           << " ndocs = " << ndocs.back()
//           << " idf = " << idf << endl;
      }

// Export file containing words sorted according to their inverse
// document frequencies:

      cout << "n_words = " << n_words << endl;
      cout << "word_importance.size() = " << word_importance.size()
           << " word_list.size() = " << word_list.size() << endl;
      templatefunc::Quicksort_descending(
         word_importance,ndocs,sorted_word_list);

      string importance_filename=curr_topic_docs_subdir+"word_importance.dat";
      ofstream importance_stream;
      filefunc::openfile(importance_filename,importance_stream);
      importance_stream << "# Word  ndocs_frac  importance" << endl << endl;
      for (int j=0; j<n_words; j++)
      {
         double ndocs_frac=double(ndocs[j])/double(n_text_files);
         importance_stream << sorted_word_list[j] << "      "
                           << ndocs_frac << "      "
                           << word_importance[j] << endl;
      }
      filefunc::closefile(importance_filename,importance_stream);
      banner="Exported "+importance_filename;
      outputfunc::write_banner(banner);

      delete ndocs_containing_term_ptr;

// Compute term-frequency inverse document frequency (TF-IDF) ratios:

      banner="Calculating TF-IDF ratios";
      outputfunc::write_banner(banner);
      genvector* tfidf_ptr=new genvector(n_words);
      genvector* normalized_tfidf_ptr=new genvector(n_words);

      gmm::row_matrix< gmm::wsvector<float> >* word_doc_sparse_matrix_ptr=
         new gmm::row_matrix< gmm::wsvector<float> >(n_words,n_text_files);
      gmm::clear(*word_doc_sparse_matrix_ptr);

      int n_nonzero_values=0;
      for (int n=0; n<n_text_files; n++)
      {
         tfidf_ptr->clear_values();
         for (int t=0; t<n_words; t++)
         {
            tfidf_ptr->put(
               t,(*term_freq_sparse_matrix_ptr)(n,t)*
               inverse_doc_frequency_ptr->get(t));
         }

// Renormalize *tfidf_ptr so that it has unit length:

         *normalized_tfidf_ptr=tfidf_ptr->unitvector();

         double TINY=1E-9;
         for (int t=0; t<n_words; t++)
         {
            double curr_tfidf=normalized_tfidf_ptr->get(t);
            if (fabs(curr_tfidf) > TINY)
            {
               (*word_doc_sparse_matrix_ptr)(t,n)=curr_tfidf;
               n_nonzero_values++;
            }
         }

      } // loop over index n labeling input text files

      delete inverse_doc_frequency_ptr;
      delete tfidf_ptr;
      delete normalized_tfidf_ptr;
   
      genmatrix* reduced_docs_matrix_ptr=NULL;
      genmatrix* reduced_words_matrix_ptr=NULL;

      cout << "n_words = " << n_words 
           << " n_documents = " << n_text_files << endl;
//       outputfunc::print_elapsed_time();

// If the word-document matrix corresponds to original, complete
// documents, we follow the Latent Semantic Analysis procedure of
// reducing the matrix' dimension from n_words x n_text_files to
// k_dims=300 x n_text_files.  
      
      int k_dims=300;

// Perform SVD on word-document matrix:

      mathfunc::sparse_SVD_approximation(
         word_doc_sparse_matrix_ptr,k_dims,n_nonzero_values,
         curr_topic_docs_subdir);

      genmatrix* Utrans_ptr=mathfunc::import_from_dense_text_format(
         curr_topic_docs_subdir+"svd-Ut.dat");
      genmatrix* Vtrans_ptr=mathfunc::import_from_dense_text_format(
         curr_topic_docs_subdir+"svd-Vt.dat");

      if (Utrans_ptr->get_mdim() != Vtrans_ptr->get_mdim())
      {
         cout << "U.ndim = " << Utrans_ptr->get_mdim() << endl;
         cout << "Vtrans.mdim = " << Vtrans_ptr->get_mdim() << endl;
         cout << "These two dimensions should be equal!" << endl;
         exit(-1);
      }
      int ddim=Utrans_ptr->get_mdim();
      genmatrix* D_ptr=mathfunc::import_from_diagonal_text_format(
         ddim,curr_topic_docs_subdir+"svd-S.dat");

//   cout << "Utrans = " << *Utrans_ptr << endl;
//   cout << "D = " << *D_ptr << endl;
//   cout << "Vtrans = " << *Vtrans_ptr << endl;

      banner="Calculating reduced document overlap matrix";
      outputfunc::write_banner(banner);

      reduced_docs_matrix_ptr=new genmatrix(k_dims,n_text_files);
      *reduced_docs_matrix_ptr = *D_ptr * *Vtrans_ptr;

      reduced_words_matrix_ptr=new genmatrix(n_words,k_dims);
      *reduced_words_matrix_ptr=Utrans_ptr->transpose() * *D_ptr;

      delete Utrans_ptr;
      delete Vtrans_ptr;
      delete D_ptr;

      genvector curr_doc(k_dims),next_doc(k_dims);
      for (int c=0; c<n_text_files; c++)
      {
         reduced_docs_matrix_ptr->get_column(c,curr_doc);
         reduced_docs_matrix_ptr->put_column(c,curr_doc.unitvector());
      }
//       outputfunc::print_elapsed_time();

//   double weight_scale_factor=1;
      double weight_scale_factor=1000;
      cout << "weight_scale_factor = " << weight_scale_factor << endl;

// Export document edge list:

      const double min_edge_weight=weight_scale_factor*0.05;
//      const double min_edge_weight=weight_scale_factor*0.1;
//      const double min_edge_weight=weight_scale_factor*0.15;

      string output_filename=curr_topic_docs_subdir+"docs_edgelist.dat";
      ofstream outstream;
      filefunc::openfile(output_filename,outstream);

      outstream << "# Edge weight threshold = " << min_edge_weight << endl;
      outstream << "# NodeID  NodeID'  Edge weight" << endl;
      outstream << endl;

      int n_edges=0;
      double max_edge_weight=0;
      for (int i=0; i<n_text_files; i++)
      {
         reduced_docs_matrix_ptr->get_column(i,curr_doc);
         for (int j=i+1; j<n_text_files; j++)
         {
            reduced_docs_matrix_ptr->get_column(j,next_doc);
            double edge_weight=weight_scale_factor*curr_doc.dot(next_doc);
            if (edge_weight < min_edge_weight) continue;
            max_edge_weight=basic_math::max(max_edge_weight,edge_weight);
            if (edge_weight < 10)
            {
//               outstream << i << "  " << j << "  " << edge_weight << endl;
               outstream << document_IDs[i] << "  " 
                         << document_IDs[j] << "  " << edge_weight << endl;
            }
            else
            {
               outstream << document_IDs[i] << "  " 
                         << document_IDs[j] << "  " << int(edge_weight) 
                         << endl;
            }
            n_edges++;
         } // loop over index j
      } // loop over index i 

      filefunc::closefile(output_filename,outstream);

      cout << "Number of documents = " << n_text_files << endl;
      cout << "Number of words in document histograms = " << n_words << endl;
      cout << "Minimum corpus word frequency = " << min_corpus_word_freq 
           << endl;
      cout << "Number of edges = " << n_edges << endl;
      cout << "Min edge weight = " << min_edge_weight << endl;
      cout << "Max edge weight = " << max_edge_weight << endl;
      banner="Exported document edge list to "+output_filename;

      outputfunc::print_elapsed_time();

      delete term_freq_sparse_matrix_ptr;
      delete word_doc_sparse_matrix_ptr;
      delete reduced_docs_matrix_ptr;
      delete reduced_words_matrix_ptr;

   } // loop over component index labeling coarse & fine topic correspondences
}


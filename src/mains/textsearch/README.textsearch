========================================================================
README for generating graphs of text documents
========================================================================
Last updated on 5/25/13; 5/27/13; 5/29/13
========================================================================

PREPARATORY TEXT & IMAGE PROGRAMS:
---------------------------------

*.  Program PDF2TEXT runs the built-in unix command pdftotext on a set of
PDF files within a specified subdirectory.

*.  Program PDF2JPG reads in pdf files from a specified subdirectory.  It
generates JPG files of the PDF documents' first pages using ImageMagick's
convert utility.

*.  Program TEXT2HTML reads in a set of text files (e.g. scraped from
Reuters' new feed) from a specified subdirectory.  It generates simple html
versions of the input text files with rudimentary formatting.  TEXT2HTML
then converts the html into postscript and jpg formats.  The html and jpg
files are archived while the postscript versions are deleted.  White
borders are cropped and image quality is reduced as much as possible to
minimize disk space usage by the output jpg files.

*.  Program HEADLINE2THUMBNAIL is a specialized utility which we wrote for
Reuters news articles.  It extracts the first headline from each input
Reuters text file and generates an HTML file containing just the headline
in large font.  The HTML output is converted to postscript and then to jpg
via ImageMagick.  Final thumbnails are deposited within the
thumbnails_subdir specified below.


Load all input documents into MALLET.  Then generate fine and coarse
topics.  See README.mallet. 

*.  Create subdirectory of bundler:

For example, bundler_IO_subdir = bundler/textdocs/reuters/

Place all output from MALLET into bundler_IO_subdir.  Also create soft
links between all jpg versions of input text documents within
bundler_IO_subdir/images/ .  Then run
mains/photosynth/generate_peter_inputs .


TEXT DOCUMENT NETWORK GENERATION PROGRAMS (Java MALLET program based)
---------------------------------------------------------------------

*.  Program TOPIC_HIERARCHY reads in a set of document words corresponding
to "coarse" and "fine" topics generated by MALLET.  For each fine topic, it
counts the number of document words associated with all coarse topics.
Based upon the maxima of these document word counts, TOPIC_HIERARCHY
associates each fine topic with a unique coarse topic.  It exports a text
file containing fine topic vs coarse topic IDs and max document word
overlap counts.

*.  Program PARSE_DOC_TOPICS parses the document-topics text file generated
by MALLET.  It sorts coarse topics according to their average document
purity fraction.  PARSE_DOC_TOPICS exports to "coarse_vs_fine_topics.dat"
the sorted coarse topics and lists their associated fine topics sorted by
fine document membership.  The top words for each sorted coarse topic and
the associated fine topics are also exported to this output file.

PARSE_DOC_TOPICS generates a second output file
("coarse_fine_topic_docs.dat") that contains document IDs sorted by the
coarse and fine topics to which they correspond.  Export to 3rd output file
("components.dat") number of nodes, row, column and alpha-numeric label for
each connected component.  Also export top 4 words for top 5 topics for
each connected component:

*.  Program DISPLAY_DOC_TEXT queries the user to enter a document ID.  It
then displays the document's beginning text.  We wrote this little utility
program in order to debug MALLET clustering.

*.  Program DOCS_IN_TOPIC parses the document-topics text file generated by
MALLET.  Given a specified topic ID, it returns names of documents for
which that topic is dominant.

*.  Program POPULATE_TOPIC_DOC_DIRS generates a set of subdirectories
labeled by coarse and fine topic IDs.  It then fills these subdirectories
with soft links to document files for each coarse and fine topic.

*.  Program TEXT2WORDS imports all text files from a specified subdirectory
as well as a stop list of common words.  It loops over all strings within
all of the input documents.  TEXT2WORDS drops any non-letter characters it
encounters within a string and converts all upper-case letters to
lower-case.  It also discards very short strings and stems all surviving
strings.  TEXT2WORDS exports a "Bag of words" file which is ordered
according to cleaned string frequency.

*.  Program TOPIC_DOCRELNS imports the "Bag of words" generated by
TEXT2WORDS into an STL map.  Looping over components corresponding to
corresponding coarse and fine topics, it also reads in text files from a
series of specified subdirectories.  TOPIC_DOCRELNS first outputs a file
containing document ID vs document filename.  Looping over all strings
within all text documents, TOPIC_DOCRELNS next performs the same cleaning
operations on each string as TEXT2WORDS did to generate the bag of words.
It then computes a normalized term frequency for each cleaned string as
well as an inverse document frequency.  TOPIC_DOCRELNS exports the file
"word_importance.dat" which contains cleaned words sorted according to
their inverse document frequencies.

TOPIC_DOCRELNS performs an SVD on the word-document matrix and reduces its
dimensionality to k_dims=300 x n_text_files.  Working with the reduced
word-document matrix, it subsequently computes tfidf genvectors for each
document which have unit magnitude.  Genmatrix *docs_overlap_ptr holds the
dotproducts between every document tfidf genvector with every
other. Finally, TOPIC_DOCRELNS exports an edge list where nodes correspond
to document IDs and edge weights equal scaled versions of the
document-document inner products.


Make sure mission and image tables of IMAGERY database are up to date !


*.  Program POPULATE_GRAPH_DIRS generates a set of subdirectories labeled
by coarse and fine topic IDs.  It then fills these subdirectories with
sift_edgelist.dat = doc_edgelist.dat files.  POPULATE_GRAPH_DIRS
subsequently runs OGDF_layout, extract_OGDF_layout, kmeans_clusters and
generate_component_hierarchy scripts on each subdirectory's
sift_edgelist.dat file.


========================================================================
Deprecated programs
========================================================================



TEXT DOCUMENT NETWORK GENERATION PROGRAMS (D. Blei C program based)
-------------------------------------------------------------------

*.  Program TEXT2WORDS imports all text files from a specified subdirectory
as well as a stop list of common words.  It loops over all strings within
all of the input documents.  TEXT2WORDS drops any non-letter characters it
encounters within a string and converts all upper-case letters to
lower-case.  It also discards very short strings and stems all surviving
strings.  TEXT2WORDS exports a "Bag of words" file which is ordered
according to cleaned string frequency.

*. Program DOCRELNS imports the "Bag of words" generated by TEXT2WORDS into
an STL map.  It also reads in all text files from a specified subdirectory.
DOCRELNS first outputs a file containing document ID vs document filename.
Looping over all strings within all text documents, DOCRELNS next performs
the same cleaning operations on each string as TEXT2WORDS did to generate
the bag of words.  It then computes a normalized term frequency for each
cleaned string as well as an inverse document frequency.  DOCRELNS exports
the file "word_importance.dat" which contains cleaned words sorted
according to their inverse document frequencies. 

DOCRELNS next performs an SVD on the word-document matrix and reduces its
dimensionality to k_dims=300 x n_text_files.  Working with the reduced
word-document matrix, it subsequently computes tfidf genvectors for each
document which have unit magnitude.  Genmatrix *docs_overlap_ptr holds the
dotproducts between every document tfidf genvector with every other.
Finally, DOCRELNS exports an edge list where nodes correspond to document
IDs and edge weights equal scaled versions of the document-document inner
products.

*.  Program CCKEYWORDS iterates over all connected components within a
specified image graph hierarchy.  For each component, it imports the text
files corresponding to node members.  Sparse word histograms are written to
an output file which can be ingested by David Blei's Latent Dirichlet
Allocation codes.  An executable script that calls Blei's codes is also
generated by CCKEYWORDS.  When the script is run, the top 5 terms in the
top 5 topics for each connected component are written to an output file
within a topics subdirectory of the connected components directory.

*.  Program SYNTHESIZE_TOPIC_DOCS imports final.beta files generated by
David Blei's Latent Dirichlet Allocation codes.  For each connected
component within a specified image graph hierarchy, it generates a text
document containing topic keywords whose repetition frequencies are
proportional to the probabilities imported from the LDA-C beta files.  The
synthesized text documents are exported to topic_subdir/topic_doc_NNN.txt.

*. Program CLUSTER_CCS imports the edge list calculated by DOCRELNS from
synthesized topic keyword text files generated by SYNTHESIZE_TOPIC_DOCS.
It then fills STL map cc_cluster_map with connected component integer IDs
as a function of cluster ID.  Connected component cluster information is
exported to an output file along with all topic keywords corresponding to
the clusters.  The row_cc, column_cc and label columns within the
connected_components table of the IMAGERY database are also updated.

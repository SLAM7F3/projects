// ========================================================================
// Program DOCRELNS imports the "Bag of words" generated by TEXT2WORDS into
// an STL map.  It also reads in all text files from a specified subdirectory.
// DOCRELNS first outputs a file containing document ID vs document filename.
// Looping over all strings within all text documents, DOCRELNS next performs
// the same cleaning operations on each string as TEXT2WORDS did to generate
// the bag of words.  It then computes a normalized term frequency for
// each cleaned string as well as an inverse document frequency.
// DOCRELNS exports the file "word_importance.dat" which contains
// cleaned words sorted according to their inverse document
// frequencies.

// DOCRELNS next performs an SVD on the word-document matrix and reduces its
// dimensionality to k_dims=300 x n_text_files.  Working with the reduced
// word-document matrix, it subsequently computes tfidf genvectors for each
// document which have unit magnitude.  Genmatrix *docs_overlap_ptr holds the
// dotproducts between every document tfidf genvector with every other.
// Finally, DOCRELNS exports an edge list where nodes correspond to document
// IDs and edge weights equal scaled versions of the document-document inner
// products.

//				docrelns

// ========================================================================
// Last updated on 12/24/12; 12/29/12; 3/3/13
// ========================================================================

#include <algorithm>
#include <iostream>
#include <map>
#include <string>
#include <vector>
#include "gmm/gmm.h"
#include "gmm/gmm_matrix.h"

#include "general/filefuncs.h"
#include "math/genvector.h"
#include "templates/mytemplates.h"
#include "general/outputfuncs.h"
#include "math/prob_distribution.h"
#include "general/stringfuncs.h"
#include "general/sysfuncs.h"
#include "time/timefuncs.h"

// ==========================================================================
int main( int argc, char** argv )
{
   using std::cin;
   using std::cout;
   using std::endl;
   using std::flush;
   using std::map;
   using std::ofstream;
   using std::string;
   using std::vector;
   std::set_new_handler(sysfunc::out_of_memory);

// First query user to specify if word-doc SVD and edge list should be
// recomputed:

   bool compute_doc_overlap_flag=false;
   cout << endl;
   cout << "Enter 'y' to compute word-doc SVD and edge list:" << endl;
   string response_char;
   cin >> response_char;
   if (response_char=="y") compute_doc_overlap_flag=true;
   timefunc::initialize_timeofday_clock();

   int doc_level=0;
   cout << "Enter document level (0 = raw docs, 1 = docs generated from topics):" << endl;
   cin >> doc_level;

   const int ascii_a=97;
   const int ascii_z=122;
   const int ascii_A=65;
   const int ascii_Z=90;
   const int ascii_hyphen=45;

// Import bag of words:

   bool astro_flag=false;
   bool reuters_flag=true;
   string arXiv_subdir="/media/66368D22368CF3F9/visualization/arXiv/";
   string astro_subdir=arXiv_subdir+"astro/";
   string reuters_subdir=arXiv_subdir+"reuters/export/";
//   string text_subdir=astro_subdir+"txt/";
   string text_subdir=reuters_subdir+"text/";
   string ccs_subdir=text_subdir+"connected_components/";
   string topic_docs_subdir=ccs_subdir+"topic_docs/";

   if (doc_level==1)
   {
      text_subdir=topic_docs_subdir;
   }

   typedef map<string,int> WORD_MAP;
   WORD_MAP::iterator word_iter,stop_word_iter;
   WORD_MAP multi_doc_word_map,stop_word_map;

// Import word list from bag of words generated by TEXT2WORDS:

   string bag_filename=text_subdir+"all_docs.wordbag";
   filefunc::ReadInfile(bag_filename);

//   int min_corpus_word_freq=0;
//   int min_corpus_word_freq=50;
   int min_corpus_word_freq=100;
//   int min_corpus_word_freq=4;
//   cout << "Enter min word frequency within documents:" << endl;
//   cin >> min_corpus_word_freq;
   if (doc_level==1) min_corpus_word_freq=1;

   int word_ID=0;
   vector<string> word_list,sorted_word_list;
   for (int i=0; i<filefunc::text_line.size(); i++)
   {
      string curr_line=filefunc::text_line[i];
      vector<string> substrings=
         stringfunc::decompose_string_into_substrings(curr_line);
      string curr_word=substrings[1];

      int corpus_freq=stringfunc::string_to_number(substrings[2]);
      if (corpus_freq < min_corpus_word_freq) continue;

      word_iter=multi_doc_word_map.find(curr_word);
      if (word_iter != multi_doc_word_map.end())
      {
         cout << "curr_word = " << curr_word << " already exists!" 
              << endl;
         outputfunc::enter_continue_char();
      }
      
      multi_doc_word_map[curr_word]=word_ID;
      word_list.push_back(curr_word);
      sorted_word_list.push_back(curr_word);
    
      word_ID++;
   } // loop over index i labeling lines in all_docs.wordbag

   int n_words=multi_doc_word_map.size();
   cout << "multi_doc_word_map.size() = " << n_words << endl;
   cout << "word_list.size() = " << word_list.size() << endl;

// Import stop word list downloaded from web and augmented by us:

   string stoplist_filename="./stop_list.txt";
   filefunc::ReadInfile(stoplist_filename);

   for (int i=0; i<filefunc::text_line.size(); i++)
   {
      string stop_word=filefunc::text_line[i];
      stop_word=stringfunc::remove_leading_whitespace(stop_word);
      stop_word=stringfunc::remove_trailing_whitespace(stop_word);
      stop_word_map[stop_word]=-1;
//      cout << i << "   stop_word = " << stop_word 
//           << " word size = " << stop_word.size() << endl;
   }
   cout << "stop_word_map.size() = " << stop_word_map.size() << endl;

// Import document text files:

   vector<string> allowed_suffixes;
   allowed_suffixes.push_back("txt");
   vector<string> text_filenames=
      filefunc::files_in_subdir_matching_specified_suffixes(
         allowed_suffixes,text_subdir);
   int n_text_files=text_filenames.size();

//   n_text_files=1000;
//   n_text_files=5000;
   int n_start=0;
   int n_stop=n_text_files;

// Export file containing document name vs document ID:

   string docs_filename=text_subdir+"doc_IDs_vs_filenames.dat";
   ofstream docstream;
   filefunc::openfile(docs_filename,docstream);
   docstream << "# Doc ID    Doc filename" << endl << endl;
   for (int n=n_start; n<n_stop; n++)
   {
      string text_filename=text_filenames[n];
      docstream << n << "   " << text_filename << endl;
   }
   filefunc::closefile(docs_filename,docstream);

// Loop over all strings within all text documents starts here:

   genvector* ndocs_containing_term_ptr=new genvector(n_words);
   ndocs_containing_term_ptr->clear_values();
   cout << "n_text_files = " << n_text_files
        << " n_words = " << n_words << endl;

   genvector* term_frequency_ptr=new genvector(n_words);
   gmm::row_matrix< gmm::wsvector<float> >* term_freq_sparse_matrix_ptr=
      new gmm::row_matrix< gmm::wsvector<float> >(n_text_files,n_words);
   gmm::clear(*term_freq_sparse_matrix_ptr);

   for (int n=n_start; n<n_stop; n++)
   {
      string text_filename=text_filenames[n];
      if (n%100==0)
         cout << "n = " << n << " Computing word histogram for "+
            filefunc::getbasename(text_filename) << endl;
      filefunc::ReadInfile(text_filename);
      term_frequency_ptr->clear_values();

      bool introduction_found_flag=false;      
      bool reuters_end_found_flag=false;
      int n_document_strings=0;
      for (int i=0; i<filefunc::text_line.size() && !introduction_found_flag; 
           i++)
      {
         string curr_line=filefunc::text_line[i];
         vector<string> substrings=
            stringfunc::decompose_string_into_substrings(curr_line);
         for (int s=0; s<substrings.size(); s++)
         {
            string curr_substring=substrings[s];

// Ignore numbers:

            if (stringfunc::is_number(curr_substring)) continue;

// Search for keywords near end of Reuters' articles:

            if (reuters_flag)
            {
               if (curr_substring=="(Reporting" ||
               curr_substring=="(reporting" || curr_substring=="(Reported" ||
               curr_substring=="(reported")
               {
                  reuters_end_found_flag=true;
               }
               else if (curr_substring=="(Editing" ||
               curr_substring=="(editing" || curr_substring=="(Edited" ||
               curr_substring=="(edited")
               {
                  reuters_end_found_flag=true;
               }
               else if (curr_substring=="(Additional" ||
               curr_substring=="(additional" || curr_substring=="(Added" ||
               curr_substring=="(added")
               {
                  reuters_end_found_flag=true;
               }
               else if (curr_substring=="(Writing" ||
               curr_substring=="(writing" || curr_substring=="(Written" ||
               curr_substring=="(written")
               {
                  reuters_end_found_flag=true;
               }
               else if (curr_substring=="(Created" ||
               curr_substring=="(created")
               {
                  reuters_end_found_flag=true;
               }
               else if (curr_substring=="(Compiled" ||
               curr_substring=="(compiled")
               {
                  reuters_end_found_flag=true;
               }
               else if (curr_substring=="(Via" ||
               curr_substring=="(via")
               {
                  reuters_end_found_flag=true;
               }
               else if (curr_substring=="(Sources:" ||
               curr_substring=="(sources" || curr_substring=="(SOURCES:" ||
               curr_substring=="(SOURCE:"|| curr_substring=="(source:" ||
               curr_substring=="Sources:" || curr_substring=="sources:" ||
               curr_substring=="SOURCES:" || curr_substring=="SOURCE:")
               {
                  reuters_end_found_flag=true;
               }
            } // reuters_flag conditional
            if (reuters_end_found_flag) break;

// Remove non-letters from strings:

            vector<int> ascii_string=stringfunc::decompose_string_to_ascii_rep(
               curr_substring);

            string cleaned_substring;
            for (int s=0; s<ascii_string.size(); s++)
            {
               int curr_ascii=ascii_string[s];
               if (curr_ascii >= ascii_a && curr_ascii <= ascii_z)
               {
                  cleaned_substring.push_back(curr_substring[s]);
               }

// Convert uppercase letters to lowercase:

               if (curr_ascii >= ascii_A && curr_ascii <= ascii_Z)
               {
                  curr_ascii += ascii_a-ascii_A;
                  char new_char=stringfunc::ascii_integer_to_char(curr_ascii);
                  cleaned_substring.push_back(new_char);
               }
            }
            cleaned_substring=
               stringfunc::remove_leading_whitespace(cleaned_substring);
            cleaned_substring=
               stringfunc::remove_trailing_whitespace(cleaned_substring);
         
            if (astro_flag)
            {
               if (cleaned_substring=="introduction")
               {
                  cout << "Introduction found" << endl;
                  introduction_found_flag=true;
               }
               else if (cleaned_substring=="pacs")
               {
                  cout << "PACS found" << endl;
                  introduction_found_flag=true;
               }
               else if (cleaned_substring=="keywords")
               {
                  cout << "Keywords found" << endl;
                  introduction_found_flag=true;
               }
               else if (cleaned_substring=="contents")
               {
                  cout << "contents found" << endl;
                  introduction_found_flag=true;
               }
               else if (cleaned_substring=="subject")
               {
                  cout << "subject found" << endl;
                  introduction_found_flag=true;
               }
               if (introduction_found_flag) break;

            } // astro_flag conditional
            

// Ignore short strings:

            if (cleaned_substring.size() < 3) continue;

// Ignore "words" containing "garbage":

            vector<string> garbage_strings;
            garbage_strings.push_back("qqq");
            garbage_strings.push_back("xxx");

            bool garbage_continue_flag=false;
            for (int g=0; g<garbage_strings.size(); g++)
            {
               int garbage_location=stringfunc::first_substring_location(
                  cleaned_substring,garbage_strings[g]);
               if (garbage_location > 0) 
               {
                  cout << "garbage_string = " << garbage_strings[g]
                       << " cleaned_substring = " << cleaned_substring
                       << endl;
                  garbage_continue_flag=true;
//                  outputfunc::enter_continue_char();
               }
            }
            if (garbage_continue_flag) continue;

// Stem cleaned substring:

// FAKE FAKE:  Sun Dec 23, 2012 at 2 pm
// Experiment with NOT stemming:

            string stemmed_substring=cleaned_substring;

/*
            string stemmed_substring=stringfunc::stem_word(cleaned_substring);
            stemmed_substring=
               stringfunc::remove_leading_whitespace(stemmed_substring);
            stemmed_substring=
               stringfunc::remove_trailing_whitespace(stemmed_substring);
*/

// Ignore stemmed strings which match "stop" words:

            stop_word_iter=stop_word_map.find(stemmed_substring);
            if (stop_word_iter != stop_word_map.end()) continue;

            word_iter=multi_doc_word_map.find(stemmed_substring);
            if (word_iter != multi_doc_word_map.end())
            {
               int word_index=word_iter->second;
               int word_freq=term_frequency_ptr->get(word_index);
               term_frequency_ptr->put(word_index,word_freq+1);
               n_document_strings++;
            }
         } // loop over index s labeling substring in current line
      } // loop over index i labeling line within current text file

//      cout << "     n_document_strings = " 
//           << n_document_strings << endl;
      
// Compute maximum term frequency for current document:

      int max_term_freq=0;
      for (int t=0; t<n_words; t++)
      {
         int curr_term_freq=term_frequency_ptr->get(t);
         max_term_freq=basic_math::max(max_term_freq,curr_term_freq);
      }
//      cout << "max_term_freq = " << max_term_freq << endl;

// Divide raw term frequency by max term frequency in order to
// minimize variation due to document length.  Then store renormalized
// term frequencies within rows of *term_freq_sparse_matrix_ptr:

      for (int t=0; t<n_words; t++)
      {
         int curr_term_freq=term_frequency_ptr->get(t);

         double renorm_term_freq=0;

// When working with "documents" generated from topic keywords, we
// have empirically found that term frequency should grow
// logarithmically rather than linearly with word count:

         if (doc_level >= 1)
         {
            if (curr_term_freq > 0)
               renorm_term_freq=1+log(curr_term_freq);
         }
         else
         {
            renorm_term_freq=double(curr_term_freq)/max_term_freq;
         }
         
         term_frequency_ptr->put(t,renorm_term_freq);
         (*term_freq_sparse_matrix_ptr)(n,t)=renorm_term_freq;

// Increment *ndocs_containing_term_ptr for current document:

         if (renorm_term_freq > 0) ndocs_containing_term_ptr->put(
            t,ndocs_containing_term_ptr->get(t)+1);

      } // loop over index t lableing terms

      if (astro_flag && !introduction_found_flag)
      {
         cout << "No intro found" << endl;
         outputfunc::enter_continue_char();
      }
/*
      if (reuters_flag && !reuters_end_found_flag)
      {
         cout << "No end to reuters article found!" << endl;
         cout << "n = " << n 
              << " text_filename = " << text_filenames[n] << endl;
         for (int l=filefunc::text_line.size()-5; l<filefunc::text_line.size();
              l++)
         {
            cout << filefunc::text_line[l] << endl;
         }
         cout << endl;
         outputfunc::enter_continue_char();
      }
*/

   } // loop over index n labeling input text files
   delete term_frequency_ptr;

// Compute inverse document frequency as indicator of individual word
// importance:

   vector<int> ndocs;
   vector<double> word_importance;
   genvector* inverse_doc_frequency_ptr=new genvector(n_words);   
   for (int j=0; j<n_words; j++)
   {
      ndocs.push_back(basic_math::round(ndocs_containing_term_ptr->get(j)));

      double quotient=double(n_text_files)/
         (1+ndocs_containing_term_ptr->get(j));
      double idf=log(quotient);

      if (ndocs.back() <= 1) idf=0;

      inverse_doc_frequency_ptr->put(j,idf);
      word_importance.push_back(idf);
//      cout << "j = " << j 
//           << " ndocs = " << ndocs.back()
//           << " idf = " << idf << endl;
   }

   delete ndocs_containing_term_ptr;

// Export file containing words sorted according to their inverse
// document frequencies:

   cout << "n_words = " << n_words << endl;
   cout << "word_importance.size() = " << word_importance.size()
        << " word_list.size() = " << word_list.size() << endl;
   templatefunc::Quicksort_descending(word_importance,ndocs,sorted_word_list);

   string importance_filename=text_subdir+"word_importance.dat";
   ofstream importance_stream;
   filefunc::openfile(importance_filename,importance_stream);
   importance_stream << "# Word  ndocs_frac  importance" << endl << endl;
   for (int j=0; j<n_words; j++)
   {
      double ndocs_frac=double(ndocs[j])/double(n_text_files);
      importance_stream << sorted_word_list[j] << "      "
                        << ndocs_frac << "      "
                        << word_importance[j] << endl;
   }
   filefunc::closefile(importance_filename,importance_stream);
   string banner="Exported "+importance_filename;
   outputfunc::write_banner(banner);

// Compute term-frequency inverse document frequency (TF-IDF) ratios:

   banner="Calculating TF-IDF ratios";
   outputfunc::write_banner(banner);
   genvector* tfidf_ptr=new genvector(n_words);
   genvector* normalized_tfidf_ptr=new genvector(n_words);

   gmm::row_matrix< gmm::wsvector<float> >* word_doc_sparse_matrix_ptr=
      new gmm::row_matrix< gmm::wsvector<float> >(n_words,n_text_files);
   gmm::clear(*word_doc_sparse_matrix_ptr);

   int n_nonzero_values=0;
   for (int n=0; n<n_text_files; n++)
   {
      tfidf_ptr->clear_values();
      for (int t=0; t<n_words; t++)
      {
         tfidf_ptr->put(
            t,(*term_freq_sparse_matrix_ptr)(n,t)*
            inverse_doc_frequency_ptr->get(t));
      }

// Renormalize *tfidf_ptr so that it has unit length:

      *normalized_tfidf_ptr=tfidf_ptr->unitvector();

      double TINY=1E-9;
      for (int t=0; t<n_words; t++)
      {
         double curr_tfidf=normalized_tfidf_ptr->get(t);
         if (fabs(curr_tfidf) > TINY)
         {
            (*word_doc_sparse_matrix_ptr)(t,n)=curr_tfidf;
            n_nonzero_values++;
         }
      }

   } // loop over index n labeling input text files

   delete inverse_doc_frequency_ptr;
   delete tfidf_ptr;
   delete normalized_tfidf_ptr;
   
   double total_time;
   genmatrix* reduced_docs_matrix_ptr=NULL;
   genmatrix* reduced_words_matrix_ptr=NULL;
   if (compute_doc_overlap_flag)
   {
      cout << "n_words = " << n_words 
           << " n_documents = " << n_text_files << endl;
      outputfunc::print_elapsed_time();

// Export sparse word-doc matrix:

      string sparse_matrix_filename=text_subdir+"sparse_matrix_txt.dat";
      mathfunc::export_to_sparse_text_format(
         word_doc_sparse_matrix_ptr,n_nonzero_values,
         sparse_matrix_filename);

      sparse_matrix_filename=text_subdir+"sparse_matrix_bin.dat";
      mathfunc::export_to_sparse_binary_format(
         word_doc_sparse_matrix_ptr,n_nonzero_values,
         sparse_matrix_filename);

      banner="Exported sparse word-doc matrix";
      outputfunc::write_banner(banner);

// If the word-document matrix corresponds to original, complete
// documents, we follow the Latent Semantic Analysis procedure of
// reducing the matrix' dimension from n_words x n_text_files to
// k_dims=300 x n_text_files.  On the other hand, we work with the
// existing word-document matrix for doc_level >= 1 input documents
// containing topic keywords:
      
//      int k_dims=100;
      int k_dims=300;
//      int k_dims=1000;


      if (doc_level >= 1)
      {
         k_dims=n_words;
         reduced_docs_matrix_ptr=new genmatrix(n_words,n_text_files);
         genvector curr_doc(n_words);
         for (int c=0; c<n_text_files; c++)
         {
            for (int r=0; r<n_words; r++)
            {
               double tfidf=(*word_doc_sparse_matrix_ptr)(r,c);
               curr_doc.put(r,tfidf);
            } // loop over index r labeling words
            reduced_docs_matrix_ptr->put_column(c,curr_doc.unitvector());
         } // loop over index c labeling text files
      }
      else
      {

// Perform SVD on word-document matrix:

         mathfunc::sparse_SVD_approximation(
            word_doc_sparse_matrix_ptr,k_dims,n_nonzero_values,text_subdir);

         genmatrix* Utrans_ptr=mathfunc::import_from_dense_text_format(
            text_subdir+"svd-Ut.dat");
         genmatrix* Vtrans_ptr=mathfunc::import_from_dense_text_format(
            text_subdir+"svd-Vt.dat");

         if (Utrans_ptr->get_mdim() != Vtrans_ptr->get_mdim())
         {
            cout << "U.ndim = " << Utrans_ptr->get_mdim() << endl;
            cout << "Vtrans.mdim = " << Vtrans_ptr->get_mdim() << endl;
            cout << "These two dimensions should be equal!" << endl;
            exit(-1);
         }
         int ddim=Utrans_ptr->get_mdim();
         genmatrix* D_ptr=mathfunc::import_from_diagonal_text_format(
            ddim,text_subdir+"svd-S.dat");

//   cout << "Utrans = " << *Utrans_ptr << endl;
//   cout << "D = " << *D_ptr << endl;
//   cout << "Vtrans = " << *Vtrans_ptr << endl;

         banner="Calculating reduced document overlap matrix";
         outputfunc::write_banner(banner);

         reduced_docs_matrix_ptr=new genmatrix(k_dims,n_text_files);
         *reduced_docs_matrix_ptr = *D_ptr * *Vtrans_ptr;

         reduced_words_matrix_ptr=new genmatrix(n_words,k_dims);
         *reduced_words_matrix_ptr=Utrans_ptr->transpose() * *D_ptr;

         delete Utrans_ptr;
         delete Vtrans_ptr;
         delete D_ptr;

// Export k_dims x n_text_files reduced documents matrix to text and
// binary output files:

         string reduced_docs_text_filename=text_subdir+"reduced_docs_txt.dat";
         reduced_docs_matrix_ptr->export_to_dense_text_format(
            reduced_docs_text_filename);

         string reduced_docs_binary_filename=
            text_subdir+"reduced_docs_bin.dat";
         reduced_docs_matrix_ptr->export_to_dense_binary_format(
            reduced_docs_binary_filename);

         string reduced_words_text_filename=
            text_subdir+"reduced_words_txt.dat";
         reduced_words_matrix_ptr->export_to_dense_text_format(
            reduced_words_text_filename);
      
         string reduced_words_binary_filename=
            text_subdir+"reduced_words_bin.dat";
         reduced_words_matrix_ptr->export_to_dense_binary_format(
            reduced_words_binary_filename);

         genvector curr_doc(k_dims);
         for (int c=0; c<n_text_files; c++)
         {
            reduced_docs_matrix_ptr->get_column(c,curr_doc);
            reduced_docs_matrix_ptr->put_column(c,curr_doc.unitvector());
         }
      } // doc_level==1 conditional
      
      outputfunc::print_elapsed_time();

      genvector curr_doc(k_dims),next_doc(k_dims);

/*
// Compute edge weight probability distribution:

      banner="Calculating edge weight probability distribution";
      outputfunc::write_banner(banner);
      vector<double>* edge_weights_ptr=new vector<double>;

      for (int i=0; i<n_text_files; i++)
      {
         if (i%100==0) cout << double(i)/n_text_files << " " << flush;
         reduced_docs_matrix_ptr->get_column(i,curr_doc);
         for (int j=i+1; j<n_text_files; j++)
         {
            reduced_docs_matrix_ptr->get_column(j,next_doc);
            double curr_edge_weight=curr_doc.dot(next_doc);
            edge_weights_ptr->push_back(curr_edge_weight);
         } // loop over index j
      } // loop over index i 
      cout << endl;

      prob_distribution prob_weights(*edge_weights_ptr,1000);
      delete edge_weights_ptr;
      prob_weights.writeprobdists(false);
      string unix_cmd="mv prob_density.* "+text_subdir;
      sysfunc::unix_command(unix_cmd);
      unix_cmd="mv prob_cum.* "+text_subdir;
      sysfunc::unix_command(unix_cmd);

      outputfunc::print_elapsed_time();
*/

//   double weight_scale_factor=1;
      double weight_scale_factor=1000;
      cout << "weight_scale_factor = " << weight_scale_factor << endl;

// Export document edge list:

//      const double min_edge_weight=weight_scale_factor*0.05;
//      const double min_edge_weight=weight_scale_factor*0.1;
//      const double min_edge_weight=weight_scale_factor*0.15;
//      const double min_edge_weight=weight_scale_factor*0.2;
      double min_edge_weight=weight_scale_factor*0.25;
      if (doc_level==1) min_edge_weight=weight_scale_factor*0.01;

      string output_filename=text_subdir+"docs_edgelist.dat";
      ofstream outstream;
      filefunc::openfile(output_filename,outstream);

      outstream << "# Edge weight threshold = " << min_edge_weight << endl;
      outstream << "# NodeID  NodeID'  Edge weight" << endl;
      outstream << endl;

      int n_edges=0;
      double max_edge_weight=0;
      for (int i=0; i<n_text_files; i++)
      {
         reduced_docs_matrix_ptr->get_column(i,curr_doc);
         for (int j=i+1; j<n_text_files; j++)
         {
            reduced_docs_matrix_ptr->get_column(j,next_doc);
            double edge_weight=weight_scale_factor*curr_doc.dot(next_doc);
            if (edge_weight < min_edge_weight) continue;
            max_edge_weight=basic_math::max(max_edge_weight,edge_weight);
            if (edge_weight < 10)
            {
               outstream << i << "  " << j << "  " << edge_weight << endl;
            }
            else
            {
               outstream << i << "  " << j << "  " << int(edge_weight) << endl;
            }
            n_edges++;
         } // loop over index j
      } // loop over index i 

      filefunc::closefile(output_filename,outstream);

      cout << "Number of documents = " << n_text_files << endl;
      cout << "Number of words in document histograms = " << n_words << endl;
      cout << "Minimum corpus word frequency = " << min_corpus_word_freq 
           << endl;
      cout << "Number of edges = " << n_edges << endl;
      cout << "Min edge weight = " << min_edge_weight << endl;
      cout << "Max edge weight = " << max_edge_weight << endl;
      banner="Exported document edge list to "+output_filename;

      outputfunc::print_elapsed_time();
      outputfunc::write_big_banner(banner);

   } // compute_doc_overlap_flag conditional

   int first_doc_ID,second_doc_ID;
   genvector* normalized_tfidf1_ptr=new genvector(n_words);
   genvector* normalized_tfidf2_ptr=new genvector(n_words);

   outputfunc::print_elapsed_time();
   
   while (true)
   {
      cout << "Enter ID for first document:" << endl;
      cin >> first_doc_ID;
      cout << "Enter ID for second document:" << endl;
      cin >> second_doc_ID;

      for (int t=0; t<n_words; t++)
      {
         normalized_tfidf1_ptr->put(
            t,(*term_freq_sparse_matrix_ptr)(first_doc_ID,t));
         normalized_tfidf2_ptr->put(
            t,(*term_freq_sparse_matrix_ptr)(second_doc_ID,t));
      }

//      doc_word_histograms_ptr->get_row(first_doc_ID,*normalized_tfidf1_ptr);
//      doc_word_histograms_ptr->get_row(second_doc_ID,*normalized_tfidf2_ptr);

/*      
      vector<double> term_freqs;
      vector<string> terms;
      for (int i=0; i<n_words; i++)
      {
         double curr_tf=normalized_tfidf1_ptr->get(i);
         if (curr_tf > 0)
         {
            term_freqs.push_back(curr_tf);
            terms.push_back(word_list[i]);
         }
      }
      templatefunc::Quicksort_descending(term_freqs,terms);

      cout << "For doc #1:" << endl;
      for (int i=0; i<term_freqs.size(); i++)
      {
         cout << i << "  term = " << terms[i]
              << " TF = " << term_freqs[i] << endl;
      }
*/

      double dotproduct=normalized_tfidf1_ptr->dot(
         *normalized_tfidf2_ptr);
      cout << "dotproduct = " << dotproduct << endl << endl;

      vector<double> word_overlap;
      for (int n=0; n<n_words; n++)
      {
         word_overlap.push_back(
            normalized_tfidf1_ptr->get(n)*normalized_tfidf2_ptr->get(n));
      }

      sorted_word_list.clear();
      for (int n=0; n<n_words; n++)
      {
         sorted_word_list.push_back(word_list[n]);
      }
      
      templatefunc::Quicksort_descending(word_overlap,sorted_word_list);

      int n=0;
      const double min_word_overlap=0.05;
      while (word_overlap[n] > min_word_overlap)
      {
         cout << sorted_word_list[n] << "  " << word_overlap[n] << endl;
         n++;
      }
      cout << endl;
      cout << "1st doc filename = " << text_filenames[first_doc_ID] << endl;
      cout << "2nd doc filename = " << text_filenames[second_doc_ID] << endl;
      cout << endl;
   }
   delete normalized_tfidf1_ptr;
   delete normalized_tfidf2_ptr;
   delete term_freq_sparse_matrix_ptr;
   delete word_doc_sparse_matrix_ptr;
   delete reduced_docs_matrix_ptr;
   delete reduced_words_matrix_ptr;
}


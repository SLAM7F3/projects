// ==========================================================================
// SVM_SYMBOL reads in 9xK dimensional features for N ~32x~32 symbol
// windows and N ~32x~32 non-text windows generated by
// POOL_SYMBOL_FEATURES.  It then uses Davis King's DLIB library to
// load the input samples along with their positive and negative
// labels.  After the ordering of the input samples is randomized,
// cross-correlation is performed in order to estimate a reasonable
// value for a linear-SVM slack variable C.  DLIB then trains binary
// decision and probabilistic decision functions. Serialized versions
// of the binary and probabilistic decision functions are exported to
// output binary files.

// 				svm_symbol

// ==========================================================================
// Last updated on 9/30/12; 10/4/12; 10/11/12
// ==========================================================================


#include <fstream>
#include <iostream>
#include <string>
#include <vector>
#include <flann/flann.hpp>
#include <flann/io/hdf5.h>
#include "dlib/svm.h"

#include "astro_geo/Clock.h"
#include "general/filefuncs.h"
#include "general/outputfuncs.h"
#include "general/stringfuncs.h"
#include "general/sysfuncs.h"
#include "time/timefuncs.h"

using std::cin;
using std::cout;
using std::endl;
using std::flush;
using std::ifstream;
using std::ios;
using std::ofstream;
using std::string;
using std::vector;

using namespace dlib;

// ==========================================================================
int main(int argc, char *argv[])
// ==========================================================================
{
   std::set_new_handler(sysfunc::out_of_memory);

//   cout << "Sleeping for 6 hours:" << endl;
//   sleep(6*3600);

   const int K=1024;

   std::vector<string> symbol_names;
//   symbol_names.push_back("yellow_radiation");  
//   symbol_names.push_back("orange_biohazard");   
//   symbol_names.push_back("blue_radiation");     
//   symbol_names.push_back("blue_water");	   
   symbol_names.push_back("blue_gas");	   
//   symbol_names.push_back("red_stop");	   
//   symbol_names.push_back("bw_skull");
//   symbol_names.push_back("bw_eat");
//   symbol_names.push_back("green_start");	   

   string final_signs_subdir="./images/final_signs/";
//   string ppt_signs_subdir="./images/ppt_signs/";
   string symbols_input_subdir=final_signs_subdir;

/*
  string symbol_name;
  cout << "Enter symbol name:" << endl;
  cout << "  yellow_radiation,orange_biohazard,blue_water" << endl;
  cout << "  blue_radiation,blue_gas,red_stop" << endl;
  cout << "  green_start,bw_skull,bw_eat:" << endl;
  cin >> symbol_name;
*/

   bool RGB_pixels_flag=false;
//   bool RGB_pixels_flag=true;
   cout << "RGB_pixels_flag = " << RGB_pixels_flag << endl;

/*
   int D=64*3;	// RGB color
   if (!RGB_pixels_flag)
   {
      D=64;	// greyscale
   }
*/

   for (unsigned int s=0; s<symbol_names.size(); s++)
   {
      string symbol_name=symbol_names[s];

      string banner="Processing symbol "+symbol_name;
      outputfunc::write_big_banner(banner);

      string symbol_filename=symbols_input_subdir+symbol_name+".png";
      string synthetic_subdir=symbols_input_subdir+"synthetic_symbols/";
      string synthetic_symbols_subdir=synthetic_subdir+symbol_name+"/";
      string dictionary_subdir=synthetic_symbols_subdir;

      cout << "dictionary_subdir = " << dictionary_subdir << endl;

      Clock clock;
      clock.set_time_based_on_local_computer_clock();
      cout << endl;
      cout << "Starting time: " << clock.YYYY_MM_DD_H_M_S() << endl << endl;

// Import text and non-text window descriptors:

      flann::Matrix<float> text_descriptors,nontext_descriptors;

      cout << "Loading text window descriptors" << endl;
      string text_descriptors_hdf5_filename=dictionary_subdir+
         "text_window_descriptors.hdf5";
      flann::load_from_file(
         text_descriptors,text_descriptors_hdf5_filename.c_str(),
         "window_descriptors");

      cout << "text_descriptors.rows = " << text_descriptors.rows << endl;
      cout << "text_descriptors.cols = " << text_descriptors.cols << endl;
      int Ntext=text_descriptors.rows;
      
      cout << "Loading non-text window descriptors" << endl;
      string nontext_descriptors_hdf5_filename=dictionary_subdir+
         "nontext_window_descriptors.hdf5";
      flann::load_from_file(
         nontext_descriptors,nontext_descriptors_hdf5_filename.c_str(),
         "window_descriptors");

      cout << "nontext_descriptors.rows = " << nontext_descriptors.rows 
           << endl;
      cout << "nontext_descriptors.cols = " << nontext_descriptors.cols 
           << endl;
      int Nnontext=nontext_descriptors.rows;

// Define N as geometric mean of Ntext and Nnontext:

      int N=sqrt(Ntext*Nnontext);
      cout << "Ntext = " << Ntext << " Nnontext = " << Nnontext << endl;
      cout << "N = sqrt(Ntext*Nnontext) = " << N << endl;

      double positive_to_negative_data_ratio=
         double(Ntext)/double(Nnontext);
      cout << "Positive-to-negative data ratio = " 
           << positive_to_negative_data_ratio << endl;

      const unsigned int nineK=9*K;
      if (nineK != text_descriptors.cols)
      {
         cout << "nineK = " << nineK 
              << " text_descriptors.cols = " << text_descriptors.cols
              << endl;
         exit(-1);
      }

// The svm functions use column vectors to contain a lot of the
// data on which they operate. So the first thing we do here is
// declare a convenient typedef.

// This typedef declares a matrix with K rows and 1 column.  It will
// be the object that contains each of our K dimensional samples. 

      typedef matrix<double, nineK, 1> sample_type;

// This is a typedef for the type of kernel we are going to use in
// this example.  In this case I have selected the linear kernel that
// can operate on our K-dim sample_type objects

      typedef linear_kernel<sample_type> kernel_type;


// On 10/11/12, we asked Davis for how to eventually work with much
// larger numbers of training samples than O(10**5).  He said that we
// basically must move towards working with sparse vectors.  Doing so
// will free up RAM and thereby increase the number of training
// samples we can input.  See svm_sparse_ex.cpp for the syntax for
// Davis' sparse vectors.  Also, the previous typedef will become

// 	typedef sparse_linear_kernel<sample_type> kernel_type

// [Davis also stressed that dotproducting input samples with
// dictionaries must be followed by some nonlinear threshold.  For
// Coates-Ng classification, the scalar nonlinear function is
// z=max{0,|Dx|-alpha} where alpha=0.5 . If no such nonlinear
// thresholding takes place, then linear classification with 9*N
// dictionary elements is mathematically equivalent to working with
// the initial image patch and not bothering with any dictionary
// dotproducting.]

// Now we make objects to contain our samples and their respective
// labels.

      std::vector<sample_type> samples;
      std::vector<double> labels;
      samples.reserve(Ntext+Nnontext);
      labels.reserve(Ntext+Nnontext);

      sample_type samp;

// Load text data into samples and labels objects:

      for (int n=0; n<Ntext; n++)
      {
         for (unsigned int k=0; k<nineK; k++)
         {
            samp(k) = text_descriptors[n][k];
            double val=text_descriptors[n][k];
            if (val > POSITIVEINFINITY)
            {
               cout << "n = " << n << " k = " << k 
                    << " text feature = " << text_descriptors[n][k]
                    << endl;
            }
         }
         samples.push_back(samp);
         labels.push_back(1);
      } // loop over index n

// Load non-text data into samples and labels objects:

      for (int n=0; n<Nnontext; n++)
      {
         for (unsigned int k=0; k<nineK; k++)
         {
            samp(k) = nontext_descriptors[n][k];
            double val=nontext_descriptors[n][k];
            if (val > POSITIVEINFINITY)
            {
               cout << "n = " << n << " k = " << k 
                    << " nontext feature = " << text_descriptors[n][k]
                    << endl;
            }
         }
         samples.push_back(samp);
         labels.push_back(-1);
      } // loop over index n

// Here we normalize all the samples by subtracting their mean and
// dividing by their standard deviation. This is generally a good idea
// since it often heads off numerical stability problems and also
// prevents one large feature from smothering others.  Doing this
// doesn't matter much in this example so I'm just doing this here so
// you can see an easy way to accomplish this with the library.

      vector_normalizer<sample_type> normalizer;
      // let the normalizer learn the mean and standard deviation of the samples
      normalizer.train(samples);
      // now normalize each sample
      for (unsigned long i = 0; i < samples.size(); ++i)
      {
         samples[i] = normalizer(samples[i]); 
      }
   
// Now that we have some data we want to train on it.  However, there
// are two parameters to the training.  These are the nu and gamma
// parameters.  Our choice for these parameters will influence how
// good the resulting decision function is.  To test how good a
// particular choice of these parameters is we can use the
// cross_validate_trainer() function to perform n-fold cross
// validation on our training data.  However, there is a problem with
// the way we have sampled our distribution above.  The problem is
// that there is a definite ordering to the samples.  That is, the
// first half of the samples look like they are from a different
// distribution than the second half.  This would screw up the cross
// validation process but we can fix it by randomizing the order of
// the samples with the following function call.

      randomize_samples(samples, labels);

// Here we make an instance of the svm_c_linear_trainer object that uses our
// kernel type:

      svm_c_linear_trainer<kernel_type> trainer;
//   svm_c_ekm_trainer<kernel_type> trainer;


/*
// CROSS VALIDATION STARTS HERE:

      double min_C=0.0016*N;
      double max_C=0.01*N;

// Now we loop over some different C values to see how good
// they are.  Note that this is a very simple way to try out a few
// possible parameter choices.  You should look at the
// model_selection_ex.cpp program for examples of more sophisticated
// strategies for determining good parameter choices.

      double max_product_correct=-1;
      double best_product_pos_correct,best_product_neg_correct;
      double max_sum_correct=-1;
      double best_sum_pos_correct,best_sum_neg_correct;
      double best_product_C,best_sum_C;

      cout << "Performing cross validation" << endl;
      for (double C = min_C; C < max_C; C *= 2)
      {
// tell the trainer the parameters we want to use

// If # class2/# class1 = alpha, then c_class1 = alpha c_class2 (rule
// of thumb)

         double C_positive=C;
         double C_negative=C*positive_to_negative_data_ratio;
         trainer.set_c_class1(C_positive);
         trainer.set_c_class2(C_negative);
         cout << "C_positive = " << C_positive/double(N)
              << " C_negative = " << C_negative/double(N) << endl;

// Print out the cross validation accuracy for 3-fold cross validation
// using the current C coeffient. cross_validate_trainer() returns a
// row vector.  The first element of the vector is the fraction of +1
// training examples correctly classified and the second number is the
// fraction of -1 training examples correctly classified.

         matrix<double,1,2> pos_neg_results=
            cross_validate_trainer(trainer,samples,labels,3);
         double pos_correct=pos_neg_results(0,0);
         double neg_correct=pos_neg_results(0,1);
         
         double product_correct=pos_correct*neg_correct;
         if (product_correct > max_product_correct)
         {
            max_product_correct=product_correct;
            best_product_pos_correct=pos_correct;
            best_product_neg_correct=neg_correct;
            best_product_C=C;
         }
         
         double sum_correct=pos_correct+neg_correct;
         if (sum_correct > max_sum_correct)
         {
            max_sum_correct=sum_correct;
            best_sum_pos_correct=pos_correct;
            best_sum_neg_correct=neg_correct;
            best_sum_C = C;
         }

         cout << "  Cross validation accuracy: " 
              << cross_validate_trainer(trainer, samples, labels, 3)
              << endl;
         cout << "  Product_correct = " << product_correct
              << " sum_correct = " << sum_correct << endl << endl;
      }

      cout << "max_product_correct = " << max_product_correct << endl;
      cout << "best_product_pos_correct = " << best_product_pos_correct << endl;
      cout << "best_product_neg_correct = " << best_product_neg_correct << endl;
      cout << "best_product_C = " << best_product_C << endl;
      cout << endl;

      cout << "max_sum_correct = " << max_sum_correct << endl;
      cout << "best_sum_pos_correct = " << best_sum_pos_correct << endl;
      cout << "best_sum_neg_correct = " << best_sum_neg_correct << endl;
      cout << "best_sum_C = " << best_sum_C << endl;

      exit(-1);

// After performing cross-validation on 10K positive and 10K negative
// radiation1 symbols on 8/16/12, we found C/N=0.1 as the best estimate
// for the linear SVM slack variable:

// C/N = 0.1
//   Cross validation accuracy:    0.998 0.992999 

// After performing cross-validation on 10K positive and 20K negative
// greyscale radiation1 symbols on 8/20/12, we found C/N=0.03 as good estimate
// for the linear SVM slack variable:

// C/N = 0.03
//   Cross validation accuracy:   0.9985 0.995128 

// -----------------------------------------------------------------------
// After performing cross-validation on 10K positive and 50K negative
// greyscale radiation1 symbols on 8/20/12, we found:

// C/N = 0.001
//   Cross validation accuracy: 0.994499 0.984795 

// C/N = 0.003
//   Cross validation accuracy:   0.9973 0.992501 

// -----------------------------------------------------------------------
// After performing cross-validation on 10K positive and 82K negative
// greyscale radiation1 symbols on 8/21/12, we found:

// C/N = 0.0016
//   Cross validation accuracy:   0.9957 0.986733 

// C/N = 0.0032
//   Cross validation accuracy:   0.9974 0.992886 

// C/N = 0.0064
//   Cross validation accuracy:   0.9981 0.995907 

// -----------------------------------------------------------------------
// After performing cross-validation on 10K positive and 97K negative
// greyscale biohazard symbols on 8/21/12, we found:

// C/N = 0.0016
//   Cross validation accuracy: 0.990999 0.996583 

// C/N = 0.0032
//   Cross validation accuracy: 0.992999 0.997258 

// -----------------------------------------------------------------------
// After performing cross-validation on 10K positive and 97K negative
// greyscale radiation2 symbols on 8/22/12, we found:

// C/N = 0.0016
//   Cross validation accuracy: 0.991899 0.996859 

// C/N = 0.0032
//  Cross validation accuracy:   0.9957 0.997494 

// C/N = 0.0064
//   Cross validation accuracy:   0.9968 0.998179 

// -----------------------------------------------------------------------
// After performing cross-validation on 10K positive and 67K negative
// color radiation1 symbols on 8/21/12, we found:

// C/N = 3e-05
//   Cross validation accuracy: 0.9971 0.9775 

// C/N = 6e-05
//   Cross validation accuracy:   0.9979 0.988106 

// C/N = 0.0001
//   Cross validation accuracy:   0.9983 0.992988 

// C/N = 0.0005
//   Cross validation accuracy:   0.9995 0.998254 

// C/N = 0.001
//   Cross validation accuracy:   0.9997 0.998964 
 
// -----------------------------------------------------------------------
// After performing cross-validation on 5K positive and 97K negative
// greyscale radiation2 symbols on 9/28/12, we found:

//      C_positive = 0.0016 C_negative = 8.18431e-05
//         Cross validation accuracy: 0.998199 0.986547 

//         C_positive = 0.0032 C_negative = 0.000163686
//         Cross validation accuracy: 0.998199 0.992481 

//         C_positive = 0.0064 C_negative = 0.000327372
//         Cross validation accuracy: 0.998199 0.996143 

// CROSS VALIDATION ENDS HERE
*/


      double C=0.0032*N;
      
      double C_positive=C;
      double C_negative=C*positive_to_negative_data_ratio;

      cout << "******************************************************" << endl;
      cout << "USING CROSS VALIDIATION PARAMETERS C_POSITIVE = "
           << C_positive << " AND C_NEGATIVE = " << C_negative << endl;
      cout << "******************************************************" << endl;
      cout << endl;

//   trainer.set_c(C);

      trainer.set_c_class1(C_positive);
      trainer.set_c_class2(C_negative);

//    trainer.be_verbose();

// Now we train on the full set of data and obtain the resulting
// decision function.  The decision function will return values >= 0
// for samples it predicts are in the +1 class and numbers < 0 for
// samples it predicts to be in the -1 class.

      typedef decision_function<kernel_type> dec_funct_type;
      typedef normalized_function<dec_funct_type> funct_type;

// Here we are making an instance of the normalized_function object.
// This object provides a convenient way to store the vector
// normalization information along with the decision function we are
// going to learn.

      funct_type learned_function;
      learned_function.normalizer = normalizer;  // save normalization 
//   information

// Perform the actual SVM training and save the results.  Print out
// the number of support vectors in the resulting decision function:

      timefunc::initialize_timeofday_clock();
      cout << endl;
      cout << "-----------------------------------------------------" << endl;
      cout << "Starting to train binary decision function:" << endl;
      learned_function.function = trainer.train(samples, labels); 
      cout << "Elapsed time = " 
           << timefunc::elapsed_timeofday_time()/60 << " mins = " 
           << timefunc::elapsed_timeofday_time()/3600 << " hours" << endl;
      cout << "Number of support vectors in our learned_function =  " 
           << learned_function.function.basis_vectors.nr() << endl;
      cout << "Number of training samples: N = " << N << endl;

// Another thing that is worth knowing is that just about everything
// in dlib is serializable. So for example, you can save the
// learned_pfunct object to disk and recall it later like so:

      string learned_funcs_subdir="./learned_functions/"+symbol_name+"/";
      filefunc::dircreate(learned_funcs_subdir);

      string output_filename=learned_funcs_subdir+
         "symbols_Ng_bifunc_"+stringfunc::number_to_string(Ntext)+"_"+
         stringfunc::number_to_string(Nnontext)+".dat";
      ofstream fout(output_filename.c_str(),ios::binary);
      serialize(learned_function,fout);
      fout.close();

      banner="Exported learned binary function to "+output_filename;
      outputfunc::write_banner(banner);

      clock.set_time_based_on_local_computer_clock();
      cout << "Current time: " << clock.YYYY_MM_DD_H_M_S() << endl;


// We can also train a decision function that reports a well
// conditioned probability instead of just a number > 0 for the +1
// class and < 0 for the -1 class.  An example of doing that follows:

      typedef probabilistic_decision_function<kernel_type> 
         probabilistic_funct_type;  
      typedef normalized_function<probabilistic_funct_type> pfunct_type;

      pfunct_type learned_pfunct; 
      learned_pfunct.normalizer = normalizer;

      timefunc::initialize_timeofday_clock();
      cout << endl;
      cout << "-----------------------------------------------------" << endl;
      cout << "Starting to train probabilistic decision function:" << endl;
      learned_pfunct.function = train_probabilistic_decision_function(
         trainer, samples, labels, 3);
      cout << "Elapsed time = " 
           << timefunc::elapsed_timeofday_time()/60 << " mins = " 
           << timefunc::elapsed_timeofday_time()/3600 << " hours" << endl;

// Now we have a function that returns the probability that a given
// sample is of the +1 class.

// print out the number of support vectors in the resulting decision
// function. (it should be the same as in the one above)

      cout << "Number of support vectors in our learned_pfunct =  " 
           << learned_pfunct.function.decision_funct.basis_vectors.nr() 
           << endl;
      cout << "Number of training samples: N = " << N << endl;

// Another thing that is worth knowing is that just about everything
// in dlib is serializable. So for example, you can save the
// learned_pfunct object to disk and recall it later like so:

      output_filename=learned_funcs_subdir
         +"symbols_Ng_pfunct_"+stringfunc::number_to_string(Ntext)+"_"
         +stringfunc::number_to_string(Nnontext)+".dat";
      ofstream fout2(output_filename.c_str(),ios::binary);
      serialize(learned_pfunct,fout2);
      fout2.close();

      banner="Exported learned probabilistic function to "+output_filename;
      outputfunc::write_banner(banner);

      clock.set_time_based_on_local_computer_clock();
      cout << "Current time: " << clock.YYYY_MM_DD_H_M_S() << endl;

// Now lets open that file back up and load the function object it
// contains:

      ifstream fin(output_filename.c_str(),ios::binary);
      deserialize(learned_pfunct, fin);

      cout << "Text binary decision function = "
           << test_binary_decision_function(
              learned_function.function,samples,labels)
           << endl;

/*
// Note that there is also an example program that comes with dlib
// called the file_to_code_ex.cpp example.  It is a simple program
// that takes a file and outputs a piece of C++ code that is able to
// fully reproduce the file's contents in the form of a std::string
// object.  So you can use that along with the std::istringstream to
// save learned decision functions inside your actual C++ code files
// if you want.

// Lastly, note that the decision functions we trained above involved
// well over 200 basis vectors.  Support vector machines in general
// tend to find decision functions that involve a lot of basis
// vectors.  This is significant because the more basis vectors in a
// decision function, the longer it takes to classify new examples.
// So dlib provides the ability to find an approximation to the normal
// output of a trainer using fewer basis vectors.

// Here we determine the cross validation accuracy when we approximate
// the output using only 10 basis vectors.  To do this we use the
// reduced2() function.  It takes a trainer object and the number of
// basis vectors to use and returns a new trainer object that applies
// the necessary post processing during the creation of decision
// function objects.

   cout << "Cross validation accuracy with only 10 support vectors: " 
        << cross_validate_trainer(reduced2(trainer,10), samples, labels, 3);

// Lets print out the original cross validation score too for
// comparison:

   cout << "cross validation accuracy with all the original support vectors: " 
        << cross_validate_trainer(trainer, samples, labels, 3);

// When you run this program you should see that, for this problem,
// you can reduce the number of basis vectors down to 10 without
// hurting the cross validation accuracy.

// To get the reduced decision function out we would just do this:
   learned_function.function = reduced2(trainer,10).train(samples, labels);

// And similarly for the probabilistic_decision_function: 

   learned_pfunct.function = train_probabilistic_decision_function(
      reduced2(trainer,10), samples, labels, 3);

   cout << "Text reduced binary decision function = "
        << test_binary_decision_function(
           reduced2(null_trainer(learned_function.function), 50).train(
              samples,labels),samples,labels)
        << endl;
*/

   } // loop over index s labeling symbol names

}


// ==========================================================================
// SVM_SHAPE reads in 11-dimensional descriptors for text and non-text
// which were previously generated by COMPUTE_SHAPE_DESCRIPTORS.  It
// uses Davis King's DLIB library to train an SVM classifier with a
// gaussian a radial-basis kernel.  Binary and probabilistic
// decision functions which indicate whether a candidate extremal
// region may contain a text character or not are exported in
// serialized DLIB formats.  Subsequent programs can then use DLIB to
// import these trained decision functions.

// 				svm_shape

// ==========================================================================
// Last updated on 8/15/12; 8/16/12; 8/31/12
// ==========================================================================

#include <fstream>
#include <iostream>
#include <string>
#include <vector>
#include <flann/flann.hpp>
#include <flann/io/hdf5.h>
#include "dlib/svm.h"

#include "astro_geo/Clock.h"
#include "general/filefuncs.h"
#include "general/outputfuncs.h"
#include "general/stringfuncs.h"
#include "general/sysfuncs.h"
#include "time/timefuncs.h"

using std::cin;
using std::cout;
using std::endl;
using std::flush;
using std::ifstream;
using std::ios;
using std::ofstream;
using std::string;
using std::vector;

using namespace dlib;

// ==========================================================================
int main(int argc, char *argv[])
// ==========================================================================
{
   std::set_new_handler(sysfunc::out_of_memory);

   string char_type;
   cout << "Enter 'b' for bright chars against dark background " << endl;
   cout << " or 'd' for dark chars against bright background" << endl;
   cin >> char_type;

   bool bright_ccs_flag=true;
   if (char_type=="d")
   {
      bright_ccs_flag=false;
   }
   
   if (bright_ccs_flag)
   {
      cout << "Processing bright chars against dark background" << endl;
   }
   else
   {
      cout << "Processing dark chars against bright background" << endl;
   }


   string alphabet_subdir="./images/ppt_signs/alphabet/";
   string ccs_subdir=alphabet_subdir;
   if (bright_ccs_flag)
   {
      ccs_subdir += "bright_connected_components/";
   }
   else
   {
      ccs_subdir += "dark_connected_components/";
   }
   string synthetic_subdir=ccs_subdir+"synthetic_letters/";

   string symbol_name;
   cout << "Enter symbol name (e.g. 'biohazard','eat','skull'):" << endl;
   cin >> symbol_name;

   string extremal_regions_subdir=synthetic_subdir+symbol_name+"/";


//   const int n_descriptors=4;
//   const int n_descriptors=7;
   const int n_descriptors=11;

   Clock clock;
   clock.set_time_based_on_local_computer_clock();
   cout << endl;
   cout << "Starting time: " << clock.YYYY_MM_DD_H_M_S() << endl << endl;

// Import text and non-text extremal region shape descriptors:

   string substring="features";
   std::vector<string> data_filenames=
      filefunc::files_in_subdir_matching_substring(
         extremal_regions_subdir,substring);
   string text_features_filename=data_filenames[0];
   cout << "text_features_filename = " << text_features_filename << endl;

   filefunc::ReadInfile(text_features_filename);
   int Ntext=filefunc::text_line.size();
   
   flann::Matrix<float> text_descriptors(
      new float[Ntext*n_descriptors],Ntext,n_descriptors);
   for (int i=0; i<Ntext; i++)
   {
      std::vector<string> substrings=
         stringfunc::decompose_string_into_substrings(filefunc::text_line[i]);
      for (int j=0; j<n_descriptors; j++)
      {
         text_descriptors[i][j]=stringfunc::string_to_number(substrings[j+2]);
      }
   }
   cout << "Number of text features = " << Ntext << endl;

   string nontext_features_filename=alphabet_subdir+"nontext_features.dat";
   filefunc::ReadInfile(nontext_features_filename);
   int Nnontext=filefunc::text_line.size();

   flann::Matrix<float> nontext_descriptors(
      new float[Nnontext*n_descriptors],Nnontext,n_descriptors);
   for (int i=0; i<Nnontext; i++)
   {
      std::vector<string> substrings=
         stringfunc::decompose_string_into_substrings(filefunc::text_line[i]);

      for (int j=0; j<n_descriptors; j++)
      {
         nontext_descriptors[i][j]=
            stringfunc::string_to_number(substrings[j+2]);
      }
   }
   cout << "Number of non-text features = " << Nnontext << endl;

   double positive_to_negative_data_ratio=
      double(Ntext)/double(Nnontext);
   cout << "Positive-to-negative data ratio = " 
        << positive_to_negative_data_ratio << endl;

// The svm functions use column vectors to contain a lot of the
// data on which they operate. So the first thing we do here is
// declare a convenient typedef.

// This typedef declares a matrix with K rows and 1 column.  It will
// be the object that contains each of our K dimensional samples. 

   const int K=n_descriptors;
   typedef matrix<double, K, 1> sample_type;

// This is a typedef for the type of kernel we are going to use in
// this example.  In this case I have selected the linear kernel that
// can operate on our K-dim sample_type objects

    typedef radial_basis_kernel<sample_type> kernel_type;

// Now we make objects to contain our samples and their respective
// labels.

   std::vector<sample_type> samples;
   std::vector<double> labels;
   
   sample_type samp;

// Load data from text extremal regions into samples and labels
// objects:

   for (int n=0; n<Ntext; n++)
   {
      for (int k=0; k<K; k++)
      {
         samp(k) = text_descriptors[n][k];
      }
      samples.push_back(samp);
      labels.push_back(1);
   } // loop over index n

// Load data from non-text extremal regions into samples and labels
// objects:

   for (int n=0; n<Nnontext; n++)
   {
      for (int k=0; k<K; k++)
      {
         samp(k) = nontext_descriptors[n][k];
      }
      samples.push_back(samp);
      labels.push_back(-1);
   } // loop over index n

// Here we normalize all the samples by subtracting their mean and
// dividing by their standard deviation. This is generally a good idea
// since it often heads off numerical stability problems and also
// prevents one large feature from smothering others.  Doing this
// doesn't matter much in this example so I'm just doing this here so
// you can see an easy way to accomplish this with the library.

   vector_normalizer<sample_type> normalizer;
   // let the normalizer learn the mean and standard deviation of the samples
   normalizer.train(samples);
   // now normalize each sample
   for (unsigned long i = 0; i < samples.size(); i++)
      samples[i] = normalizer(samples[i]); 

// Now that we have some data we want to train on it.  However, there
// are two parameters to the training.  These are the nu and gamma
// parameters.  Our choice for these parameters will influence how
// good the resulting decision function is.  To test how good a
// particular choice of these parameters is we can use the
// cross_validate_trainer() function to perform n-fold cross
// validation on our training data.  However, there is a problem with
// the way we have sampled our distribution above.  The problem is
// that there is a definite ordering to the samples.  That is, the
// first half of the samples look like they are from a different
// distribution than the second half.  This would screw up the cross
// validation process but we can fix it by randomizing the order of
// the samples with the following function call.

   randomize_samples(samples, labels);

// Here we make an instance of the svm_c_trainer object that uses our
// kernel type:

   svm_c_trainer<kernel_type> trainer;
//   krr_trainer<kernel_type> trainer;

/*
// CROSS VALIDATION STARTS HERE:

// Now we loop over some different C values to see how good
// they are.  Note that this is a very simple way to try out a few
// possible parameter choices.  You should look at the
// model_selection_ex.cpp program for examples of more sophisticated
// strategies for determining good parameter choices.

// For 4 features, Davis King strongly suspects gaussian width sigma
// should be "fairly large" = i.e. >= O(1).  So gamma ~ 1/sigma should
// be <= O(1):

   double min_gamma=0.1;
   double max_gamma=.41;	
   double delta_gamma=2;

// Smaller values of C (for the svm_c_trainer) correspond to bigger
// margins which are desirable:

   double min_C=4;
//   double min_C=0.5; // smaller C means bigger margin
   double max_C=4.1;
   double delta_C=2;

// Smaller gamma --> larger gaussian support
// Smaller C --> bigger margin

   double max_pos_correct=-1;
   double max_product_correct=-1;
   double max_sum_correct=-1;

   double best_pos_correct_pos_correct,best_pos_correct_neg_correct;
   double best_product_pos_correct,best_product_neg_correct;
   double best_sum_pos_correct,best_sum_neg_correct;

   double best_pos_correct_gamma,best_pos_correct_C;
   double best_product_gamma,best_sum_gamma;
   double best_product_C,best_sum_C;

   cout << "Performing cross validation" << endl;
   for (double gamma=min_gamma; gamma < max_gamma; gamma *= delta_gamma)
   {
      trainer.set_kernel(kernel_type(gamma));
      for (double C = min_C; C < max_C; C *= delta_C)
      {

// If # class2/# class1 = alpha, then c_class1 = alpha c_class2 (rule
// of thumb)

         double C_positive=C;
         double C_negative=C*positive_to_negative_data_ratio;

         trainer.set_c_class1(C_positive);
         trainer.set_c_class2(C_negative);
//         trainer.set_c(C);

         cout << "C_positive = " << C_positive
              << " C_negative = " << C_negative << endl;

// Print out the cross validation accuracy for 3-fold cross validation
// using the current C coeffient. cross_validate_trainer() returns a
// row vector.  The first element of the vector is the fraction of +1
// training examples correctly classified and the second number is the
// fraction of -1 training examples correctly classified.

// Can hardwire cross correlation performance for one of the two
// classes using roc_c1_trainer() or roc_c2_trainer().  

         matrix<double,1,2> pos_neg_results=
            cross_validate_trainer(trainer,samples,labels,3);
//            cross_validate_trainer(
//               roc_c1_trainer(trainer,0.95),samples,labels,3);
         double pos_correct=pos_neg_results(0,0);
         double neg_correct=pos_neg_results(0,1);

         if (pos_correct > max_pos_correct)
         {
            max_pos_correct=pos_correct;
            best_pos_correct_pos_correct=pos_correct;
            best_pos_correct_neg_correct=neg_correct;
            best_pos_correct_gamma=gamma;
            best_pos_correct_C=C;
         }
         
         double product_correct=pos_correct*neg_correct;
         if (product_correct > max_product_correct)
         {
            max_product_correct=product_correct;
            best_product_pos_correct=pos_correct;
            best_product_neg_correct=neg_correct;
            best_product_gamma=gamma;
            best_product_C=C;
         }
         
         double sum_correct=pos_correct+neg_correct;
         if (sum_correct > max_sum_correct)
         {
            max_sum_correct=sum_correct;
            best_sum_pos_correct=pos_correct;
            best_sum_neg_correct=neg_correct;
            best_sum_gamma=gamma;
            best_sum_C = C;
         }

         cout << " gamma = " << gamma << " C = " << C 
              << "  Cross validation accuracy: " << pos_neg_results 
              << endl;
//         cout << "    Product_correct = " << product_correct
//              << " sum_correct = " << sum_correct << endl << endl;
      }

   } // loop over gamma

   cout << "max_pos_correct = " << max_pos_correct << endl;
   cout << "best_pos_correct_pos_correct = " 
        << best_pos_correct_pos_correct << endl;
   cout << "best_pos_correct_neg_correct = " 
        << best_pos_correct_neg_correct << endl;
   cout << "best_pos_correct_gamma = " << best_pos_correct_gamma << endl;
   cout << "best_pos_correct_C = " << best_pos_correct_C << endl;
   cout << endl;

   cout << "max_product_correct = " << max_product_correct << endl;
   cout << "best_product_pos_correct = " << best_product_pos_correct << endl;
   cout << "best_product_neg_correct = " << best_product_neg_correct << endl;
   cout << "best_product_gamma = " << best_product_gamma << endl;
   cout << "best_product_C = " << best_product_C << endl;
   cout << endl;

   cout << "max_sum_correct = " << max_sum_correct << endl;
   cout << "best_sum_pos_correct = " << best_sum_pos_correct << endl;
   cout << "best_sum_neg_correct = " << best_sum_neg_correct << endl;
   cout << "best_sum_gamma = " << best_sum_gamma<< endl;
   cout << "best_sum_C = " << best_sum_C << endl;


   exit(-1);
*/

// After performing cross-validation on 1.1K text & 3.5K non-text samples on 
// 7/6/12, we found the following parameter estimates:

//	max_product_correct = 0.798222
//	best_product_pos_correct = 0.947507
//	best_product_neg_correct = 0.842445
//	best_product_gamma = 3.2
//	best_product_C = 4

// After performing cross-validation on 6K text & 5K non-text samples
// on 7/9/12, we found the following parameter estimates:

//	max_product_correct = 0.763431
//	best_product_pos_correct = 0.927434
//	best_product_neg_correct = 0.823165
//	best_product_gamma = 3.2
//	best_product_C = 2

// After performing cross-validation on 6K text & 5K non-text samples
// for 7 shape features on 7/10/12, we found the following parameter
// estimates:

//	max_sum_correct = 1.8127
//	best_sum_pos_correct = 0.938983
//	best_sum_neg_correct = 0.873717
//	best_sum_gamma = 0.4
//	best_sum_C = 4

// After performing cross-validation on 6K text & 5K non-text samples
// for 11 shape features on 7/11/12, we found the following parameter
// estimates:

//	max_product_correct = 0.877921
//	best_product_pos_correct = 0.953652
//	best_product_neg_correct = 0.920589
//	best_product_gamma = 0.4
//	best_product_C = 4

// After performing cross-validation on 16K text & 11K non-text samples
// for 11 shape features on 7/11/12, we found the following parameter
// estimates:

//	gamma = 0.4 C = 2  Cross validation accuracy: 0.960216 0.922672 

// After performing cross-validation on 16K text & 11K non-text
// samples for 10 shape features on 8/6/12, we found the following
// parameter estimates:

// 	gamma = 0.2 C = 0.5  Cross validation accuracy: 0.999812        1 

// After performing cross-validation on 4K symbol & 11K non-text
// samples for 11 shapes features on 7/11/12, we found the following
// parameter estimates

// max_product_correct = 0.966841
// best_product_pos_correct = 0.983746
// best_product_neg_correct = 0.982816
// best_product_gamma = 0.4
// best_product_C = 4

// After performing cross validation on 14K TOC12 distinctive symbols
// and 11K non-text samples for 11 shape features on 8/8/12, we found

//	max_product_correct = 0.93203
//	best_product_pos_correct = 0.980779
//	best_product_neg_correct = 0.950296
//	best_product_gamma = 0.4
//	best_product_C = 4

// On 8/31/12, we ran cross validation on several thousand TOC12
// "letters" for both bright and dark connected components and several
// different symbols.  We found gamma=0.2 and C=4 are reasonable
// parameter estimates which yield cross validation results of at
// least 97%

// CROSS VALIDATION ENDS HERE

   double gamma=0.2;
   double C=4;
   
   double C_positive=C;
   double C_negative=C*positive_to_negative_data_ratio;

   cout << "******************************************************" << endl;
   cout << "USING CROSS VALIDIATION PARAMETERS C_POSITIVE = "
        << C_positive << " AND C_NEGATIVE = " << C_negative << endl;
   cout << "******************************************************" << endl;
   cout << endl;


   trainer.set_kernel(kernel_type(gamma));   

   trainer.set_c_class1(C_positive);
   trainer.set_c_class2(C_negative);
//   trainer.set_c(C);	// for svm_c_trainer

//   trainer.be_verbose(); // for KRR trainer
//   trainer.use_classification_loss_for_loo_cv();	// KRR trainer


// Now we train on the full set of data and obtain the resulting
// decision function.  The decision function will return values >= 0
// for samples it predicts are in the +1 class and numbers < 0 for
// samples it predicts to be in the -1 class.

   typedef decision_function<kernel_type> dec_funct_type;
   typedef normalized_function<dec_funct_type> funct_type;

// Here we are making an instance of the normalized_function object.
// This object provides a convenient way to store the vector
// normalization information along with the decision function we are
// going to learn.

   funct_type learned_function;
   learned_function.normalizer = normalizer;  // save normalization 
					      //   information

// Perform the actual SVM training and save the results.  Print out
// the number of support vectors in the resulting decision function:

   timefunc::initialize_timeofday_clock();
   cout << endl;

   cout << "-----------------------------------------------------" << endl;
   cout << "Starting to train binary decision function:" << endl;


   learned_function.function = trainer.train(samples, labels); 

   cout << "Elapsed time = " 
        << timefunc::elapsed_timeofday_time()/60 << " mins = " 
        << timefunc::elapsed_timeofday_time()/3600 << " hours" << endl;
   cout << "Number of support vectors in our learned_function =  " 
        << learned_function.function.basis_vectors.nr() << endl;
   cout << "Number of text+non-text training samples = " 
        << Ntext+Nnontext << endl;

// Another thing that is worth knowing is that just about everything
// in dlib is serializable. So for example, you can save the
// learned_pfunct object to disk and recall it later like so:

// We can also train a decision function that reports a well
// conditioned probability instead of just a number > 0 for the +1
// class and < 0 for the -1 class.  An example of doing that follows:

   typedef probabilistic_decision_function<kernel_type> 
      probabilistic_funct_type;  
   typedef normalized_function<probabilistic_funct_type> pfunct_type;

   pfunct_type learned_pfunct; 
   learned_pfunct.normalizer = normalizer;

   timefunc::initialize_timeofday_clock();
   cout << endl;
   cout << "-----------------------------------------------------" << endl;
   cout << "Starting to train probabilistic decision function:" << endl;

   learned_pfunct.function = train_probabilistic_decision_function(
      trainer, samples, labels, 3);

   cout << "Elapsed time = " 
        << timefunc::elapsed_timeofday_time()/60 << " mins = " 
        << timefunc::elapsed_timeofday_time()/3600 << " hours" << endl;

// Now we have a function that returns the probability that a given
// sample is of the +1 class.

// print out the number of support vectors in the resulting decision
// function. (it should be the same as in the one above)

   cout << "Number of support vectors in our learned_pfunct =  " 
        << learned_pfunct.function.decision_funct.basis_vectors.nr() << endl;
   cout << "Number of text+non-text training samples = " 
        << Ntext+Nnontext << endl;

   cout << "Text binary decision function = "
        << test_binary_decision_function(
           learned_function.function,samples,labels)
        << endl;


// Another thing that is worth knowing is that just about everything
// in dlib is serializable. So for example, you can save the
// learned_pfunct object to disk and recall it later like so:

   string pfunction_filename=extremal_regions_subdir;
   if (bright_ccs_flag)
   {
      pfunction_filename += "bright_";
   }
   else
   {
      pfunction_filename += "dark_";
   }
   pfunction_filename += symbol_name+"_pfunct.dat";
//      stringfunc::number_to_string(Ntext)+"_"+
//      stringfunc::number_to_string(Nnontext)+".dat";

   ofstream fout2(pfunction_filename.c_str(),ios::binary);
   serialize(learned_pfunct,fout2);
   fout2.close();

   string banner="Exported learned probabilistic function to "
      +pfunction_filename;
   outputfunc::write_banner(banner);

   clock.set_time_based_on_local_computer_clock();
   cout << "Current time: " << clock.YYYY_MM_DD_H_M_S() << endl;

// Now let's open the serialized probabilistic classifier file back up
// and load the function object it contains:

   cout << "Reloading probabilistic learned function:" << endl;
   ifstream fin2(pfunction_filename.c_str(),ios::binary);
   pfunct_type imported_learned_pfunct; 
   deserialize(imported_learned_pfunct, fin2);

/*
   for (int i=0; i<samples.size(); i++)
   {
      cout << "i = " << i << " label = " << labels[i]
//           << " class = " << learned_function.function(samples[i]) 
           << " class = " << imported_learned_funct.function(samples[i]) 
//           << " prob = " << learned_pfunct.function(samples[i]) 
           << " prob = " << imported_learned_pfunct.function(samples[i]) 
           << endl;
   }
*/

   cout << "At end of SVM_SHAPE" << endl;
   exit(-1);
   

// Note that there is also an example program that comes with dlib
// called the file_to_code_ex.cpp example.  It is a simple program
// that takes a file and outputs a piece of C++ code that is able to
// fully reproduce the file's contents in the form of a std::string
// object.  So you can use that along with the std::istringstream to
// save learned decision functions inside your actual C++ code files
// if you want.

// Lastly, note that the decision functions we trained above involved
// well over 200 basis vectors.  Support vector machines in general
// tend to find decision functions that involve a lot of basis
// vectors.  This is significant because the more basis vectors in a
// decision function, the longer it takes to classify new examples.
// So dlib provides the ability to find an approximation to the normal
// output of a trainer using fewer basis vectors.

// Here we determine the cross validation accuracy when we approximate
// the output using only 10 basis vectors.  To do this we use the
// reduced2() function.  It takes a trainer object and the number of
// basis vectors to use and returns a new trainer object that applies
// the necessary post processing during the creation of decision
// function objects.

//   cout << "Cross validation accuracy with only 10 support vectors: " 
//        << cross_validate_trainer(reduced2(trainer,10), samples, labels, 3)
//        << endl;

// Lets print out the original cross validation score too for
// comparison:

//   cout << "cross validation accuracy with all the original support vectors: " 
//        << cross_validate_trainer(trainer, samples, labels, 3);

// When you run this program you should see that, for this problem,
// you can reduce the number of basis vectors down to 10 without
// hurting the cross validation accuracy.

// To get the reduced decision function out we would just do this:
//   learned_function.function = reduced2(trainer,10).train(samples, labels);

// And similarly for the probabilistic_decision_function: 

//   learned_pfunct.function = train_probabilistic_decision_function(
//      reduced2(trainer,10), samples, labels, 3);

   int n_reduced_vecs=100;
   double eps=1E-6;
   cout << "Text reduced binary decision function = "
        << test_binary_decision_function(
           reduced2(null_trainer(learned_function.function), 
           n_reduced_vecs,eps).train(
              samples,labels),samples,labels)
        << endl;

// To force reduced trainer to have a 95% success rate on first class,
// chant

   cout << "Text reduced binary decision function = "
        << test_binary_decision_function(
           roc_c1_trainer(
              reduced2(
                 null_trainer(learned_function.function), 
                 n_reduced_vecs,eps),0.95).train(
                    samples,labels),samples,labels)
           << endl;

}


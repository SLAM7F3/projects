// ==========================================================================
// SVM_RECOG reads in 9xK dimensional features for N ~32x~32 positive windows
// and N' ~32x~32 negative windows generated by POOL_FEATURES.  It then
// uses Davis King's DLIB library to load the input samples along with
// their positive and negative labels.  After the ordering of the
// input samples is randomized, cross-correlation is performed in
// order to estimate a reasonable value for a linear-SVM slack
// variable C.  DLIB then trains binary decision and probabilistic
// decision functions. Serialized versions of the binary and
// probabilistic decision functions are exported to output binary
// files.

// 				svm_recog

// ==========================================================================
// Last updated on 6/1/14; 6/2/14; 6/3/14; 6/22/14
// ==========================================================================


#include <fstream>
#include <iostream>
#include <string>
#include <vector>
#include <flann/flann.hpp>
#include <flann/io/hdf5.h>
#include "dlib/svm.h"

#include "astro_geo/Clock.h"
#include "general/filefuncs.h"
#include "general/outputfuncs.h"
#include "general/stringfuncs.h"
#include "general/sysfuncs.h"
#include "time/timefuncs.h"

using std::cin;
using std::cout;
using std::endl;
using std::flush;
using std::ifstream;
using std::ios;
using std::ofstream;
using std::string;
using std::vector;

// ==========================================================================
int main(int argc, char *argv[])
// ==========================================================================
{
   std::set_new_handler(sysfunc::out_of_memory);

// Note: Next 3 constants should be set automatically and hot
// hardwired in this file!

   const unsigned int K=1000;
   const unsigned int nineK=9*K;
   unsigned int n_positive_descriptors=15000;
   unsigned int n_negative_descriptors=15047;

   timefunc::initialize_timeofday_clock();

   string trees_subdir="./trees/";
   string dictionary_subdir=trees_subdir+"dictionary/";
   string learned_funcs_subdir=trees_subdir+"learned_functions/";
   filefunc::dircreate(learned_funcs_subdir);

// As of 8/25/12, we're running out of RAM on BEAST!  So we must limit
// the number of training samples...

   const unsigned int N_max=30000;
//   const unsigned int N_max=35000;	// Too large with 3x3 pooling!
   unsigned Npositives=basic_math::min(n_positive_descriptors,N_max);
   unsigned Nnegatives=basic_math::min(n_negative_descriptors,N_max);

// Define N as geometric mean of Npositives and Nnegatives:

   cout << "Npositives = " << Npositives << " Nnegatives = " << Nnegatives 
        << endl;
   int N=sqrt(1.0*Npositives*Nnegatives);
   cout << "N = sqrt(Npositives*Nnegatives) = " << N << endl;
   cout << "sqrt(Npositives)*sqrt(Nnegatives) = "
        << sqrt(Npositives)*sqrt(Nnegatives) << endl;

   double positive_to_negative_data_ratio=
      double(Npositives)/double(Nnegatives);
   cout << "Positive-to-negative data ratio = " 
        << positive_to_negative_data_ratio << endl;

// The svm functions use column vectors to contain a lot of the
// data on which they operate. So the first thing we do here is
// declare a convenient typedef.

// This typedef declares a matrix with K rows and 1 column.  It will
// be the object that contains each of our K dimensional samples. 

   typedef dlib::matrix<double, nineK, 1> sample_type;

// This is a typedef for the type of kernel we are going to use in
// this example.  In this case I have selected the linear kernel that
// can operate on our K-dim sample_type objects

   typedef dlib::linear_kernel<sample_type> kernel_type;

// Now we make objects to contain our samples and their respective
// labels.

   std::vector<sample_type> samples;
   std::vector<double> labels;
   samples.reserve(Npositives+Nnegatives);
   labels.reserve(Npositives+Nnegatives);

   sample_type samp;

// Import positive window descriptors generated via program
// POOL_FEATURES:

   cout << "Importing positive window descriptors:" << endl;
   flann::Matrix<float>* positive_descriptors_ptr=
      new flann::Matrix<float>(
         new float[n_positive_descriptors*K],n_positive_descriptors,K);
   string positive_descriptors_hdf5_filename=dictionary_subdir+
      "positive_window_descriptors.hdf5";
   flann::load_from_file(
      (*positive_descriptors_ptr),positive_descriptors_hdf5_filename.c_str(),
      "window_descriptors");

   cout << "positive_descriptors.rows = " << positive_descriptors_ptr->rows 
        << endl;
   cout << "positive_descriptors.cols = " << positive_descriptors_ptr->cols 
        << endl;
//   int Npositives=positive_descriptors_ptr->rows;

   if (nineK != positive_descriptors_ptr->cols)
   {
      cout << "nineK = " << nineK 
           << " positive_descriptors.cols = " << positive_descriptors_ptr->cols
           << endl;
      exit(-1);
   }

// Copy positive data into samples and labels objects:

   cout << "Copying positive data:" << endl;
   for (unsigned int n=0; n<Npositives; n++)
   {
      bool OK_descriptor_flag=true;
      for (unsigned int k=0; k<nineK; k++)
      {
         double curr_component=(*positive_descriptors_ptr)[n][k];
         if (mathfunc::my_isnan(curr_component))
         {
            OK_descriptor_flag=false;
            break;
         }
         samp(k)=curr_component;
//         if (samp(k) > 100 || samp(k) < 0 || mathfunc::my_isnan(samp(k)) )
//         {
//            cout << "n = " << n << " k = " << k 
//                 << " positive feature = " << (*positive_descriptors_ptr)[n][k]
//                 << endl;
//         }
      } // loop over index k 

      if (OK_descriptor_flag)
      {
         samples.push_back(samp);
         labels.push_back(1);
      }
      
   } // loop over index n
   delete [] positive_descriptors_ptr->ptr();
   delete positive_descriptors_ptr;

// Import negative window descriptors generated via program
// POOL_FEATURES:

   cout << "Importing negative window descriptors:" << endl;
   flann::Matrix<float>* negative_descriptors_ptr=
      new flann::Matrix<float>(
         new float[n_negative_descriptors*K],n_negative_descriptors,K);
   string negative_descriptors_hdf5_filename=dictionary_subdir+
      "negative_window_descriptors.hdf5";
   flann::load_from_file(
      (*negative_descriptors_ptr),negative_descriptors_hdf5_filename.c_str(),
      "window_descriptors");

   cout << "negative_descriptors.rows = " << negative_descriptors_ptr->rows 
        << endl;
   cout << "negative_descriptors.cols = " << negative_descriptors_ptr->cols 
        << endl;
//   int Nnegatives=negative_descriptors_ptr->rows;

// Copy negative data into samples and labels objects:

   cout << "Copying negative data:" << endl;
   for (unsigned int n=0; n<Nnegatives; n++)
   {
      bool OK_descriptor_flag=true;
      for (unsigned int k=0; k<nineK; k++)
      {
         double curr_component=(*negative_descriptors_ptr)[n][k];
         if (mathfunc::my_isnan(curr_component))
         {
            OK_descriptor_flag=false;
            break;
         }
         samp(k)=curr_component;
      } // loop over index k
      if (OK_descriptor_flag)
      {
         samples.push_back(samp);
         labels.push_back(-1);
      }
   } // loop over index n
   delete [] negative_descriptors_ptr->ptr();
   delete negative_descriptors_ptr;

// Here we normalize all the samples by subtracting their mean and
// dividing by their standard deviation. This is generally a good idea
// since it often heads off numerical stability problems and also
// prevents one large feature from smothering others.  Doing this
// doesn't matter much in this example so I'm just doing this here so
// you can see an easy way to accomplish this with the library.

   dlib::vector_normalizer<sample_type> normalizer;
   // let the normalizer learn the mean and standard deviation of the samples
   normalizer.train(samples);
   // now normalize each sample

   cout << "Normalizing samples:" << endl;
   for (unsigned long i = 0; i < samples.size(); ++i)
   {
      samples[i] = normalizer(samples[i]); 
   }
   
// Now that we have some data we want to train on it.  However, there
// are two parameters to the training.  These are the nu and gamma
// parameters.  Our choice for these parameters will influence how
// good the resulting decision function is.  To test how good a
// particular choice of these parameters is we can use the
// cross_validate_trainer() function to perform n-fold cross
// validation on our training data.  However, there is a problem with
// the way we have sampled our distribution above.  The problem is
// that there is a definite ordering to the samples.  That is, the
// first half of the samples look like they are from a different
// distribution than the second half.  This would screw up the cross
// validation process but we can fix it by randomizing the order of
// the samples with the following function call.

   cout << "Before randomizing samples" << endl;
   dlib::randomize_samples(samples, labels);

// Here we make an instance of the svm_c_linear_trainer object that uses our
// kernel type:

   dlib::svm_c_linear_trainer<kernel_type> trainer;
//   dlib::svm_c_ekm_trainer<kernel_type> trainer;

/*
// CROSS VALIDATION STARTS HERE:

   double min_C=1.0/4.0 * 500.0;
   double max_C=4.0 * 500.0;

// Now we loop over some different C values to see how good
// they are.  Note that this is a very simple way to try out a few
// possible parameter choices.  You should look at the
// model_selection_ex.cpp program for examples of more sophisticated
// strategies for determining good parameter choices.

   double max_product_correct=-1;
   double best_product_pos_correct,best_product_neg_correct;
   double max_sum_correct=-1;
   double best_sum_pos_correct,best_sum_neg_correct;
   double best_product_C,best_sum_C;

   cout << "Performing cross validation" << endl;
   for (double C = min_C; C <= max_C; C *= 2)
   {
      // tell the trainer the parameters we want to use

// If # class2/# class1 = alpha, then c_class1 = alpha c_class2 (rule
// of thumb)

      double C_positive=C;
      double C_negative=C*positive_to_negative_data_ratio;
      trainer.set_c_class1(C_positive);
      trainer.set_c_class2(C_negative);
      cout << "C_positive = " << C_positive
           << " C_negative = " << C_negative << endl;

// Print out the cross validation accuracy for 3-fold cross validation
// using the current C coeffient. cross_validate_trainer() returns a
// row vector.  The first element of the vector is the fraction of +1
// training examples correctly classified and the second number is the
// fraction of -1 training examples correctly classified.

      dlib::matrix<double,1,2> pos_neg_results=
         cross_validate_trainer(trainer,samples,labels,3);
      double pos_correct=pos_neg_results(0,0);
      double neg_correct=pos_neg_results(0,1);
         
      double product_correct=pos_correct*neg_correct;
      if (product_correct > max_product_correct)
      {
         max_product_correct=product_correct;
         best_product_pos_correct=pos_correct;
         best_product_neg_correct=neg_correct;
         best_product_C=C;
      }
         
      double sum_correct=pos_correct+neg_correct;
      if (sum_correct > max_sum_correct)
      {
         max_sum_correct=sum_correct;
         best_sum_pos_correct=pos_correct;
         best_sum_neg_correct=neg_correct;
         best_sum_C = C;
      }

      cout << "  Cross validation accuracy: " 
           << cross_validate_trainer(trainer, samples, labels, 3)
           << endl;
      cout << "  Product_correct = " << product_correct
           << " sum_correct = " << sum_correct << endl << endl;

      cout << "Elapsed time = " 
           << timefunc::elapsed_timeofday_time()/60 << " mins = " 
           << timefunc::elapsed_timeofday_time()/3600 << " hours" << endl;

   } // loop over index C labeling cross validation parameter value

   cout << "max_product_correct = " << max_product_correct << endl;
   cout << "best_product_pos_correct = " << best_product_pos_correct << endl;
   cout << "best_product_neg_correct = " << best_product_neg_correct << endl;
   cout << "best_product_C = " << best_product_C << endl;
   cout << endl;

   cout << "max_sum_correct = " << max_sum_correct << endl;
   cout << "best_sum_pos_correct = " << best_sum_pos_correct << endl;
   cout << "best_sum_neg_correct = " << best_sum_neg_correct << endl;
   cout << "best_sum_C = " << best_sum_C << endl;

   exit(-1);
*/

// Cross validiation results on 15K positive & 15K negative 9K-dimensional
// descriptors:

/*
Elapsed time = 8.35102 mins = 0.139184 hours
C_positive = 500 C_negative = 498.438
  Cross validation accuracy: 0.975267 0.962845 

  Product_correct = 0.93903 sum_correct = 1.93811

10:30 am Jun 24, 2013:

C_positive = 1000 C_negative = 996.876
  Cross validation accuracy:    0.975 0.961981 

  Product_correct = 0.937931 sum_correct = 1.93698
*/


// CROSS VALIDATION ENDS HERE

   double C=1000;
   trainer.set_c(C);
//    trainer.be_verbose();

// Now we train on the full set of data and obtain the resulting
// decision function.  The decision function will return values >= 0
// for samples it predicts are in the +1 class and numbers < 0 for
// samples it predicts to be in the -1 class.

   typedef dlib::decision_function<kernel_type> dec_funct_type;
   typedef dlib::normalized_function<dec_funct_type> funct_type;

// Here we are making an instance of the normalized_function object.
// This object provides a convenient way to store the vector
// normalization information along with the decision function we are
// going to learn.

   funct_type learned_function;
   learned_function.normalizer = normalizer;  // save normalization 
					      //   information

// Perform the actual SVM training and save the results.  Print out
// the number of support vectors in the resulting decision function:

   cout << endl;
   cout << "-----------------------------------------------------" << endl;
   cout << "Starting to train binary decision function:" << endl;
   learned_function.function = trainer.train(samples, labels); 
   cout << "Elapsed time = " 
        << timefunc::elapsed_timeofday_time()/60 << " mins = " 
        << timefunc::elapsed_timeofday_time()/3600 << " hours" << endl;
   cout << "Number of support vectors in our learned_function =  " 
        << learned_function.function.basis_vectors.nr() << endl;
   cout << "Number of training samples: N = " << N << endl;

// Another thing that is worth knowing is that just about everything
// in dlib is serializable. So for example, you can save the
// learned_pfunct object to disk and recall it later like so:

   string output_filename=learned_funcs_subdir+
      "Ng_bifunc_"+stringfunc::number_to_string(Npositives)+"_"+
      stringfunc::number_to_string(Nnegatives)+".dat";
   ofstream fout(output_filename.c_str(),ios::binary);
   dlib::serialize(learned_function,fout);
   fout.close();

   string banner="Exported learned binary function to "+output_filename;
   outputfunc::write_banner(banner);

// We can also train a decision function that reports a well
// conditioned probability instead of just a number > 0 for the +1
// class and < 0 for the -1 class.  An example of doing that follows:

   typedef dlib::probabilistic_decision_function<kernel_type> 
      probabilistic_funct_type;  
   typedef dlib::normalized_function<probabilistic_funct_type> pfunct_type;

   pfunct_type learned_pfunct; 
   learned_pfunct.normalizer = normalizer;

   cout << endl;
   cout << "-----------------------------------------------------" << endl;
   cout << "Starting to train probabilistic decision function:" << endl;
   learned_pfunct.function = train_probabilistic_decision_function(
      trainer, samples, labels, 3);
   cout << "Elapsed time = " 
        << timefunc::elapsed_timeofday_time()/60 << " mins = " 
        << timefunc::elapsed_timeofday_time()/3600 << " hours" << endl;

// Now we have a function that returns the probability that a given
// sample is of the +1 class.

// print out the number of support vectors in the resulting decision
// function. (it should be the same as in the one above)

   cout << "Number of support vectors in our learned_pfunct =  " 
        << learned_pfunct.function.decision_funct.basis_vectors.nr() << endl;
   cout << "Number of training samples: N = " << N << endl;

// Another thing that is worth knowing is that just about everything
// in dlib is serializable. So for example, you can save the
// learned_pfunct object to disk and recall it later like so:

   output_filename=learned_funcs_subdir
      +"Ng_pfunct_"+stringfunc::number_to_string(Npositives)+"_"
      +stringfunc::number_to_string(Nnegatives)+".dat";
   ofstream fout2(output_filename.c_str(),ios::binary);
   serialize(learned_pfunct,fout2);
   fout2.close();

   banner="Exported learned probabilistic function to "+output_filename;
   outputfunc::write_banner(banner);

// Now lets open that file back up and load the function object it
// contains:

   ifstream fin(output_filename.c_str(),ios::binary);
   deserialize(learned_pfunct, fin);

   cout << "Binary decision function = "
        << test_binary_decision_function(
           learned_function.function,samples,labels)
        << endl;

   exit(-1);

// Note that there is also an example program that comes with dlib
// called the file_to_code_ex.cpp example.  It is a simple program
// that takes a file and outputs a piece of C++ code that is able to
// fully reproduce the file's contents in the form of a std::string
// object.  So you can use that along with the std::istringstream to
// save learned decision functions inside your actual C++ code files
// if you want.

// Lastly, note that the decision functions we trained above involved
// well over 200 basis vectors.  Support vector machines in general
// tend to find decision functions that involve a lot of basis
// vectors.  This is significant because the more basis vectors in a
// decision function, the longer it takes to classify new examples.
// So dlib provides the ability to find an approximation to the normal
// output of a trainer using fewer basis vectors.

// Here we determine the cross validation accuracy when we approximate
// the output using only 10 basis vectors.  To do this we use the
// reduced2() function.  It takes a trainer object and the number of
// basis vectors to use and returns a new trainer object that applies
// the necessary post processing during the creation of decision
// function objects.

   cout << "Cross validation accuracy with only 10 support vectors: " 
        << cross_validate_trainer(reduced2(trainer,10), samples, labels, 3);

// Lets print out the original cross validation score too for
// comparison:

   cout << "cross validation accuracy with all the original support vectors: " 
        << cross_validate_trainer(trainer, samples, labels, 3);

// When you run this program you should see that, for this problem,
// you can reduce the number of basis vectors down to 10 without
// hurting the cross validation accuracy.

// To get the reduced decision function out we would just do this:
   learned_function.function = reduced2(trainer,10).train(samples, labels);

// And similarly for the probabilistic_decision_function: 

   learned_pfunct.function = train_probabilistic_decision_function(
      reduced2(trainer,10), samples, labels, 3);

   cout << "Reduced binary decision function = "
        << test_binary_decision_function(
           reduced2(null_trainer(learned_function.function), 50).train(
              samples,labels),samples,labels)
        << endl;


}


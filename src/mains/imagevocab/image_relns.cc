// ==========================================================================
// Program IMAGE_RELNS imports Voronoi cell histograms generated by
// program IMAGE_WORD_HISTOGRAMS for some number of input images.  It
// fills a term-frequency matrix whose entries are renormalized to
// take into account variable image "document" size.  Inverse document
// frequencies are next computed to assess individual SIFT word
// importance. SIFT words whose importance lie below some minimal
// threshold are added to a stop-list.  TF-IDF ratios are formed and
// exported to text and binary file formats.

//				./image_relns



/*
Number of images = 2093
Number of SIFT words in image histograms = 12347
Number of edges = 2189278
Min edge weight = 0
Max edge weight = 0.955447
Elapsed time = 350.6 secs =   5.84 minutes =   0.097 hours 


Number of images = 1820
Number of SIFT words in image histograms = 12347
Number of edges = 1655290
Min edge weight = 0
Max edge weight = 0.959977
Elapsed time = 284.0 secs =   4.73 minutes =   0.079 hours 

*/

// ==========================================================================
// Last updated on 8/23/13; 8/30/13; 9/1/13
// ==========================================================================

#include <iostream>
#include <map>
#include <string>
#include <vector>
#include "gmm/gmm.h"
#include "gmm/gmm_matrix.h"

#include "general/filefuncs.h"
#include "math/genvector.h"
#include "math/mathfuncs.h"
#include "general/outputfuncs.h"
#include "math/prob_distribution.h"
#include "general/stringfuncs.h"
#include "general/sysfuncs.h"
#include "templates/mytemplates.h"
#include "time/timefuncs.h"

using std::cin;
using std::cout;
using std::endl;
using std::flush;
using std::ifstream;
using std::ios;
using std::map;
using std::ofstream;
using std::string;
using std::vector;

// ==========================================================================
int main(int argc, char *argv[])
// ==========================================================================
{
   std::set_new_handler(sysfunc::out_of_memory);

   timefunc::initialize_timeofday_clock();      

   string sift_keys_subdir="/data/sift_keyfiles/";
//   string image_words_subdir=sift_keys_subdir+"image_words/";
   string image_words_subdir=
      sift_keys_subdir+"BostonBombing_imagewords_32K_clusters/";

// Import Voronoi cell histograms for input images:

   cout << "Importing Voronoi cell histograms:" << endl;
   vector<string> allowed_suffixes;
   allowed_suffixes.push_back("words");
   vector<string> voronoi_cell_filenames=
      filefunc::files_in_subdir_matching_specified_suffixes(
         allowed_suffixes,image_words_subdir);
   int n_images=voronoi_cell_filenames.size();
   cout << "n_images = " << n_images << endl;

//   int n_words=7378;	// 10M Boston Bombing descriptors; 
		        //	10K requested voronoi cells
   int n_words=23227;   // 10 M Boston Bombing descriptors
			//      32K requested voronoi cells
//   int n_words=12347;	// 32M descriptors; 15K requested voronoi cells
//   int n_words=16384;
//   int n_words=24718;
//   int n_words=65536;

   cout << "n_images = " << n_images << endl;
   string n_images_str=stringfunc::number_to_string(n_images);

// Export image_list.dat:

   ofstream imagestream;
   string imagelist_filename=sift_keys_subdir+"image_list.dat";
   filefunc::openfile(imagelist_filename,imagestream);
   imagestream << "# image_ID  image_basename " << endl << endl;

   for (int n=0; n<n_images; n++)
   {
      string image_basename=filefunc::getbasename(voronoi_cell_filenames[n]);

      int terminator_posn=stringfunc::first_substring_location(
         image_basename,".words");
      vector<string> substrings=stringfunc::decompose_string_into_substrings(
         image_basename,"_");
      if (substrings.size() > 2)
      {
//         terminator_posn=image_basename.find_last_of("_");
      }
      image_basename=image_basename.substr(0,terminator_posn);
      imagestream << n << "    "
                  << image_basename << endl;
   }
   filefunc::closefile(imagelist_filename,imagestream);
   string banner="Exported image basenames to "+imagelist_filename;
   outputfunc::write_banner(banner);

// Import image voronoi cell occupancy histograms from text files
// generated by program IMAGE_WORD_HISTOGRAMS:

   genvector* ndocs_containing_term_ptr=new genvector(n_words);
   ndocs_containing_term_ptr->clear_values();

   gmm::row_matrix< gmm::wsvector<float> >* term_freq_sparse_matrix_ptr=
      new gmm::row_matrix< gmm::wsvector<float> >(n_images,n_words);
   gmm::clear(*term_freq_sparse_matrix_ptr);

   vector<int> total_term_frequency;
   for (int t=0; t<n_words; t++)
   {
      total_term_frequency.push_back(0);
   }
   
   int n_start=0;
   int n_stop=n_images;
   for (int n=n_start; n<n_stop; n++)
   {
      if (n > 0 && n%100==0) cout << n << " " << flush;
      if (n%1000==0) cout << endl;
      
      filefunc::ReadInfile(voronoi_cell_filenames[n]);

      vector<double> first_column_values,second_column_values;
      for (unsigned int l=0; l<filefunc::text_line.size(); l++)
      {
         vector<double> column_values=stringfunc::string_to_numbers(
            filefunc::text_line[l]);
         first_column_values.push_back(column_values[0]);
         second_column_values.push_back(column_values[1]);
      }
      int max_term_freq=mathfunc::maximal_value(second_column_values);
//      int n_terms_in_curr_doc=mathfunc::mean(second_column_values)*
//         second_column_values.size();

      for (int t=0; t<n_words; t++)
      {
         (*term_freq_sparse_matrix_ptr)(n,t)=0;
      } // loop over index t labeling terms
      
// Divide raw term frequency by max term frequency in order to
// minimize variation due to document length.  Then store renormalized
// term frequencies within rows of *term_freq_sparse_matrix_ptr:

      for (unsigned int f=0; f<first_column_values.size(); f++)
      {
         int t=first_column_values[f];
         int curr_term_freq=second_column_values[f];
         double renorm_term_freq=0.5*(1+double(curr_term_freq)/max_term_freq);
//         double renorm_term_freq=double(curr_term_freq)/max_term_freq;
//         double renorm_term_freq=double(curr_term_freq)/n_terms_in_curr_doc;
         (*term_freq_sparse_matrix_ptr)(n,t)=renorm_term_freq;
         total_term_frequency[t] += curr_term_freq;

// Increment *ndocs_containing_term_ptr for current document:

         if (renorm_term_freq > 0)
         {            
            ndocs_containing_term_ptr->put(
               t,ndocs_containing_term_ptr->get(t)+1);
         }
         
      } // loop over index f labeling non-zero frequency terms

   } // loop over index n effectively labeling image "documents"
   cout << endl;

// Compute inverse document frequency as indicator of individual SIFT
// word importance:

   vector<int> sorted_word_IDs,ndocs;
   vector<double> word_importance;
   genvector* inverse_doc_frequency_ptr=new genvector(n_words);   
   for (int t=0; t<n_words; t++)
   {
      sorted_word_IDs.push_back(t);
      ndocs.push_back(basic_math::round(ndocs_containing_term_ptr->get(t)));

      double quotient=double(n_images)/(1+ndocs_containing_term_ptr->get(t));
      double idf=log(quotient);

      inverse_doc_frequency_ptr->put(t,idf);
      word_importance.push_back(idf);
//      cout << "t = " << t 
//           << " ndocs = " << ndocs.back()
//           << " idf = " << idf << endl;
   }
   delete ndocs_containing_term_ptr;

// Export density and cumulative distributions for total term
// frequencies:

   prob_distribution total_term_freqs_probs(total_term_frequency,10000,0);
   int term_freq_98=
      total_term_freqs_probs.find_x_corresponding_to_pcum(0.98);
   cout << "term_freq_98 = " << term_freq_98 << endl;

   for (int t=0; t<n_words; t++)
   {
      total_term_frequency[t]=basic_math::min(
         total_term_frequency[t],term_freq_98);
   }
   total_term_freqs_probs=prob_distribution(total_term_frequency,10000,0);
   total_term_freqs_probs.writeprobdists(false);

   int min_term_freq=total_term_freqs_probs.find_x_corresponding_to_pcum(0.1);
   int max_term_freq=total_term_freqs_probs.find_x_corresponding_to_pcum(0.95);
   
   cout << "min_term_freq = " << min_term_freq << endl;
   cout << "max_term_freq = " << max_term_freq << endl;

// Export file containing words sorted according to their inverse
// document frequencies:

   cout << "n_words = " << n_words 
        << " word_importance.size() = " << word_importance.size() << endl;
   templatefunc::Quicksort_descending(word_importance,ndocs,sorted_word_IDs);

// Declare any word with an importance less than min_word_importance
// to be a stop word:

   double min_word_importance=-1;
   double max_ndocs_frac=0.5;

   int n_stop_words=0;
   vector<bool> stop_word_vec;
   for (int t=0; t<n_words; t++)
   {
      double ndocs_frac=double(ndocs[t])/double(n_images);
      if (ndocs_frac > max_ndocs_frac && min_word_importance < 0)
      {
         min_word_importance=word_importance[t];
      }

//      if (word_importance[t] < min_word_importance)


      if (total_term_frequency[t] < min_term_freq ||
          total_term_frequency[t] > max_term_freq)
      {
         stop_word_vec.push_back(true);
         n_stop_words++;
      }
      else
      {
         stop_word_vec.push_back(false);
      }
   }

   int n_go_words=n_words-n_stop_words;
   cout << "min_word_importance = " << min_word_importance << endl;
   cout << "n_go_words = " << n_go_words << endl;
   cout << "n_stop_words = " << n_stop_words << endl;

   string importance_filename=
      sift_keys_subdir+"word_importance_nimages_"+n_images_str+".dat";
   ofstream importance_stream;
   filefunc::openfile(importance_filename,importance_stream);
   importance_stream << "# Word  ndocs_frac  importance" << endl;
   importance_stream << "# min_word_importance = " << min_word_importance
                     << endl;
   importance_stream << "# n_go_words = " << n_go_words  << endl;
   importance_stream << "# n_stop_words = " << n_stop_words << endl << endl;
   for (int t=0; t<n_words; t++)
   {
      double ndocs_frac=double(ndocs[t])/double(n_images);
      importance_stream << sorted_word_IDs[t] << "      "
                        << ndocs_frac << "      "
                        << word_importance[t] << endl;
   }
   filefunc::closefile(importance_filename,importance_stream);
   banner="Exported "+importance_filename;
   outputfunc::write_banner(banner);

// Compute term-frequency inverse document frequency (TF-IDF) ratios:

   banner="Calculating TF-IDF ratios";
   outputfunc::write_banner(banner);
   genvector* tfidf_ptr=new genvector(n_words);
   genvector* normalized_tfidf_ptr=new genvector(n_words);

//   gmm::row_matrix< gmm::wsvector<float> >* word_doc_sparse_matrix_ptr=
//      new gmm::row_matrix< gmm::wsvector<float> >(n_words,n_images);
   gmm::col_matrix< gmm::wsvector<float> >* word_doc_sparse_matrix_ptr=
      new gmm::col_matrix< gmm::wsvector<float> >(n_words,n_images);
   gmm::clear(*word_doc_sparse_matrix_ptr);

   int n_nonzero_values=0;
   double TINY=1E-9;
   for (int n=n_start; n<n_stop; n++)
   {
      if (n%100==0) cout << n << " " << flush;
      tfidf_ptr->clear_values();
      for (int t=0; t<n_words; t++)
      {
         if (stop_word_vec[t]) continue;
         tfidf_ptr->put(
            t,(*term_freq_sparse_matrix_ptr)(n,t)*
            inverse_doc_frequency_ptr->get(t));
      }

// Renormalize *tfidf_ptr so that it has unit length:

      *normalized_tfidf_ptr=tfidf_ptr->unitvector();


      for (int t=0; t<n_words; t++)
      {
         double curr_tfidf=normalized_tfidf_ptr->get(t);

         if (fabs(curr_tfidf) > TINY)
         {
            (*word_doc_sparse_matrix_ptr)(t,n)=curr_tfidf;
            n_nonzero_values++;
         }
      } // loop over index t labeling terms

   } // loop over index n labeling input images
   cout << endl;
   double nonzero_values_frac=double(n_nonzero_values)/
      double(n_words*(n_stop-n_start));
   cout << "n_nonzero_values = " << n_nonzero_values << endl;
   cout << "nonzero_values_frac = " << nonzero_values_frac << endl;

   delete inverse_doc_frequency_ptr;
   delete term_freq_sparse_matrix_ptr;
   delete tfidf_ptr;
   delete normalized_tfidf_ptr;

// Export image document edge list:

   banner="Exporting image document edge list:";
   outputfunc::write_banner(banner);
   
   string edgelist_filename=sift_keys_subdir+"docs_edgelist.dat";
   ofstream outstream;
   filefunc::openfile(edgelist_filename,outstream);

//   double min_edge_weight=0;
   double min_edge_weight=0.165;   // 99th percentile for Boston Bombing
   outstream << "# Edge weight threshold = " << min_edge_weight << endl;
   outstream << "# NodeID  NodeID'  Edge weight" << endl;
   outstream << endl;

   int n_edges=0;
   double max_edge_weight=0;
   vector<float> curr_normalized_tfidf;
   curr_normalized_tfidf.reserve(n_words);
   
   vector<double> edge_weights;

//   int nskip=1;
//   int nskip=3;
   int nskip=10;
//   int nskip=32;
//   int nskip=100;
   for (int n=n_start; n<n_stop; n += nskip)
   {
      outputfunc::update_progress_fraction(n,10,n_stop);
      curr_normalized_tfidf.clear();
      for (int t=0; t<n_words; t++)
      {
         curr_normalized_tfidf.push_back(
            (*word_doc_sparse_matrix_ptr)(t,n));
      } // loop over index t labeling dictionary words

      for (int m=n+1; m<n_stop; m++)
      {
         double dotproduct=0;
         for (int t=0; t<n_words; t++)
         {
            dotproduct += curr_normalized_tfidf.at(t)*
               (*word_doc_sparse_matrix_ptr)(t,m);
         }

         double edge_weight=dotproduct;
         edge_weights.push_back(edge_weight);

         if (edge_weight < min_edge_weight) continue;
         max_edge_weight=basic_math::max(max_edge_weight,edge_weight);
         outstream << n << "  " << m << "  " << edge_weight << endl;
         n_edges++;

/*         
         if (dotproduct > 0.99)
         {
            double dotprod=0;
            cout << "n = " << n << " m = " << m << endl;
            for (int t=0; t<n_words; t++)
            {
               double curr_coeff=(*word_doc_sparse_matrix_ptr)(t,n);
               double next_coeff=(*word_doc_sparse_matrix_ptr)(t,m);
               dotprod += curr_coeff*next_coeff;
               if (curr_coeff > TINY && next_coeff > TINY)
                  cout << t << ":" << curr_coeff << "," 
                       << next_coeff << "  " << flush;
            }
            cout << endl;
            cout << "dotprod = " << dotprod << endl;
            outputfunc::enter_continue_char();
         }
*/

      } // loop over index m
      
   } // loop over index n labeling images

   filefunc::closefile(edgelist_filename,outstream);
   cout << endl;

   cout << "Number of images = " << n_images << endl;
   cout << "Number of SIFT 'go' words in image histograms = " << n_go_words 
        << endl;
   cout << "Number of SIFT 'stop' words in image histograms = " 
        << n_stop_words << endl;
   cout << "Number of edges = " << n_edges << endl;
   cout << "Min edge weight = " << min_edge_weight << endl;
   cout << "Max edge weight = " << max_edge_weight << endl;
   banner="Exported document edge list to "+edgelist_filename;
   outputfunc::write_banner(banner);

// Export density and cumulative distributions for edge weights:

   prob_distribution edgeweights_probs(edge_weights,500,0);
   edgeweights_probs.writeprobdists(false);

/*
// Export sparse word-doc matrix containing renormalized TF-IDF values:

   string sparse_matrix_filename=sift_keys_subdir+"sparse_matrix_txt.dat";
   mathfunc::export_to_sparse_text_format(
      word_doc_sparse_matrix_ptr,n_nonzero_values,
      sparse_matrix_filename);

   sparse_matrix_filename=sift_keys_subdir+"sparse_matrix_bin.dat";
   mathfunc::export_to_sparse_binary_format(
      word_doc_sparse_matrix_ptr,n_nonzero_values,
      sparse_matrix_filename);

   banner=
      "Exported sparse word-doc matrix containing renormalized TF-IDF values";
   outputfunc::write_banner(banner);
*/

   delete word_doc_sparse_matrix_ptr;

   cout << "At end of program IMAGE_RELNS" << endl;
   outputfunc::print_elapsed_time();
}
